0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
Thu Apr 14 10:20:23 2022
Using weight: weight for classifier: SvB_MA
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 2396428
wDB 113735.127030
wDBn -29.981275
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 128876
wT 4694.203751
wTn -16.779144
Not obvious how to handle negative ttbar weights, for now remove them
nS 645566
nB 2524908
sum_wS 201.702339
sum_wB 118506.072474
nzz =   68867, wzz =   71.9, wzz_SR =   59.2
nzh =  541264, wzh =   92.5, wzh_SR =   77.9
nhh =   35435, whh =   37.3, whh_SR =   33.4
sum_wS_SR 170.521446
sum_wB_SR 50064.653301
sum_wSp_SR 203.140887
sum_wSn_SR 32.619440
Negative weight signal events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 263.065004
sigScaleZH 390.354672
sigScaleHH 609.145944
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.192666
Setup training/validation tensors
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6337 (41,  9, 18, 33, 18) | 0.864 | 1.021 | 89.58 | 86.75 |---------------------------|
0             Validation | 0.6335 (41,  9, 18, 33, 18) | 0.871 | 1.026 | 89.61 | 86.79 |###########################| ^ (0.3%, 11.59, 1.5, 5%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1032 batches)
0 >>  2/20 <<   Training | 0.6074 (38, 10, 17, 33, 22) | 1.058 | 1.241 | 90.64 | 87.75 |-------------------------------------|
0             Validation | 0.6089 (38, 10, 17, 33, 22) | 1.084 | 1.267 | 90.54 | 87.74 |#####################################| ^ (0.4%, 9.86, 1.2, 19%) 
0 >>  3/20 <<   Training | 0.6010 (39, 11, 18, 31, 21) | 1.013 | 1.192 | 90.85 | 87.82 |--------------------------------------|
0             Validation | 0.6023 (39, 10, 18, 31, 21) | 1.034 | 1.211 | 90.93 | 87.83 |######################################| ^ (0.4%, 9.25, 1.9, 1%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (516 batches)
0 >>  4/20 <<   Training | 0.5998 (41, 10, 16, 33, 20) | 1.005 | 1.180 | 91.03 | 88.05 |----------------------------------------|
0             Validation | 0.6010 (41, 10, 16, 33, 20) | 1.021 | 1.194 | 90.98 | 88.04 |########################################| ^ (0.4%, 8.72, 1.9, 0%) 
0 >>  5/20 <<   Training | 0.5918 (39, 11, 19, 31, 20) | 0.987 | 1.159 | 90.50 | 87.98 |---------------------------------------|
0             Validation | 0.5928 (39, 11, 19, 31, 19) | 0.996 | 1.164 | 90.64 | 88.00 |#######################################| ^ (0.3%, 9.10, 2.0, 0%) 
Thu Apr 14 10:59:16 2022
Using weight: weight for classifier: SvB_MA
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 2396428
wDB 113735.127030
wDBn -29.981275
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 128876
wT 4694.203751
wTn -16.779144
nS 645566
nB 2525304
sum_wS 201.702339
sum_wB 118489.293330
nzz =   68867, wzz =   71.9, wzz_SR =   59.2
nzh =  541264, wzh =   92.5, wzh_SR =   77.9
nhh =   35435, whh =   37.3, whh_SR =   33.4
sum_wS_SR 170.521446
sum_wB_SR 50056.193175
sum_wSp_SR 203.140887
sum_wSn_SR 32.619440
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 263.031478
sigScaleZH 390.285279
sigScaleHH 609.010109
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.192394
Setup training/validation tensors
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6235 (39,  9, 19, 35, 18) | 0.832 | 0.986 | 90.11 | 86.97 |-----------------------------|
0             Validation | 0.6238 (39,  9, 19, 35, 18) | 0.842 | 0.997 | 90.07 | 87.01 |##############################| ^ (0.3%, 9.59, 1.7, 2%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1032 batches)
0 >>  2/20 <<   Training | 0.6108 (42, 10, 16, 30, 20) | 0.991 | 1.164 | 90.59 | 87.58 |-----------------------------------|
0             Validation | 0.6122 (42, 10, 17, 30, 20) | 1.020 | 1.195 | 90.59 | 87.54 |###################################| ^ (0.3%, 9.98, 1.1, 26%) 
0 >>  3/20 <<   Training | 0.5991 (40,  9, 17, 33, 21) | 1.020 | 1.195 | 90.69 | 87.92 |---------------------------------------|
0             Validation | 0.6013 (40,  9, 17, 33, 20) | 1.036 | 1.216 | 90.64 | 87.86 |######################################| ^ (0.2%, 10.66, 2.3, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (516 batches)
0 >>  4/20 <<   Training | 0.5937 (42,  8, 18, 31, 21) | 1.022 | 1.202 | 90.83 | 87.92 |---------------------------------------|
0             Validation | 0.5953 (42,  8, 19, 30, 20) | 1.028 | 1.204 | 90.79 | 87.88 |######################################| ^ (0.2%, 8.78, 1.1, 33%) 
0 >>  5/20 <<   Training | 0.5897 (40, 10, 18, 31, 21) | 1.065 | 1.254 | 90.96 | 88.08 |----------------------------------------|
0             Validation | 0.5914 (40, 10, 19, 31, 20) | 1.093 | 1.282 | 90.86 | 88.02 |########################################| ^ (0.3%, 10.15, 1.4, 9%) 
0 >>  6/20 <<   Training | 0.5894 (37,  9, 18, 34, 21) | 1.091 | 1.280 | 90.94 | 88.04 |----------------------------------------|
0             Validation | 0.5915 (37,  9, 18, 34, 20) | 1.115 | 1.305 | 90.91 | 87.96 |#######################################| ^ (0.3%, 9.03, 1.3, 14%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (258 batches)
0 >>  7/20 <<   Training | 0.5851 (36, 10, 18, 34, 22) | 1.123 | 1.315 | 91.05 | 88.32 |-------------------------------------------|
0             Validation | 0.5874 (36, 10, 19, 33, 22) | 1.167 | 1.363 | 90.96 | 88.24 |##########################################| ^ (0.3%, 10.48, 1.8, 1%) 
0 >>  8/20 <<   Training | 0.5889 (40,  9, 17, 31, 22) | 1.088 | 1.275 | 91.05 | 88.29 |------------------------------------------|
0             Validation | 0.5916 (40,  9, 17, 31, 22) | 1.092 | 1.277 | 90.91 | 88.22 |##########################################| ^ (0.3%, 10.09, 1.7, 2%) 
0 >>  9/20 <<   Training | 0.5895 (41, 10, 16, 32, 20) | 1.072 | 1.256 | 91.09 | 88.29 |------------------------------------------|
0             Validation | 0.5920 (41, 10, 17, 32, 20) | 1.092 | 1.278 | 90.95 | 88.19 |#########################################| ^ (0.3%, 9.31, 1.9, 1%) 
0 >> 10/20 <<   Training | 0.5920 (40,  9, 16, 33, 22) | 1.076 | 1.261 | 91.09 | 88.26 |------------------------------------------|
0             Validation | 0.5945 (39,  9, 17, 32, 21) | 1.082 | 1.269 | 91.00 | 88.18 |#########################################| ^ (0.3%, 10.18, 1.4, 9%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (129 batches)
0 >> 11/20 <<   Training | 0.5835 (39,  9, 19, 33, 19) | 1.072 | 1.256 | 90.81 | 88.32 |-------------------------------------------|
0             Validation | 0.5860 (39,  9, 19, 33, 19) | 1.090 | 1.273 | 90.65 | 88.26 |##########################################| ^ (0.3%, 10.94, 1.6, 3%) 
0 >> 12/20 <<   Training | 0.5824 (38, 10, 18, 32, 21) | 1.110 | 1.303 | 90.98 | 88.39 |-------------------------------------------|
0             Validation | 0.5850 (37, 10, 19, 32, 21) | 1.142 | 1.336 | 90.86 | 88.31 |###########################################| ^ (0.3%, 10.70, 1.6, 4%) 
0 >> 13/20 <<   Training | 0.5857 (39, 10, 17, 33, 20) | 1.089 | 1.276 | 91.11 | 88.31 |-------------------------------------------|
0             Validation | 0.5887 (39, 10, 18, 33, 20) | 1.119 | 1.308 | 90.98 | 88.22 |##########################################| ^ (0.3%, 11.20, 1.5, 5%) 
0 >> 14/20 <<   Training | 0.5843 (42, 10, 18, 30, 20) | 1.070 | 1.255 | 91.08 | 88.36 |-------------------------------------------|
0             Validation | 0.5870 (42, 10, 18, 30, 19) | 1.081 | 1.264 | 90.94 | 88.27 |##########################################| ^ (0.3%, 10.24, 1.7, 2%) 
0 >> 15/20 <<   Training | 0.5864 (39, 10, 17, 32, 21) | 1.093 | 1.281 | 91.05 | 88.38 |-------------------------------------------|
0             Validation | 0.5895 (39, 10, 17, 32, 21) | 1.107 | 1.293 | 90.86 | 88.29 |##########################################| ^ (0.3%, 10.63, 1.7, 2%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.5826 (39,  9, 17, 33, 21) | 1.106 | 1.296 | 91.15 | 88.44 |--------------------------------------------|
0             Validation | 0.5854 (39,  9, 18, 33, 20) | 1.126 | 1.317 | 91.01 | 88.36 |###########################################| ^ (0.3%, 10.56, 2.2, 0%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.5812 (39,  9, 18, 33, 20) | 1.111 | 1.302 | 91.12 | 88.45 |--------------------------------------------|
0             Validation | 0.5841 (39,  9, 18, 32, 20) | 1.123 | 1.313 | 90.98 | 88.36 |###########################################| ^ (0.3%, 10.56, 2.1, 0%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.5821 (40,  9, 17, 33, 20) | 1.102 | 1.292 | 91.16 | 88.44 |--------------------------------------------|
0             Validation | 0.5851 (40,  9, 18, 33, 20) | 1.118 | 1.306 | 91.01 | 88.35 |###########################################| ^ (0.3%, 10.44, 2.2, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.5812 (39,  9, 18, 33, 20) | 1.111 | 1.303 | 91.13 | 88.44 |--------------------------------------------|
0             Validation | 0.5841 (39,  9, 18, 33, 20) | 1.120 | 1.310 | 90.99 | 88.35 |###########################################| ^ (0.3%, 10.52, 2.3, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.5813 (39,  9, 18, 33, 20) | 1.107 | 1.298 | 91.13 | 88.44 |--------------------------------------------|
0             Validation | 0.5842 (39,  9, 18, 33, 20) | 1.119 | 1.308 | 90.99 | 88.35 |###########################################| ^ (0.3%, 10.51, 2.3, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_8_np1061_seed0_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
Mon Apr 18 17:36:36 2022
Using weight: weight for classifier: SvB_MA
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 2396428
wDB 114777.240743
wDBn -53.981695
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 128876
wT 4694.203751
wTn -16.779144
nS 645566
nB 2525304
sum_wS 201.702339
sum_wB 119579.407884
nzz =   68867, wzz =   71.9, wzz_SR =   59.2
nzh =  541264, wzh =   92.5, wzh_SR =   77.9
nhh =   35435, whh =   37.3, whh_SR =   33.4
sum_wS_SR 170.521446
sum_wB_SR 50599.001540
sum_wSp_SR 203.140887
sum_wSn_SR 32.619440
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 264.795507
sigScaleZH 394.679756
sigScaleHH 619.280794
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.192839
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_8_np1061_seed0_lr0.01_epochs20_offset0.log
Setup training/validation tensors
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
Tue Apr 19 05:59:16 2022
Using weight: weight for classifier: SvB_MA
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 2396428
wDB 114777.240743
wDBn -53.981695
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 128876
wT 4694.203751
wTn -16.779144
nS 645566
nB 2525304
sum_wS 201.702339
sum_wB 119579.407884
nzz =   68867, wzz =   71.9, wzz_SR =   59.2
nzh =  541264, wzh =   92.5, wzh_SR =   77.9
nhh =   35435, whh =   37.3, whh_SR =   33.4
sum_wS_SR 170.521446
sum_wB_SR 50599.001540
sum_wSp_SR 203.140887
sum_wSn_SR 32.619440
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 264.795507
sigScaleZH 394.679756
sigScaleHH 619.280794
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.192839
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_8_np1061_seed0_lr0.01_epochs20_offset0.log
Setup training/validation tensors
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6286 (44, 10, 17, 32, 17) | 0.871 | 1.025 | 90.25 | 87.01 |------------------------------|
0             Validation | 0.6293 (43, 10, 17, 32, 17) | 0.860 | 1.009 | 90.15 | 87.07 |##############################| ^ (0.2%, 11.41, 8.8, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1032 batches)
0 >>  2/20 <<   Training | 0.5990 (39, 10, 19, 32, 20) | 1.043 | 1.223 | 90.59 | 87.84 |--------------------------------------|
0             Validation | 0.6015 (38, 10, 19, 33, 20) | 1.016 | 1.189 | 90.40 | 87.82 |######################################| ^ (0.2%, 10.64, 9.1, 0%) 
0 >>  3/20 <<   Training | 0.6012 (37, 11, 17, 32, 23) | 1.043 | 1.222 | 90.86 | 87.94 |---------------------------------------|
0             Validation | 0.6035 (36, 11, 17, 32, 22) | 1.016 | 1.189 | 90.79 | 87.87 |######################################| ^ (0.2%, 10.94, 9.6, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (516 batches)
0 >>  4/20 <<   Training | 0.5878 (37, 10, 20, 32, 20) | 1.063 | 1.244 | 90.85 | 88.05 |----------------------------------------|
0             Validation | 0.5910 (37, 10, 20, 33, 20) | 1.042 | 1.216 | 90.50 | 87.99 |#######################################| ^ (0.3%, 10.24, 9.1, 0%) 
0 >>  5/20 <<   Training | 0.5896 (40, 10, 18, 31, 19) | 0.988 | 1.157 | 91.04 | 88.01 |----------------------------------------|
0             Validation | 0.5928 (39, 11, 18, 32, 19) | 0.966 | 1.131 | 90.86 | 87.91 |#######################################| ^ (0.3%, 10.21, 9.5, 0%) 
0 >>  6/20 <<   Training | 0.5926 (34, 10, 19, 34, 23) | 1.059 | 1.242 | 90.91 | 88.10 |-----------------------------------------|
0             Validation | 0.5961 (33, 10, 19, 34, 22) | 1.028 | 1.205 | 90.68 | 88.01 |########################################| ^ (0.3%, 11.76, 8.9, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (258 batches)
0 >>  7/20 <<   Training | 0.5854 (40, 10, 18, 32, 19) | 1.033 | 1.211 | 91.24 | 88.21 |------------------------------------------|
0             Validation | 0.5881 (39, 11, 18, 32, 19) | 1.007 | 1.177 | 91.09 | 88.16 |#########################################| ^ (0.3%, 11.00, 10.0, 0%) 
0 >>  8/20 <<   Training | 0.5871 (36,  9, 18, 34, 21) | 1.095 | 1.283 | 91.36 | 88.20 |-----------------------------------------|
0             Validation | 0.5903 (36, 10, 18, 35, 21) | 1.076 | 1.256 | 91.08 | 88.17 |#########################################| ^ (0.2%, 10.21, 9.0, 0%) 
0 >>  9/20 <<   Training | 0.5860 (38, 10, 18, 33, 21) | 1.044 | 1.225 | 91.26 | 88.18 |-----------------------------------------|
0             Validation | 0.5895 (38, 10, 18, 33, 21) | 1.015 | 1.188 | 91.06 | 88.12 |#########################################| ^ (0.2%, 10.10, 8.5, 0%) 
0 >> 10/20 <<   Training | 0.5909 (40,  8, 17, 34, 20) | 1.087 | 1.278 | 91.25 | 88.29 |------------------------------------------|
0             Validation | 0.5942 (39,  8, 17, 34, 20) | 1.056 | 1.237 | 90.91 | 88.23 |##########################################| ^ (0.3%, 10.95, 10.3, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (129 batches)
0 >> 11/20 <<   Training | 0.5913 (40, 11, 17, 31, 21) | 1.028 | 1.207 | 91.36 | 88.20 |------------------------------------------|
0             Validation | 0.5942 (39, 11, 17, 32, 21) | 1.005 | 1.175 | 91.18 | 88.17 |#########################################| ^ (0.2%, 10.45, 9.7, 0%) 
0 >> 12/20 <<   Training | 0.5827 (36, 10, 18, 33, 22) | 1.097 | 1.285 | 91.32 | 88.40 |-------------------------------------------|
0             Validation | 0.5864 (36, 10, 18, 34, 22) | 1.064 | 1.243 | 91.05 | 88.33 |###########################################| ^ (0.3%, 10.85, 9.5, 0%) 
0 >> 13/20 <<   Training | 0.5851 (37,  9, 18, 32, 22) | 1.071 | 1.255 | 91.33 | 88.18 |-----------------------------------------|
0             Validation | 0.5887 (37,  9, 18, 33, 22) | 1.051 | 1.227 | 91.13 | 88.11 |#########################################| ^ (0.3%, 10.70, 9.3, 0%) 
0 >> 14/20 <<   Training | 0.5844 (40,  9, 17, 34, 20) | 1.081 | 1.267 | 91.39 | 88.38 |-------------------------------------------|
0             Validation | 0.5876 (39,  9, 17, 35, 20) | 1.057 | 1.233 | 91.18 | 88.32 |###########################################| ^ (0.2%, 10.86, 9.5, 0%) 
0 >> 15/20 <<   Training | 0.5828 (41,  9, 18, 30, 20) | 1.062 | 1.245 | 91.18 | 88.32 |-------------------------------------------|
0             Validation | 0.5852 (41,  9, 18, 31, 20) | 1.039 | 1.214 | 90.97 | 88.28 |##########################################| ^ (0.2%, 10.92, 9.6, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.5809 (40,  9, 18, 33, 20) | 1.084 | 1.271 | 91.35 | 88.42 |--------------------------------------------|
0             Validation | 0.5841 (39, 10, 18, 33, 20) | 1.056 | 1.233 | 91.08 | 88.36 |###########################################| ^ (0.3%, 10.80, 9.4, 0%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.5790 (38, 10, 18, 33, 20) | 1.097 | 1.285 | 91.33 | 88.43 |--------------------------------------------|
0             Validation | 0.5825 (38, 10, 18, 33, 20) | 1.068 | 1.246 | 91.08 | 88.36 |###########################################| ^ (0.3%, 10.85, 9.7, 0%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.5802 (40, 10, 18, 32, 20) | 1.087 | 1.274 | 91.40 | 88.43 |--------------------------------------------|
0             Validation | 0.5835 (39, 10, 18, 33, 20) | 1.060 | 1.236 | 91.16 | 88.36 |###########################################| ^ (0.3%, 10.78, 9.8, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.5799 (39, 10, 18, 32, 20) | 1.087 | 1.274 | 91.40 | 88.43 |--------------------------------------------|
0             Validation | 0.5833 (39, 10, 18, 33, 20) | 1.062 | 1.238 | 91.16 | 88.36 |###########################################| ^ (0.3%, 10.78, 9.8, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.5800 (39, 10, 18, 32, 20) | 1.087 | 1.274 | 91.41 | 88.43 |--------------------------------------------|
0             Validation | 0.5834 (39, 10, 18, 33, 20) | 1.061 | 1.238 | 91.17 | 88.36 |###########################################| ^ (0.3%, 10.77, 9.9, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_8_np1061_seed0_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

0 >> DONE <<
0 Minimum Loss = 0.579014
