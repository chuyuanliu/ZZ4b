Tue Apr 19 12:51:35 2022
Using weight: weight for classifier: SvB_MA
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 2396428
wDB 114777.240743
wDBn -53.981695
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 128876
wT 4694.203751
wTn -16.779144
nS 645566
nB 2525304
sum_wS 201.702339
sum_wB 119579.407884
nzz =   68867, wzz =   71.9, wzz_SR =   59.2
nzh =  541264, wzh =   92.5, wzh_SR =   77.9
nhh =   35435, whh =   37.3, whh_SR =   33.4
sum_wS_SR 170.521446
sum_wB_SR 50599.001540
sum_wSp_SR 203.140887
sum_wSn_SR 32.619440
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 264.795507
sigScaleZH 394.679756
sigScaleHH 619.280794
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.192839
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_8_np1061_seed0_lr0.01_epochs20_offset1.log
Setup training/validation tensors
1 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >>  1/20 <<   Training | 0.6355 (38, 10, 18, 34, 20) | 0.899 | 1.058 | 89.55 | 86.74 |---------------------------|
1             Validation | 0.6308 (38, 10, 17, 34, 20) | 0.979 | 1.151 | 90.08 | 86.82 |############################| ^ (0.5%, 8.90, 6.1, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1033 batches)
1 >>  2/20 <<   Training | 0.6217 (31, 10, 19, 37, 22) | 0.950 | 1.119 | 90.06 | 86.92 |-----------------------------|
1             Validation | 0.6167 (32, 10, 18, 37, 22) | 1.043 | 1.227 | 90.54 | 87.01 |##############################| ^ (0.4%, 9.77, 7.0, 0%) 
1 >>  3/20 <<   Training | 0.6116 (43, 10, 15, 32, 20) | 0.920 | 1.080 | 90.74 | 87.81 |--------------------------------------|
1             Validation | 0.6090 (44, 10, 14, 32, 20) | 0.943 | 1.106 | 91.03 | 87.88 |######################################| ^ (0.3%, 9.57, 7.4, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (516 batches)
1 >>  4/20 <<   Training | 0.5985 (40, 10, 17, 32, 20) | 1.030 | 1.206 | 90.68 | 88.06 |----------------------------------------|
1             Validation | 0.5958 (41, 10, 16, 32, 21) | 1.100 | 1.286 | 91.06 | 88.07 |########################################| ^ (0.2%, 8.97, 6.3, 0%) 
1 >>  5/20 <<   Training | 0.5948 (39,  9, 18, 32, 22) | 1.050 | 1.230 | 90.87 | 88.10 |----------------------------------------|
1             Validation | 0.5905 (39,  9, 17, 33, 22) | 1.121 | 1.312 | 91.33 | 88.19 |#########################################| ^ (0.4%, 10.78, 8.1, 0%) 
1 >>  6/20 <<   Training | 0.5980 (37,  9, 18, 35, 21) | 1.052 | 1.231 | 90.48 | 88.04 |----------------------------------------|
1             Validation | 0.5947 (37,  9, 17, 35, 21) | 1.155 | 1.352 | 90.81 | 88.13 |#########################################| ^ (0.4%, 10.89, 6.7, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (258 batches)
1 >>  7/20 <<   Training | 0.5941 (39, 10, 18, 32, 21) | 1.032 | 1.206 | 90.62 | 88.16 |-----------------------------------------|
1             Validation | 0.5905 (40,  9, 17, 32, 21) | 1.128 | 1.318 | 91.06 | 88.24 |##########################################| ^ (0.3%, 10.94, 6.1, 0%) 
1 >>  8/20 <<   Training | 0.5925 (39, 10, 20, 32, 19) | 1.055 | 1.237 | 90.36 | 88.12 |-----------------------------------------|
1             Validation | 0.5883 (40,  9, 18, 32, 19) | 1.117 | 1.306 | 90.75 | 88.22 |##########################################| ^ (0.4%, 10.58, 7.9, 0%) 
1 >>  9/20 <<   Training | 0.5929 (41, 10, 17, 31, 20) | 0.997 | 1.168 | 90.96 | 88.04 |----------------------------------------|
1             Validation | 0.5895 (41, 10, 16, 31, 20) | 1.070 | 1.254 | 91.39 | 88.09 |########################################| ^ (0.3%, 10.18, 6.7, 0%) 
1 >> 10/20 <<   Training | 0.5906 (40,  9, 18, 33, 19) | 1.042 | 1.218 | 90.82 | 88.17 |-----------------------------------------|
1             Validation | 0.5873 (40,  9, 17, 33, 20) | 1.140 | 1.333 | 91.24 | 88.22 |##########################################| ^ (0.3%, 10.35, 6.8, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (129 batches)
1 >> 11/20 <<   Training | 0.5948 (37, 10, 16, 35, 22) | 1.078 | 1.260 | 91.02 | 88.25 |------------------------------------------|
1             Validation | 0.5917 (37, 10, 16, 35, 22) | 1.166 | 1.365 | 91.42 | 88.32 |###########################################| ^ (0.3%, 10.35, 6.9, 0%) 
1 >> 12/20 <<   Training | 0.5909 (41,  9, 17, 31, 20) | 1.033 | 1.209 | 90.84 | 88.27 |------------------------------------------|
1             Validation | 0.5878 (41,  9, 16, 31, 21) | 1.105 | 1.292 | 91.27 | 88.35 |###########################################| ^ (0.3%, 10.03, 6.6, 0%) 
1 >> 13/20 <<   Training | 0.5862 (39, 10, 18, 31, 21) | 1.062 | 1.242 | 90.93 | 88.28 |------------------------------------------|
1             Validation | 0.5822 (39, 10, 17, 32, 21) | 1.154 | 1.350 | 91.36 | 88.36 |###########################################| ^ (0.3%, 10.34, 6.4, 0%) 
1 >> 14/20 <<   Training | 0.5885 (38,  9, 18, 33, 21) | 1.057 | 1.238 | 90.96 | 88.27 |------------------------------------------|
1             Validation | 0.5844 (39,  9, 17, 33, 21) | 1.136 | 1.329 | 91.36 | 88.40 |###########################################| ^ (0.4%, 10.38, 7.1, 0%) 
1 >> 15/20 <<   Training | 0.5889 (41, 10, 18, 31, 19) | 0.996 | 1.166 | 90.93 | 88.26 |------------------------------------------|
1             Validation | 0.5855 (41, 10, 17, 32, 20) | 1.058 | 1.239 | 91.36 | 88.34 |###########################################| ^ (0.3%, 10.61, 6.5, 0%) 
Decay learning rate: 0.010000 -> 0.002500
1 >> 16/20 <<   Training | 0.5868 (41, 10, 18, 32, 19) | 1.021 | 1.195 | 90.97 | 88.32 |-------------------------------------------|
1             Validation | 0.5832 (41, 10, 17, 32, 20) | 1.106 | 1.294 | 91.41 | 88.40 |###########################################| ^ (0.3%, 10.47, 6.6, 0%) 
Decay learning rate: 0.002500 -> 0.000625
1 >> 17/20 <<   Training | 0.5853 (39, 10, 18, 33, 20) | 1.053 | 1.231 | 91.03 | 88.34 |-------------------------------------------|
1             Validation | 0.5817 (40, 10, 17, 33, 20) | 1.146 | 1.340 | 91.44 | 88.43 |############################################| ^ (0.3%, 10.35, 6.5, 0%) 
Decay learning rate: 0.000625 -> 0.000156
1 >> 18/20 <<   Training | 0.5860 (40, 10, 18, 32, 20) | 1.048 | 1.225 | 91.07 | 88.35 |-------------------------------------------|
1             Validation | 0.5826 (40, 10, 17, 32, 20) | 1.141 | 1.334 | 91.48 | 88.43 |############################################| ^ (0.3%, 10.19, 6.1, 0%) 
Decay learning rate: 0.000156 -> 0.000039
1 >> 19/20 <<   Training | 0.5855 (39, 10, 18, 32, 20) | 1.054 | 1.232 | 91.06 | 88.35 |-------------------------------------------|
1             Validation | 0.5821 (40, 10, 17, 33, 20) | 1.150 | 1.345 | 91.47 | 88.43 |############################################| ^ (0.3%, 10.23, 6.1, 0%) 
Decay learning rate: 0.000039 -> 0.000010
1 >> 20/20 <<   Training | 0.5856 (39, 10, 18, 32, 20) | 1.054 | 1.232 | 91.06 | 88.35 |-------------------------------------------|
1             Validation | 0.5821 (40, 10, 17, 33, 20) | 1.150 | 1.345 | 91.47 | 88.43 |############################################| ^ (0.3%, 10.23, 6.1, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_8_np1061_seed0_lr0.01_epochs20_offset1_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

1 >> DONE <<
1 Minimum Loss = 0.585295
Mon May  9 14:37:26 2022
Using weight: weight for classifier: SvB_MA
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 3279028
wDB 147993.557827
wDBn -59.208815
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 179878
wT 6758.331864
wTn -23.581879
nS 940327
nB 3458906
sum_wS 167.472288
sum_wB 154870.307320
nzz =  323399, wzz =   57.4, wzz_SR =   46.2
nzh =  574169, wzh =   74.7, wzh_SR =   61.9
nhh =   42759, whh =   35.4, whh_SR =   31.2
sum_wS_SR 139.340355
sum_wB_SR 57305.889757
sum_wSp_SR 164.693187
sum_wSn_SR 25.352832
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 295.764663
sigScaleZH 457.493368
sigScaleHH 679.928873
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.099681
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_8_np1061_seed0_lr0.01_epochs20_offset1.log
Setup training/validation tensors
1 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) |  S/B   | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >>  1/20 <<   Training | 0.6160 (37, 13, 14, 27, 19) |  9/158 | 0.958 | 90.42 | 88.71 |-----------------------------------------------|
1             Validation | 0.6175 (36, 13, 14, 27, 20) |  9/157 | 0.949 | 90.31 | 88.65 |##############################################| ^ (0.2%, 8.72, 9.6, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1432 batches)
1 >>  2/20 <<   Training | 0.5925 (37, 13, 15, 27, 18) | 17/376 | 0.966 | 89.94 | 89.32 |-----------------------------------------------------|
1             Validation | 0.5938 (37, 13, 15, 27, 18) | 17/361 | 0.983 | 89.73 | 89.28 |####################################################| ^ (0.2%, 8.83, 8.7, 0%) 
1 >>  3/20 <<   Training | 0.5940 (37, 14, 14, 25, 20) | 16/304 | 1.019 | 90.41 | 89.52 |-------------------------------------------------------|
1             Validation | 0.5957 (37, 13, 14, 25, 20) | 16/311 | 1.007 | 90.26 | 89.45 |######################################################| ^ (0.2%, 9.28, 10.1, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (716 batches)
1 >>  4/20 <<   Training | 0.5822 (38, 11, 14, 28, 18) | 19/371 | 1.046 | 90.61 | 89.70 |--------------------------------------------------------|
1             Validation | 0.5833 (38, 11, 14, 28, 18) | 19/382 | 1.035 | 90.55 | 89.64 |########################################################| ^ (0.2%, 9.68, 9.4, 0%) 
1 >>  5/20 <<   Training | 0.5787 (34, 12, 16, 29, 19) | 13/232 | 1.058 | 90.24 | 89.66 |--------------------------------------------------------|
1             Validation | 0.5796 (34, 11, 16, 29, 19) | 14/218 | 1.079 | 90.11 | 89.62 |########################################################| ^ (0.1%, 9.03, 10.2, 0%) 
1 >>  6/20 <<   Training | 0.5860 (41, 11, 14, 26, 18) | 20/416 | 1.002 | 90.45 | 89.65 |--------------------------------------------------------|
1             Validation | 0.5880 (41, 11, 14, 26, 18) | 20/430 | 0.987 | 90.31 | 89.56 |#######################################################| ^ (0.2%, 9.15, 11.8, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (358 batches)
1 >>  7/20 <<   Training | 0.5715 (34, 11, 16, 30, 19) | 11/168 | 1.093 | 90.61 | 89.81 |----------------------------------------------------------|
1             Validation | 0.5731 (34, 11, 16, 30, 18) | 11/164 | 1.093 | 90.51 | 89.72 |#########################################################| ^ (0.2%, 9.19, 8.8, 0%) 
1 >>  8/20 <<   Training | 0.5725 (37, 11, 15, 28, 20) | 15/243 | 1.100 | 90.87 | 89.86 |----------------------------------------------------------|
1             Validation | 0.5738 (37, 11, 15, 28, 20) | 15/235 | 1.108 | 90.74 | 89.81 |##########################################################| ^ (0.2%, 9.04, 10.4, 0%) 
1 >>  9/20 <<   Training | 0.5747 (36, 11, 15, 29, 19) | 11/138 | 1.132 | 90.55 | 89.81 |----------------------------------------------------------|
1             Validation | 0.5762 (36, 11, 15, 29, 19) | 11/139 | 1.127 | 90.38 | 89.74 |#########################################################| ^ (0.2%, 9.23, 9.6, 0%) 
1 >> 10/20 <<   Training | 0.5688 (34, 12, 16, 30, 18) | 13/210 | 1.088 | 90.82 | 89.70 |---------------------------------------------------------|
1             Validation | 0.5703 (34, 11, 17, 30, 18) | 13/199 | 1.100 | 90.71 | 89.65 |########################################################| ^ (0.2%, 8.92, 10.3, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (179 batches)
1 >> 11/20 <<   Training | 0.5682 (34, 11, 16, 29, 20) | 11/140 | 1.154 | 90.81 | 89.93 |-----------------------------------------------------------|
1             Validation | 0.5694 (34, 11, 16, 30, 20) | 11/140 | 1.147 | 90.69 | 89.88 |##########################################################| ^ (0.2%, 8.75, 8.7, 0%) 
1 >> 12/20 <<   Training | 0.5716 (36, 11, 14, 28, 19) | 16/271 | 1.107 | 90.93 | 89.97 |-----------------------------------------------------------|
1             Validation | 0.5734 (36, 11, 15, 29, 19) | 17/271 | 1.103 | 90.82 | 89.89 |##########################################################| ^ (0.2%, 8.70, 10.3, 0%) 
1 >> 13/20 <<   Training | 0.5716 (32, 12, 15, 29, 22) |  9/108 | 1.169 | 90.91 | 89.94 |-----------------------------------------------------------|
1             Validation | 0.5734 (32, 12, 16, 29, 22) |  9/ 99 | 1.185 | 90.79 | 89.86 |##########################################################| ^ (0.2%, 8.46, 8.3, 0%) 
1 >> 14/20 <<   Training | 0.5665 (35, 12, 16, 29, 18) | 13/190 | 1.121 | 90.82 | 89.92 |-----------------------------------------------------------|
1             Validation | 0.5678 (35, 11, 16, 29, 18) | 13/184 | 1.124 | 90.71 | 89.86 |##########################################################| ^ (0.2%, 8.35, 8.9, 0%) 
1 >> 15/20 <<   Training | 0.5675 (35, 11, 16, 29, 18) | 13/175 | 1.134 | 90.68 | 89.93 |-----------------------------------------------------------|
1             Validation | 0.5688 (35, 11, 16, 29, 18) | 13/175 | 1.130 | 90.55 | 89.87 |##########################################################| ^ (0.2%, 8.80, 8.9, 0%) 
Decay learning rate: 0.010000 -> 0.002500
1 >> 16/20 <<   Training | 0.5670 (36, 11, 15, 29, 19) | 14/222 | 1.120 | 90.93 | 89.97 |-----------------------------------------------------------|
1             Validation | 0.5685 (36, 11, 16, 29, 19) | 15/218 | 1.121 | 90.82 | 89.90 |##########################################################| ^ (0.2%, 8.47, 9.1, 0%) 
Decay learning rate: 0.002500 -> 0.000625
1 >> 17/20 <<   Training | 0.5664 (35, 11, 15, 29, 19) | 13/189 | 1.148 | 90.95 | 90.00 |-----------------------------------------------------------|
1             Validation | 0.5679 (35, 11, 16, 29, 20) | 13/187 | 1.144 | 90.84 | 89.93 |###########################################################| ^ (0.2%, 8.58, 9.2, 0%) 
Decay learning rate: 0.000625 -> 0.000156
1 >> 18/20 <<   Training | 0.5664 (35, 11, 15, 29, 19) | 14/206 | 1.138 | 90.95 | 90.00 |------------------------------------------------------------|
1             Validation | 0.5679 (35, 11, 16, 29, 19) | 14/199 | 1.142 | 90.83 | 89.93 |###########################################################| ^ (0.2%, 8.58, 8.7, 0%) 
Decay learning rate: 0.000156 -> 0.000039
1 >> 19/20 <<   Training | 0.5664 (35, 11, 15, 29, 19) | 14/205 | 1.138 | 90.94 | 90.00 |------------------------------------------------------------|
1             Validation | 0.5679 (35, 11, 16, 29, 19) | 14/200 | 1.141 | 90.83 | 89.94 |###########################################################| ^ (0.2%, 8.58, 9.0, 0%) 
Decay learning rate: 0.000039 -> 0.000010
1 >> 20/20 <<   Training | 0.5664 (35, 11, 15, 29, 19) | 14/207 | 1.136 | 90.94 | 90.00 |------------------------------------------------------------|
1             Validation | 0.5679 (35, 11, 16, 29, 19) | 14/201 | 1.141 | 90.82 | 89.93 |###########################################################| ^ (0.2%, 8.58, 9.0, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_8_np1061_seed0_lr0.01_epochs20_offset1_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

1 >> DONE <<
1 Minimum Loss = 0.566414
