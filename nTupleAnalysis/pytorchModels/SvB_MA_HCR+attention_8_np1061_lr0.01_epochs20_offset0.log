0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6466 (37, 10, 24, 28, 16) | 0.937 | 1.076 | 89.93 | 86.09 |--------------------|
0             Validation | 0.6469 (37, 10, 24, 28, 16) | 0.944 | 1.083 | 89.82 | 86.13 |#####################| ^ (0.4%, 7.93, 2.0, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1008 batches)
0 >>  2/20 <<   Training | 0.6203 (30, 11, 27, 31, 17) | 1.021 | 1.166 | 90.52 | 86.94 |-----------------------------|
0             Validation | 0.6202 (30, 11, 27, 31, 17) | 1.026 | 1.170 | 90.39 | 87.06 |##############################| ^ (0.4%, 9.91, 2.3, 0%) 
0 >>  3/20 <<   Training | 0.6128 (34, 10, 27, 29, 15) | 1.047 | 1.197 | 90.45 | 87.25 |--------------------------------|
0             Validation | 0.6129 (34, 10, 27, 29, 15) | 1.056 | 1.208 | 90.44 | 87.34 |#################################| ^ (0.4%, 8.85, 1.9, 1%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (504 batches)
0 >>  4/20 <<   Training | 0.6093 (38, 12, 21, 29, 16) | 1.038 | 1.187 | 90.96 | 87.31 |---------------------------------|
0             Validation | 0.6102 (38, 12, 21, 28, 16) | 1.044 | 1.194 | 90.83 | 87.38 |#################################| ^ (0.3%, 8.78, 1.8, 1%) 
0 >>  5/20 <<   Training | 0.6043 (34, 10, 25, 30, 16) | 1.075 | 1.228 | 90.83 | 87.43 |----------------------------------|
0             Validation | 0.6047 (34, 10, 25, 30, 16) | 1.082 | 1.233 | 90.79 | 87.51 |###################################| ^ (0.4%, 9.64, 1.8, 1%) 
0 >>  6/20 <<   Training | 0.6130 (39, 10, 21, 29, 16) | 1.020 | 1.167 | 90.70 | 87.35 |---------------------------------|
0             Validation | 0.6135 (39, 10, 21, 28, 16) | 1.012 | 1.158 | 90.57 | 87.46 |##################################| ^ (0.4%, 10.40, 1.9, 1%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (252 batches)
0 >>  7/20 <<   Training | 0.6013 (31, 11, 27, 30, 16) | 1.100 | 1.256 | 90.90 | 87.58 |-----------------------------------|
0             Validation | 0.6023 (31, 11, 27, 30, 16) | 1.091 | 1.250 | 90.78 | 87.61 |####################################| ^ (0.3%, 9.90, 2.2, 0%) 
0 >>  8/20 <<   Training | 0.5985 (35, 10, 24, 29, 17) | 1.093 | 1.247 | 91.32 | 87.62 |------------------------------------|
0             Validation | 0.5996 (35, 10, 24, 28, 17) | 1.105 | 1.262 | 91.13 | 87.67 |####################################| ^ (0.3%, 9.29, 1.5, 6%) 
0 >>  9/20 <<   Training | 0.5986 (32, 11, 25, 31, 17) | 1.112 | 1.269 | 91.12 | 87.64 |------------------------------------|
0             Validation | 0.5996 (32, 11, 25, 30, 17) | 1.122 | 1.279 | 91.02 | 87.69 |####################################| ^ (0.3%, 9.50, 1.6, 3%) 
0 >> 10/20 <<   Training | 0.5986 (32, 11, 26, 31, 16) | 1.105 | 1.260 | 91.00 | 87.65 |------------------------------------|
0             Validation | 0.5996 (31, 11, 26, 30, 16) | 1.113 | 1.270 | 90.93 | 87.70 |####################################| ^ (0.3%, 9.68, 1.6, 3%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (126 batches)
0 >> 11/20 <<   Training | 0.5972 (39,  9, 25, 27, 15) | 1.073 | 1.225 | 91.18 | 87.70 |-------------------------------------|
0             Validation | 0.5990 (39,  9, 25, 27, 15) | 1.066 | 1.216 | 91.04 | 87.74 |#####################################| ^ (0.3%, 10.04, 1.4, 10%) 
0 >> 12/20 <<   Training | 0.5973 (35, 10, 23, 32, 16) | 1.079 | 1.233 | 91.32 | 87.61 |------------------------------------|
0             Validation | 0.5995 (34, 10, 23, 31, 16) | 1.082 | 1.235 | 91.12 | 87.60 |###################################| ^ (0.3%, 10.43, 1.8, 1%) 
0 >> 13/20 <<   Training | 0.5973 (37, 11, 23, 27, 17) | 1.070 | 1.222 | 91.37 | 87.69 |------------------------------------|
0             Validation | 0.5990 (37, 11, 23, 27, 17) | 1.075 | 1.227 | 91.19 | 87.72 |#####################################| ^ (0.3%, 9.73, 1.9, 1%) 
0 >> 14/20 <<   Training | 0.5968 (32, 10, 27, 29, 16) | 1.108 | 1.265 | 91.03 | 87.68 |------------------------------------|
0             Validation | 0.5985 (32, 10, 28, 29, 16) | 1.103 | 1.258 | 90.85 | 87.74 |#####################################| ^ (0.3%, 10.33, 1.9, 0%) 
0 >> 15/20 <<   Training | 0.5954 (38, 10, 23, 28, 16) | 1.099 | 1.254 | 91.35 | 87.73 |-------------------------------------|
0             Validation | 0.5967 (38, 10, 23, 28, 16) | 1.107 | 1.263 | 91.21 | 87.76 |#####################################| ^ (0.3%, 9.25, 1.4, 9%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.5930 (36, 10, 24, 29, 16) | 1.103 | 1.258 | 91.29 | 87.77 |-------------------------------------|
0             Validation | 0.5952 (36, 10, 25, 29, 16) | 1.104 | 1.259 | 91.13 | 87.76 |#####################################| ^ (0.2%, 9.84, 1.9, 1%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.5916 (36, 10, 24, 29, 16) | 1.108 | 1.264 | 91.39 | 87.83 |--------------------------------------|
0             Validation | 0.5937 (36, 10, 24, 29, 16) | 1.120 | 1.278 | 91.22 | 87.84 |######################################| ^ (0.2%, 10.00, 1.3, 11%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.5914 (36, 10, 24, 29, 16) | 1.110 | 1.267 | 91.37 | 87.83 |--------------------------------------|
0             Validation | 0.5935 (35, 10, 25, 29, 16) | 1.122 | 1.280 | 91.20 | 87.84 |######################################| ^ (0.2%, 10.03, 1.4, 10%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.5914 (35, 10, 24, 29, 16) | 1.118 | 1.276 | 91.36 | 87.83 |--------------------------------------|
0             Validation | 0.5935 (35, 10, 25, 29, 16) | 1.126 | 1.285 | 91.19 | 87.84 |######################################| ^ (0.2%, 10.05, 1.4, 8%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.5914 (35, 10, 24, 29, 16) | 1.115 | 1.273 | 91.36 | 87.83 |--------------------------------------|
0             Validation | 0.5935 (35, 10, 25, 29, 16) | 1.130 | 1.289 | 91.20 | 87.84 |######################################| ^ (0.2%, 10.04, 1.4, 8%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_8_np1061_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6763 (43, 14, 20, 26, 13) | 0.892 | 1.041 | 89.59 | 85.94 |-------------------|
0             Validation | 0.6739 (43, 14, 19, 26, 13) | 0.882 | 1.032 | 89.69 | 86.00 |####################| ^ (0.4%, 10.41, 1.0, 45%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (961 batches)
0 >>  2/20 <<   Training | 0.6238 (37, 10, 23, 29, 15) | 1.058 | 1.229 | 90.71 | 86.90 |----------------------------|
0             Validation | 0.6226 (37, 10, 23, 29, 15) | 1.054 | 1.228 | 90.87 | 86.93 |#############################| ^ (0.2%, 12.08, 1.6, 2%) 
0 >>  3/20 <<   Training | 0.6245 (38, 12, 22, 27, 15) | 1.039 | 1.202 | 90.57 | 86.83 |----------------------------|
0             Validation | 0.6225 (38, 12, 22, 27, 15) | 1.029 | 1.195 | 90.82 | 86.87 |############################| ^ (0.3%, 13.80, 1.5, 4%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (480 batches)
0 >>  4/20 <<   Training | 0.6145 (32, 11, 29, 29, 15) | 1.110 | 1.285 | 90.38 | 87.13 |-------------------------------|
0             Validation | 0.6145 (32, 11, 28, 29, 15) | 1.115 | 1.297 | 90.54 | 87.12 |###############################| ^ (0.3%, 12.60, 2.6, 0%) 
0 >>  5/20 <<   Training | 0.6235 (31,  9, 30, 30, 15) | 1.078 | 1.251 | 89.53 | 86.95 |-----------------------------|
0             Validation | 0.6234 (31,  9, 29, 30, 15) | 1.073 | 1.249 | 89.62 | 86.95 |#############################| ^ (0.2%, 14.57, 1.7, 2%) 
0 >>  6/20 <<   Training | 0.6116 (38, 10, 25, 29, 14) | 1.065 | 1.236 | 90.64 | 87.09 |------------------------------|
0             Validation | 0.6117 (38, 10, 24, 29, 14) | 1.039 | 1.212 | 90.68 | 87.11 |###############################| ^ (0.2%, 14.36, 1.5, 5%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (240 batches)
0 >>  7/20 <<   Training | 0.6034 (36, 11, 25, 29, 14) | 1.100 | 1.275 | 90.92 | 87.44 |----------------------------------|
0             Validation | 0.6037 (36, 11, 25, 30, 14) | 1.108 | 1.288 | 90.90 | 87.44 |##################################| ^ (0.2%, 12.46, 1.8, 1%) 
0 >>  8/20 <<   Training | 0.6096 (32,  8, 28, 32, 15) | 1.130 | 1.307 | 90.56 | 87.39 |---------------------------------|
0             Validation | 0.6103 (32,  8, 28, 32, 15) | 1.109 | 1.288 | 90.58 | 87.37 |#################################| ^ (0.2%, 12.82, 1.4, 7%) 
0 >>  9/20 <<   Training | 0.6036 (33, 10, 24, 32, 16) | 1.128 | 1.301 | 90.94 | 87.39 |---------------------------------|
0             Validation | 0.6043 (33, 10, 24, 32, 16) | 1.127 | 1.307 | 90.92 | 87.36 |#################################| ^ (0.3%, 12.18, 1.7, 2%) 
0 >> 10/20 <<   Training | 0.6042 (38, 10, 21, 30, 16) | 1.058 | 1.225 | 91.24 | 87.35 |---------------------------------|
0             Validation | 0.6046 (38, 10, 21, 30, 16) | 1.066 | 1.238 | 91.30 | 87.35 |#################################| ^ (0.3%, 13.53, 1.1, 26%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (120 batches)
0 >> 11/20 <<   Training | 0.5975 (37, 11, 24, 29, 15) | 1.132 | 1.308 | 91.21 | 87.61 |------------------------------------|
0             Validation | 0.5983 (37, 11, 24, 29, 15) | 1.117 | 1.296 | 91.22 | 87.58 |###################################| ^ (0.2%, 12.76, 1.6, 4%) 
0 >> 12/20 <<   Training | 0.6019 (30, 11, 26, 33, 16) | 1.171 | 1.352 | 91.08 | 87.52 |-----------------------------------|
0             Validation | 0.6026 (30, 11, 26, 33, 16) | 1.163 | 1.347 | 91.10 | 87.49 |##################################| ^ (0.3%, 13.37, 0.9, 58%) 
0 >> 13/20 <<   Training | 0.5985 (33, 10, 27, 31, 15) | 1.151 | 1.331 | 91.02 | 87.56 |-----------------------------------|
0             Validation | 0.5997 (33, 10, 27, 31, 15) | 1.139 | 1.323 | 90.97 | 87.52 |###################################| ^ (0.2%, 14.02, 1.7, 2%) 
0 >> 14/20 <<   Training | 0.6018 (33, 10, 28, 30, 15) | 1.130 | 1.306 | 90.58 | 87.52 |-----------------------------------|
0             Validation | 0.6030 (33, 10, 28, 30, 15) | 1.110 | 1.289 | 90.50 | 87.49 |##################################| ^ (0.2%, 14.28, 1.9, 1%) 
0 >> 15/20 <<   Training | 0.5975 (35, 10, 25, 30, 15) | 1.133 | 1.309 | 91.05 | 87.60 |-----------------------------------|
0             Validation | 0.5986 (35, 10, 25, 30, 15) | 1.156 | 1.344 | 90.99 | 87.56 |###################################| ^ (0.2%, 14.27, 1.3, 15%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.5949 (35, 10, 25, 30, 14) | 1.148 | 1.326 | 91.23 | 87.65 |------------------------------------|
0             Validation | 0.5961 (35, 10, 25, 30, 14) | 1.150 | 1.335 | 91.16 | 87.62 |####################################| ^ (0.2%, 13.84, 1.2, 19%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.5938 (36, 10, 25, 30, 15) | 1.157 | 1.336 | 91.28 | 87.70 |-------------------------------------|
0             Validation | 0.5950 (35, 10, 25, 30, 15) | 1.146 | 1.330 | 91.23 | 87.66 |####################################| ^ (0.2%, 13.64, 1.5, 6%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.5937 (35, 10, 25, 30, 15) | 1.156 | 1.336 | 91.26 | 87.71 |-------------------------------------|
0             Validation | 0.5948 (35, 10, 25, 30, 15) | 1.149 | 1.333 | 91.20 | 87.67 |####################################| ^ (0.2%, 13.59, 1.4, 8%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.5936 (36, 10, 25, 30, 15) | 1.154 | 1.333 | 91.27 | 87.71 |-------------------------------------|
0             Validation | 0.5948 (35, 10, 25, 30, 15) | 1.149 | 1.333 | 91.22 | 87.67 |####################################| ^ (0.2%, 13.59, 1.4, 8%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.5936 (36, 10, 25, 30, 15) | 1.155 | 1.334 | 91.27 | 87.71 |-------------------------------------|
0             Validation | 0.5948 (35, 10, 25, 30, 15) | 1.149 | 1.333 | 91.21 | 87.67 |####################################| ^ (0.2%, 13.61, 1.5, 6%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_8_np1061_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
