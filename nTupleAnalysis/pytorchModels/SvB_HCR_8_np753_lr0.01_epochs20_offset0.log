0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7175 (32, 10,  8, 10, 22) | 6.805 | 7.119 | 83.93 | 89.49 |------------------------------------------------------|
0             Validation | 0.7163 (32, 10,  8, 10, 22) | 6.812 | 7.114 | 83.77 | 89.52 |#######################################################| ^ (0.2%, 19.84, 1.5, 6%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1008 batches)
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7135 (37, 11, 25, 28, 12) | 0.665 | 0.843 | 85.85 | 82.90 ||
0             Validation | 0.7117 (37, 11, 25, 28, 12) | 0.666 | 0.840 | 85.81 | 83.17 || ^ (0.8%, 9.15, 1.8, 1%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1008 batches)
0 >>  2/20 <<   Training | 0.6718 (32, 10, 26, 31, 14) | 0.996 | 1.144 | 86.76 | 85.37 |-------------|
0             Validation | 0.6704 (32, 10, 26, 31, 14) | 0.981 | 1.124 | 86.67 | 85.53 |###############| ^ (0.5%, 8.99, 1.5, 4%) 
0 >>  3/20 <<   Training | 0.6748 (35, 11, 24, 30, 14) | 0.983 | 1.139 | 86.58 | 85.28 |------------|
0             Validation | 0.6745 (34, 11, 24, 29, 14) | 0.997 | 1.154 | 86.32 | 85.46 |##############| ^ (0.6%, 9.77, 2.2, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (504 batches)
0 >>  4/20 <<   Training | 0.6621 (33, 12, 24, 30, 15) | 1.072 | 1.228 | 87.30 | 85.74 |-----------------|
0             Validation | 0.6625 (33, 12, 24, 30, 15) | 1.063 | 1.217 | 87.13 | 85.85 |##################| ^ (0.4%, 9.31, 2.3, 0%) 
0 >>  5/20 <<   Training | 0.6605 (34, 11, 25, 30, 14) | 1.076 | 1.230 | 87.41 | 85.70 |----------------|
0             Validation | 0.6614 (34, 11, 25, 29, 14) | 1.088 | 1.244 | 87.16 | 85.78 |#################| ^ (0.3%, 10.14, 1.5, 6%) 
0 >>  6/20 <<   Training | 0.6603 (35, 11, 25, 29, 14) | 1.079 | 1.238 | 87.67 | 85.85 |------------------|
0             Validation | 0.6606 (35, 11, 25, 28, 14) | 1.075 | 1.234 | 87.49 | 85.99 |###################| ^ (0.4%, 9.15, 1.4, 8%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (252 batches)
0 >>  7/20 <<   Training | 0.6570 (35, 11, 23, 31, 14) | 1.111 | 1.272 | 87.72 | 86.02 |--------------------|
0             Validation | 0.6576 (35, 11, 23, 30, 14) | 1.120 | 1.281 | 87.55 | 86.11 |#####################| ^ (0.3%, 9.10, 1.8, 1%) 
0 >>  8/20 <<   Training | 0.6555 (32, 12, 27, 30, 14) | 1.113 | 1.270 | 87.54 | 85.93 |-------------------|
0             Validation | 0.6559 (32, 12, 27, 29, 14) | 1.123 | 1.285 | 87.27 | 86.05 |####################| ^ (0.4%, 9.11, 2.1, 0%) 
0 >>  9/20 <<   Training | 0.6543 (32, 11, 26, 30, 14) | 1.124 | 1.284 | 87.65 | 86.04 |--------------------|
0             Validation | 0.6544 (32, 11, 26, 29, 14) | 1.134 | 1.296 | 87.39 | 86.17 |#####################| ^ (0.4%, 9.47, 1.5, 4%) 
0 >> 10/20 <<   Training | 0.6531 (32, 12, 26, 30, 14) | 1.124 | 1.284 | 87.68 | 86.09 |--------------------|
0             Validation | 0.6542 (31, 12, 26, 29, 14) | 1.137 | 1.298 | 87.34 | 86.18 |#####################| ^ (0.3%, 9.96, 2.2, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (126 batches)
0 >> 11/20 <<   Training | 0.6524 (36, 11, 24, 29, 13) | 1.142 | 1.303 | 87.89 | 86.13 |---------------------|
0             Validation | 0.6537 (36, 11, 25, 28, 13) | 1.144 | 1.307 | 87.58 | 86.21 |######################| ^ (0.3%, 9.42, 1.3, 12%) 
0 >> 12/20 <<   Training | 0.6518 (35, 11, 24, 29, 14) | 1.148 | 1.310 | 87.96 | 86.10 |---------------------|
0             Validation | 0.6529 (35, 11, 24, 29, 14) | 1.149 | 1.311 | 87.59 | 86.19 |#####################| ^ (0.3%, 9.02, 1.6, 3%) 
0 >> 13/20 <<   Training | 0.6510 (33, 12, 26, 29, 14) | 1.148 | 1.310 | 87.81 | 86.17 |---------------------|
0             Validation | 0.6523 (33, 12, 26, 28, 14) | 1.153 | 1.317 | 87.47 | 86.24 |######################| ^ (0.3%, 9.97, 2.4, 0%) 
0 >> 14/20 <<   Training | 0.6512 (33, 11, 26, 30, 14) | 1.148 | 1.310 | 87.79 | 86.14 |---------------------|
0             Validation | 0.6517 (33, 11, 26, 30, 14) | 1.146 | 1.309 | 87.55 | 86.25 |######################| ^ (0.4%, 9.84, 2.0, 0%) 
0 >> 15/20 <<   Training | 0.6517 (34, 11, 25, 30, 14) | 1.131 | 1.293 | 87.94 | 86.10 |---------------------|
0             Validation | 0.6529 (34, 11, 25, 30, 14) | 1.126 | 1.289 | 87.65 | 86.20 |######################| ^ (0.4%, 9.41, 1.9, 1%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6494 (33, 12, 25, 30, 14) | 1.151 | 1.314 | 87.92 | 86.20 |---------------------|
0             Validation | 0.6504 (33, 12, 25, 29, 14) | 1.148 | 1.309 | 87.61 | 86.30 |######################| ^ (0.4%, 9.52, 1.9, 1%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6491 (33, 11, 25, 30, 14) | 1.157 | 1.322 | 87.92 | 86.22 |----------------------|
0             Validation | 0.6501 (33, 11, 26, 29, 14) | 1.155 | 1.321 | 87.60 | 86.32 |#######################| ^ (0.3%, 9.63, 1.9, 1%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6489 (34, 11, 25, 29, 14) | 1.158 | 1.323 | 87.95 | 86.23 |----------------------|
0             Validation | 0.6500 (34, 11, 25, 29, 14) | 1.155 | 1.320 | 87.63 | 86.32 |#######################| ^ (0.3%, 9.60, 1.8, 1%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6489 (34, 11, 25, 29, 14) | 1.159 | 1.323 | 87.94 | 86.23 |----------------------|
0             Validation | 0.6500 (34, 11, 25, 29, 14) | 1.157 | 1.322 | 87.62 | 86.32 |#######################| ^ (0.3%, 9.62, 1.9, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6489 (34, 11, 25, 29, 14) | 1.159 | 1.323 | 87.94 | 86.23 |----------------------|
0             Validation | 0.6500 (34, 11, 25, 29, 14) | 1.157 | 1.322 | 87.62 | 86.32 |#######################| ^ (0.3%, 9.62, 1.9, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_lr0.01_epochs20_offset0_epoch20.pkl
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7135 (37, 11, 25, 28, 12) | 0.665 | 0.843 | 85.85 | 82.90 ||
0             Validation | 0.7117 (37, 11, 25, 28, 12) | 0.666 | 0.840 | 85.81 | 83.17 || ^ (0.8%, 9.15, 1.8, 1%) 0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6937 (41, 11, 23, 26, 12) | 0.412 | 0.482 | 87.55 | 84.15 |-|
0             Validation | 0.6928 (42, 11, 23, 26, 13) | 0.431 | 0.503 | 87.78 | 84.25 |##| ^ (0.4%, 11.40, 1.7, 2%) 0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6937 (41, 11, 23, 26, 12) | 0.412 | 0.482 | 87.55 | 84.15 |-|
0             Validation | 0.6928 (42, 11, 23, 26, 13) | 0.431 | 0.503 | 87.78 | 84.25 |##| ^ (0.4%, 11.40, 1.7, 2%) 0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6937 (41, 11, 23, 26, 12) | 0.412 | 0.482 | 87.55 | 84.15 |-|
0             Validation | 0.6928 (42, 11, 23, 26, 13) | 0.431 | 0.503 | 87.78 | 84.25 |##| ^ (0.4%, 11.40, 1.7, 2%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1008 batches)
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6937 (41, 11, 23, 26, 12) | 0.412 | 0.482 | 87.55 | 84.15 |-|
0             Validation | 0.6928 (42, 11, 23, 26, 13) | 0.431 | 0.503 | 87.78 | 84.25 |##| ^ (0.4%, 11.40, 1.7, 2%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1008 batches)
0 >>  2/20 <<   Training | 0.6637 (30, 11, 25, 31, 17) | 0.543 | 0.623 | 88.37 | 85.55 |---------------|
0             Validation | 0.6633 (31, 11, 25, 31, 17) | 0.561 | 0.642 | 88.38 | 85.58 |###############| ^ (0.3%, 10.74, 1.9, 1%) 
0 >>  3/20 <<   Training | 0.6673 (31, 11, 24, 32, 17) | 0.488 | 0.566 | 88.16 | 85.34 |-------------|
0             Validation | 0.6674 (31, 11, 24, 32, 17) | 0.534 | 0.617 | 88.07 | 85.43 |##############| ^ (0.4%, 10.93, 2.9, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (504 batches)
0 >>  4/20 <<   Training | 0.6606 (39, 12, 23, 26, 15) | 0.538 | 0.617 | 88.85 | 85.68 |----------------|
0             Validation | 0.6614 (39, 12, 23, 25, 16) | 0.567 | 0.650 | 88.77 | 85.73 |#################| ^ (0.3%, 9.67, 1.7, 2%) 
0 >>  5/20 <<   Training | 0.6558 (32, 11, 27, 29, 15) | 0.556 | 0.635 | 88.67 | 85.81 |------------------|
0             Validation | 0.6569 (32, 11, 27, 29, 16) | 0.561 | 0.640 | 88.52 | 85.84 |##################| ^ (0.3%, 10.65, 1.5, 5%) 
0 >>  6/20 <<   Training | 0.6563 (33, 12, 27, 27, 16) | 0.544 | 0.622 | 88.85 | 85.73 |-----------------|
0             Validation | 0.6560 (33, 12, 27, 27, 16) | 0.542 | 0.622 | 88.85 | 85.84 |##################| ^ (0.4%, 9.47, 1.7, 2%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (252 batches)
0 >>  7/20 <<   Training | 0.6564 (33, 12, 23, 31, 16) | 0.529 | 0.608 | 88.89 | 85.79 |-----------------|
0             Validation | 0.6569 (33, 12, 23, 31, 16) | 0.563 | 0.646 | 88.68 | 85.87 |##################| ^ (0.3%, 9.80, 1.7, 2%) 
0 >>  8/20 <<   Training | 0.6504 (33, 12, 24, 29, 16) | 0.561 | 0.642 | 89.25 | 85.90 |------------------|
0             Validation | 0.6506 (33, 12, 24, 29, 16) | 0.589 | 0.674 | 89.05 | 85.97 |###################| ^ (0.4%, 10.32, 1.2, 20%) 
0 >>  9/20 <<   Training | 0.6526 (33, 11, 24, 29, 17) | 0.565 | 0.646 | 89.20 | 85.82 |------------------|
0             Validation | 0.6526 (33, 11, 24, 29, 17) | 0.586 | 0.670 | 89.06 | 85.92 |###################| ^ (0.4%, 10.44, 1.7, 2%) 
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6937 (41, 11, 23, 26, 12) | 0.290 | 0.610 | 87.55 | 84.15 |-|
0             Validation | 0.6928 (42, 11, 23, 26, 13) | 0.288 | 0.607 | 87.78 | 84.25 |##| ^ (0.4%, 11.40, 1.7, 2%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1008 batches)
0 >>  2/20 <<   Training | 0.6637 (30, 11, 25, 31, 17) | 0.568 | 1.170 | 88.37 | 85.55 |---------------|
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6937 (41, 11, 23, 26, 12) | 0.440 | 0.515 | 87.55 | 84.15 |-|
0             Validation | 0.6928 (42, 11, 23, 26, 13) | 0.441 | 0.516 | 87.78 | 84.25 |##| ^ (0.4%, 11.40, 1.7, 2%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1008 batches)
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6937 (41, 11, 23, 26, 12) | 0.588 | 0.687 | 87.55 | 84.15 |-|
0             Validation | 0.6928 (42, 11, 23, 26, 13) | 0.595 | 0.695 | 87.78 | 84.25 |##| ^ (0.4%, 11.40, 1.7, 2%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1008 batches)
0 >>  2/20 <<   Training | 0.6637 (30, 11, 25, 31, 17) | 0.787 | 0.902 | 88.37 | 85.55 |---------------|
0             Validation | 0.6633 (31, 11, 25, 31, 17) | 0.793 | 0.907 | 88.38 | 85.58 |###############| ^ (0.3%, 10.74, 1.9, 1%) 
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6937 (41, 11, 23, 26, 12) | 0.741 | 0.863 | 87.55 | 84.15 |-|
0             Validation | 0.6928 (42, 11, 23, 26, 13) | 0.747 | 0.870 | 87.78 | 84.25 |##| ^ (0.4%, 11.40, 1.7, 2%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1008 batches)
0 >>  2/20 <<   Training | 0.6637 (30, 11, 25, 31, 17) | 0.929 | 1.063 | 88.37 | 85.55 |---------------|
0             Validation | 0.6633 (31, 11, 25, 31, 17) | 0.934 | 1.067 | 88.38 | 85.58 |###############| ^ (0.3%, 10.74, 1.9, 1%) 
0 >>  3/20 <<   Training | 0.6673 (31, 11, 24, 32, 17) | 0.877 | 1.013 | 88.16 | 85.34 |-------------|
0             Validation | 0.6674 (31, 11, 24, 32, 17) | 0.906 | 1.045 | 88.07 | 85.43 |##############| ^ (0.4%, 10.93, 2.9, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (504 batches)
0 >>  4/20 <<   Training | 0.6606 (39, 12, 23, 26, 15) | 0.884 | 1.015 | 88.85 | 85.68 |----------------|
0             Validation | 0.6614 (39, 12, 23, 25, 16) | 0.893 | 1.024 | 88.77 | 85.73 |#################| ^ (0.3%, 9.67, 1.7, 2%) 
0 >>  5/20 <<   Training | 0.6558 (32, 11, 27, 29, 15) | 0.945 | 1.080 | 88.67 | 85.81 |------------------|
0             Validation | 0.6569 (32, 11, 27, 29, 16) | 0.946 | 1.081 | 88.52 | 85.84 |##################| ^ (0.3%, 10.65, 1.5, 5%) 
0 >>  6/20 <<   Training | 0.6563 (33, 12, 27, 27, 16) | 0.915 | 1.047 | 88.85 | 85.73 |-----------------|
0             Validation | 0.6560 (33, 12, 27, 27, 16) | 0.918 | 1.049 | 88.85 | 85.84 |##################| ^ (0.4%, 9.47, 1.7, 2%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (252 batches)
0 >>  7/20 <<   Training | 0.6564 (33, 12, 23, 31, 16) | 0.914 | 1.047 | 88.89 | 85.79 |-----------------|
0             Validation | 0.6569 (33, 12, 23, 31, 16) | 0.928 | 1.063 | 88.68 | 85.87 |##################| ^ (0.3%, 9.80, 1.7, 2%) 
0 >>  8/20 <<   Training | 0.6504 (33, 12, 24, 29, 16) | 0.939 | 1.074 | 89.25 | 85.90 |------------------|
0             Validation | 0.6506 (33, 12, 24, 29, 16) | 0.945 | 1.079 | 89.05 | 85.97 |###################| ^ (0.4%, 10.32, 1.2, 20%) 
0 >>  9/20 <<   Training | 0.6526 (33, 11, 24, 29, 17) | 0.936 | 1.071 | 89.20 | 85.82 |------------------|
0             Validation | 0.6526 (33, 11, 24, 29, 17) | 0.964 | 1.101 | 89.06 | 85.92 |###################| ^ (0.4%, 10.44, 1.7, 2%) 
0 >> 10/20 <<   Training | 0.6494 (34, 12, 25, 27, 16) | 0.948 | 1.084 | 89.22 | 86.00 |--------------------|
0             Validation | 0.6499 (34, 12, 25, 27, 16) | 0.962 | 1.100 | 89.06 | 86.07 |####################| ^ (0.4%, 10.61, 1.5, 5%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (126 batches)
0 >> 11/20 <<   Training | 0.6500 (38, 11, 24, 27, 15) | 0.923 | 1.056 | 89.33 | 86.03 |--------------------|
0             Validation | 0.6513 (38, 11, 25, 26, 15) | 0.937 | 1.072 | 89.12 | 86.07 |####################| ^ (0.3%, 10.28, 1.4, 9%) 
0 >> 12/20 <<   Training | 0.6480 (34, 12, 25, 28, 16) | 0.954 | 1.091 | 89.30 | 86.06 |--------------------|
0             Validation | 0.6489 (34, 12, 25, 28, 16) | 0.962 | 1.099 | 89.09 | 86.11 |#####################| ^ (0.4%, 9.96, 2.3, 0%) 
0 >> 13/20 <<   Training | 0.6479 (32, 12, 25, 30, 17) | 0.970 | 1.107 | 89.46 | 86.06 |--------------------|
0             Validation | 0.6488 (32, 12, 25, 30, 17) | 0.964 | 1.102 | 89.26 | 86.10 |####################| ^ (0.3%, 10.43, 1.5, 5%) 
0 >> 14/20 <<   Training | 0.6473 (34, 11, 23, 29, 17) | 0.957 | 1.094 | 89.45 | 86.05 |--------------------|
0             Validation | 0.6482 (34, 11, 23, 29, 17) | 0.962 | 1.100 | 89.24 | 86.09 |####################| ^ (0.3%, 10.57, 1.7, 2%) 
0 >> 15/20 <<   Training | 0.6469 (35, 11, 23, 29, 16) | 0.952 | 1.088 | 89.51 | 86.05 |--------------------|
0             Validation | 0.6477 (35, 11, 23, 29, 17) | 0.960 | 1.098 | 89.29 | 86.10 |####################| ^ (0.3%, 10.11, 1.5, 5%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6458 (33, 12, 24, 30, 16) | 0.966 | 1.104 | 89.46 | 86.12 |---------------------|
0             Validation | 0.6468 (33, 12, 24, 30, 17) | 0.968 | 1.106 | 89.24 | 86.16 |#####################| ^ (0.3%, 10.52, 1.6, 4%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6455 (33, 11, 25, 29, 16) | 0.971 | 1.110 | 89.48 | 86.13 |---------------------|
0             Validation | 0.6463 (33, 11, 25, 29, 17) | 0.982 | 1.122 | 89.26 | 86.18 |#####################| ^ (0.3%, 10.61, 1.7, 2%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6453 (34, 12, 24, 29, 16) | 0.966 | 1.104 | 89.49 | 86.13 |---------------------|
0             Validation | 0.6462 (34, 12, 24, 29, 16) | 0.966 | 1.103 | 89.27 | 86.18 |#####################| ^ (0.3%, 10.57, 1.6, 3%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6453 (34, 12, 24, 29, 16) | 0.967 | 1.105 | 89.48 | 86.13 |---------------------|
0             Validation | 0.6462 (34, 12, 24, 29, 16) | 0.969 | 1.106 | 89.27 | 86.18 |#####################| ^ (0.3%, 10.59, 1.8, 1%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6453 (34, 12, 24, 29, 16) | 0.969 | 1.107 | 89.49 | 86.13 |---------------------|
0             Validation | 0.6462 (34, 12, 24, 29, 16) | 0.969 | 1.107 | 89.27 | 86.18 |#####################| ^ (0.3%, 10.59, 1.7, 2%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6924 (38, 10, 23, 29, 14) | 0.808 | 0.928 | 87.78 | 84.55 |-----|
0             Validation | 0.6882 (38, 10, 23, 29, 14) | 0.824 | 0.946 | 87.73 | 84.76 |#######| ^ (0.6%, 10.87, 2.0, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1007 batches)
0 >>  2/20 <<   Training | 0.6732 (31, 11, 25, 31, 17) | 0.880 | 1.009 | 87.89 | 85.19 |-----------|
0             Validation | 0.6690 (31, 11, 24, 32, 17) | 0.891 | 1.022 | 87.78 | 85.39 |#############| ^ (0.6%, 12.21, 2.4, 0%) 
0 >>  3/20 <<   Training | 0.6668 (34, 12, 26, 28, 16) | 0.901 | 1.031 | 88.43 | 85.26 |------------|
0             Validation | 0.6637 (34, 12, 25, 28, 16) | 0.916 | 1.049 | 88.31 | 85.40 |##############| ^ (0.4%, 14.87, 2.5, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (503 batches)
0 >>  4/20 <<   Training | 0.6590 (36, 11, 25, 28, 15) | 0.917 | 1.049 | 88.62 | 85.65 |----------------|
0             Validation | 0.6556 (36, 11, 25, 28, 15) | 0.925 | 1.057 | 88.53 | 85.82 |##################| ^ (0.5%, 12.98, 2.8, 0%) 
0 >>  5/20 <<   Training | 0.6579 (32, 11, 24, 30, 17) | 0.933 | 1.067 | 88.98 | 85.70 |-----------------|
0             Validation | 0.6550 (33, 11, 23, 30, 17) | 0.955 | 1.090 | 88.76 | 85.89 |##################| ^ (0.6%, 13.37, 2.7, 0%) 
0 >>  6/20 <<   Training | 0.6562 (34, 11, 25, 27, 17) | 0.936 | 1.070 | 88.94 | 85.72 |-----------------|
0             Validation | 0.6530 (34, 11, 25, 27, 17) | 0.951 | 1.087 | 88.80 | 85.90 |##################| ^ (0.6%, 14.06, 2.3, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (251 batches)
0 >>  7/20 <<   Training | 0.6536 (35, 11, 25, 28, 16) | 0.936 | 1.069 | 89.14 | 85.79 |-----------------|
0             Validation | 0.6509 (36, 11, 24, 28, 16) | 0.949 | 1.083 | 88.93 | 85.97 |###################| ^ (0.5%, 12.27, 2.2, 0%) 
0 >>  8/20 <<   Training | 0.6546 (33, 11, 26, 30, 15) | 0.918 | 1.049 | 89.01 | 85.74 |-----------------|
0             Validation | 0.6515 (33, 11, 26, 30, 15) | 0.934 | 1.068 | 88.83 | 85.92 |###################| ^ (0.5%, 12.83, 2.2, 0%) 
0 >>  9/20 <<   Training | 0.6531 (36, 11, 24, 28, 16) | 0.929 | 1.061 | 89.25 | 85.80 |------------------|
0             Validation | 0.6503 (36, 11, 23, 28, 16) | 0.947 | 1.083 | 89.05 | 85.99 |###################| ^ (0.5%, 13.13, 2.2, 0%) 
0 >> 10/20 <<   Training | 0.6564 (37, 12, 22, 28, 16) | 0.912 | 1.043 | 89.40 | 85.79 |-----------------|
0             Validation | 0.6535 (37, 12, 22, 28, 16) | 0.931 | 1.064 | 89.15 | 85.98 |###################| ^ (0.6%, 12.79, 2.6, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (125 batches)
0 >> 11/20 <<   Training | 0.6519 (35, 12, 25, 27, 16) | 0.944 | 1.079 | 89.04 | 85.88 |------------------|
0             Validation | 0.6487 (35, 12, 25, 27, 16) | 0.962 | 1.102 | 88.88 | 86.06 |####################| ^ (0.6%, 12.58, 1.8, 1%) 
0 >> 12/20 <<   Training | 0.6509 (34, 11, 25, 28, 16) | 0.947 | 1.082 | 89.23 | 85.88 |------------------|
0             Validation | 0.6479 (34, 11, 24, 28, 17) | 0.971 | 1.109 | 89.03 | 86.06 |####################| ^ (0.6%, 13.83, 2.9, 0%) 
0 >> 13/20 <<   Training | 0.6502 (34, 12, 25, 28, 16) | 0.953 | 1.089 | 89.32 | 85.90 |------------------|
0             Validation | 0.6473 (34, 12, 24, 28, 16) | 0.965 | 1.103 | 89.07 | 86.08 |####################| ^ (0.5%, 13.08, 2.4, 0%) 
0 >> 14/20 <<   Training | 0.6517 (35, 11, 26, 27, 16) | 0.947 | 1.082 | 89.05 | 85.87 |------------------|
0             Validation | 0.6486 (35, 11, 25, 27, 16) | 0.966 | 1.106 | 88.86 | 86.06 |####################| ^ (0.5%, 12.27, 2.7, 0%) 
0 >> 15/20 <<   Training | 0.6519 (35, 11, 25, 28, 16) | 0.942 | 1.076 | 89.16 | 85.80 |------------------|
0             Validation | 0.6494 (35, 11, 24, 28, 16) | 0.944 | 1.079 | 88.97 | 85.96 |###################| ^ (0.5%, 12.57, 2.2, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6487 (34, 11, 25, 28, 16) | 0.958 | 1.094 | 89.29 | 85.95 |-------------------|
0             Validation | 0.6455 (34, 11, 25, 28, 16) | 0.982 | 1.123 | 89.10 | 86.14 |#####################| ^ (0.6%, 13.34, 2.2, 0%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6481 (34, 11, 25, 29, 16) | 0.963 | 1.100 | 89.34 | 85.98 |-------------------|
0             Validation | 0.6451 (34, 11, 25, 29, 16) | 0.981 | 1.122 | 89.14 | 86.15 |#####################| ^ (0.6%, 13.09, 1.9, 1%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6479 (34, 11, 25, 29, 16) | 0.961 | 1.098 | 89.37 | 85.98 |-------------------|
0             Validation | 0.6450 (34, 11, 24, 29, 16) | 0.985 | 1.127 | 89.17 | 86.16 |#####################| ^ (0.6%, 13.07, 2.1, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6479 (34, 11, 25, 29, 16) | 0.960 | 1.097 | 89.37 | 85.98 |-------------------|
0             Validation | 0.6450 (34, 11, 24, 29, 16) | 0.984 | 1.126 | 89.17 | 86.16 |#####################| ^ (0.6%, 13.07, 2.1, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6479 (34, 11, 25, 29, 16) | 0.960 | 1.097 | 89.37 | 85.98 |-------------------|
0             Validation | 0.6450 (34, 11, 24, 29, 16) | 0.985 | 1.127 | 89.17 | 86.16 |#####################| ^ (0.6%, 13.07, 2.1, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6824 (36, 12, 23, 29, 15) | 0.830 | 0.965 | 87.58 | 84.77 |-------|
0             Validation | 0.6799 (36, 12, 23, 29, 15) | 0.841 | 0.980 | 87.80 | 84.86 |########| ^ (0.3%, 13.55, 0.8, 66%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (961 batches)
0 >>  2/20 <<   Training | 0.6665 (33, 12, 26, 29, 14) | 0.904 | 1.050 | 88.27 | 85.13 |-----------|
0             Validation | 0.6655 (33, 12, 26, 29, 14) | 0.919 | 1.073 | 88.41 | 85.13 |###########| ^ (0.2%, 12.75, 1.2, 20%) 
0 >>  3/20 <<   Training | 0.6615 (34, 11, 25, 30, 15) | 0.947 | 1.098 | 88.52 | 85.46 |--------------|
0             Validation | 0.6608 (34, 11, 24, 30, 15) | 0.953 | 1.107 | 88.54 | 85.47 |##############| ^ (0.2%, 13.76, 1.2, 22%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (480 batches)
0 >>  4/20 <<   Training | 0.6575 (35, 11, 23, 29, 16) | 0.934 | 1.082 | 89.06 | 85.57 |---------------|
0             Validation | 0.6569 (35, 11, 23, 29, 17) | 0.943 | 1.096 | 89.22 | 85.55 |###############| ^ (0.2%, 13.69, 1.8, 1%) 
0 >>  5/20 <<   Training | 0.6584 (32, 11, 28, 29, 15) | 0.980 | 1.137 | 88.67 | 85.55 |---------------|
0             Validation | 0.6581 (32, 11, 27, 29, 16) | 0.987 | 1.147 | 88.79 | 85.53 |###############| ^ (0.2%, 14.47, 1.8, 1%) 
0 >>  6/20 <<   Training | 0.6592 (30, 12, 27, 30, 15) | 0.982 | 1.141 | 88.67 | 85.51 |---------------|
0             Validation | 0.6579 (30, 12, 27, 31, 15) | 0.973 | 1.136 | 88.90 | 85.52 |###############| ^ (0.2%, 13.60, 1.5, 5%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (240 batches)
0 >>  7/20 <<   Training | 0.6529 (34, 12, 25, 29, 16) | 0.984 | 1.140 | 89.07 | 85.71 |-----------------|
0             Validation | 0.6527 (34, 11, 25, 29, 16) | 0.975 | 1.132 | 89.18 | 85.69 |################| ^ (0.2%, 14.45, 1.7, 2%) 
0 >>  8/20 <<   Training | 0.6525 (35, 12, 23, 29, 16) | 0.971 | 1.125 | 89.36 | 85.82 |------------------|
0             Validation | 0.6524 (35, 12, 23, 29, 16) | 0.973 | 1.130 | 89.42 | 85.78 |#################| ^ (0.2%, 15.52, 1.4, 7%) 
0 >>  9/20 <<   Training | 0.6514 (34, 11, 25, 29, 16) | 0.993 | 1.151 | 89.32 | 85.81 |------------------|
0             Validation | 0.6515 (34, 11, 25, 29, 16) | 1.000 | 1.162 | 89.42 | 85.79 |#################| ^ (0.2%, 13.79, 1.4, 10%) 
0 >> 10/20 <<   Training | 0.6533 (36, 12, 24, 28, 15) | 0.971 | 1.126 | 89.22 | 85.77 |-----------------|
0             Validation | 0.6535 (36, 12, 24, 28, 15) | 0.974 | 1.133 | 89.29 | 85.71 |#################| ^ (0.2%, 15.19, 1.3, 13%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (120 batches)
0 >> 11/20 <<   Training | 0.6503 (36, 11, 24, 29, 15) | 0.991 | 1.148 | 89.29 | 85.87 |------------------|
0             Validation | 0.6502 (36, 11, 24, 29, 15) | 0.994 | 1.155 | 89.40 | 85.83 |##################| ^ (0.2%, 14.75, 1.8, 1%) 
0 >> 12/20 <<   Training | 0.6497 (33, 12, 25, 30, 15) | 1.000 | 1.158 | 89.23 | 85.93 |-------------------|
0             Validation | 0.6498 (33, 12, 25, 31, 15) | 1.003 | 1.166 | 89.31 | 85.87 |##################| ^ (0.2%, 14.34, 2.3, 0%) 
0 >> 13/20 <<   Training | 0.6501 (33, 11, 25, 31, 15) | 1.007 | 1.166 | 89.21 | 85.88 |------------------|
0             Validation | 0.6508 (33, 11, 25, 31, 16) | 1.003 | 1.164 | 89.27 | 85.82 |##################| ^ (0.2%, 14.43, 1.6, 3%) 
0 >> 14/20 <<   Training | 0.6502 (34, 11, 27, 28, 15) | 0.997 | 1.157 | 89.03 | 85.90 |-------------------|
0             Validation | 0.6506 (34, 11, 26, 28, 15) | 1.000 | 1.163 | 89.11 | 85.85 |##################| ^ (0.2%, 13.94, 1.6, 4%) 
0 >> 15/20 <<   Training | 0.6516 (32, 11, 27, 29, 15) | 1.001 | 1.161 | 89.04 | 85.88 |------------------|
0             Validation | 0.6523 (32, 11, 27, 29, 15) | 1.007 | 1.171 | 89.07 | 85.82 |##################| ^ (0.2%, 14.38, 1.8, 1%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6481 (35, 11, 24, 29, 15) | 0.997 | 1.155 | 89.34 | 85.97 |-------------------|
0             Validation | 0.6485 (35, 11, 24, 29, 15) | 0.994 | 1.155 | 89.40 | 85.91 |###################| ^ (0.2%, 15.04, 1.7, 2%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6476 (34, 11, 25, 29, 15) | 1.011 | 1.171 | 89.34 | 85.98 |-------------------|
0             Validation | 0.6481 (34, 11, 25, 29, 15) | 1.010 | 1.173 | 89.43 | 85.91 |###################| ^ (0.2%, 14.69, 2.6, 0%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6474 (34, 11, 25, 29, 15) | 1.009 | 1.169 | 89.39 | 85.99 |-------------------|
0             Validation | 0.6479 (34, 11, 25, 30, 15) | 1.001 | 1.163 | 89.47 | 85.92 |###################| ^ (0.2%, 14.76, 2.1, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6474 (34, 11, 25, 29, 15) | 1.008 | 1.168 | 89.39 | 85.99 |-------------------|
0             Validation | 0.6479 (34, 11, 25, 29, 15) | 1.000 | 1.162 | 89.46 | 85.92 |###################| ^ (0.2%, 14.75, 2.1, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6474 (34, 11, 25, 29, 15) | 1.008 | 1.168 | 89.39 | 85.99 |-------------------|
0             Validation | 0.6479 (34, 11, 25, 29, 15) | 1.000 | 1.162 | 89.46 | 85.92 |###################| ^ (0.2%, 14.75, 2.2, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
