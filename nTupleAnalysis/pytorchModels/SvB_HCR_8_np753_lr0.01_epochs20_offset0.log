0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7175 (32, 10,  8, 10, 22) | 6.805 | 7.119 | 83.93 | 89.49 |------------------------------------------------------|
0             Validation | 0.7163 (32, 10,  8, 10, 22) | 6.812 | 7.114 | 83.77 | 89.52 |#######################################################| ^ (0.2%, 19.84, 1.5, 6%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1008 batches)
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7135 (37, 11, 25, 28, 12) | 0.665 | 0.843 | 85.85 | 82.90 ||
0             Validation | 0.7117 (37, 11, 25, 28, 12) | 0.666 | 0.840 | 85.81 | 83.17 || ^ (0.8%, 9.15, 1.8, 1%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1008 batches)
0 >>  2/20 <<   Training | 0.6718 (32, 10, 26, 31, 14) | 0.996 | 1.144 | 86.76 | 85.37 |-------------|
0             Validation | 0.6704 (32, 10, 26, 31, 14) | 0.981 | 1.124 | 86.67 | 85.53 |###############| ^ (0.5%, 8.99, 1.5, 4%) 
0 >>  3/20 <<   Training | 0.6748 (35, 11, 24, 30, 14) | 0.983 | 1.139 | 86.58 | 85.28 |------------|
0             Validation | 0.6745 (34, 11, 24, 29, 14) | 0.997 | 1.154 | 86.32 | 85.46 |##############| ^ (0.6%, 9.77, 2.2, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (504 batches)
0 >>  4/20 <<   Training | 0.6621 (33, 12, 24, 30, 15) | 1.072 | 1.228 | 87.30 | 85.74 |-----------------|
0             Validation | 0.6625 (33, 12, 24, 30, 15) | 1.063 | 1.217 | 87.13 | 85.85 |##################| ^ (0.4%, 9.31, 2.3, 0%) 
0 >>  5/20 <<   Training | 0.6605 (34, 11, 25, 30, 14) | 1.076 | 1.230 | 87.41 | 85.70 |----------------|
0             Validation | 0.6614 (34, 11, 25, 29, 14) | 1.088 | 1.244 | 87.16 | 85.78 |#################| ^ (0.3%, 10.14, 1.5, 6%) 
0 >>  6/20 <<   Training | 0.6603 (35, 11, 25, 29, 14) | 1.079 | 1.238 | 87.67 | 85.85 |------------------|
0             Validation | 0.6606 (35, 11, 25, 28, 14) | 1.075 | 1.234 | 87.49 | 85.99 |###################| ^ (0.4%, 9.15, 1.4, 8%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (252 batches)
0 >>  7/20 <<   Training | 0.6570 (35, 11, 23, 31, 14) | 1.111 | 1.272 | 87.72 | 86.02 |--------------------|
0             Validation | 0.6576 (35, 11, 23, 30, 14) | 1.120 | 1.281 | 87.55 | 86.11 |#####################| ^ (0.3%, 9.10, 1.8, 1%) 
0 >>  8/20 <<   Training | 0.6555 (32, 12, 27, 30, 14) | 1.113 | 1.270 | 87.54 | 85.93 |-------------------|
0             Validation | 0.6559 (32, 12, 27, 29, 14) | 1.123 | 1.285 | 87.27 | 86.05 |####################| ^ (0.4%, 9.11, 2.1, 0%) 
0 >>  9/20 <<   Training | 0.6543 (32, 11, 26, 30, 14) | 1.124 | 1.284 | 87.65 | 86.04 |--------------------|
0             Validation | 0.6544 (32, 11, 26, 29, 14) | 1.134 | 1.296 | 87.39 | 86.17 |#####################| ^ (0.4%, 9.47, 1.5, 4%) 
0 >> 10/20 <<   Training | 0.6531 (32, 12, 26, 30, 14) | 1.124 | 1.284 | 87.68 | 86.09 |--------------------|
0             Validation | 0.6542 (31, 12, 26, 29, 14) | 1.137 | 1.298 | 87.34 | 86.18 |#####################| ^ (0.3%, 9.96, 2.2, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (126 batches)
0 >> 11/20 <<   Training | 0.6524 (36, 11, 24, 29, 13) | 1.142 | 1.303 | 87.89 | 86.13 |---------------------|
0             Validation | 0.6537 (36, 11, 25, 28, 13) | 1.144 | 1.307 | 87.58 | 86.21 |######################| ^ (0.3%, 9.42, 1.3, 12%) 
0 >> 12/20 <<   Training | 0.6518 (35, 11, 24, 29, 14) | 1.148 | 1.310 | 87.96 | 86.10 |---------------------|
0             Validation | 0.6529 (35, 11, 24, 29, 14) | 1.149 | 1.311 | 87.59 | 86.19 |#####################| ^ (0.3%, 9.02, 1.6, 3%) 
0 >> 13/20 <<   Training | 0.6510 (33, 12, 26, 29, 14) | 1.148 | 1.310 | 87.81 | 86.17 |---------------------|
0             Validation | 0.6523 (33, 12, 26, 28, 14) | 1.153 | 1.317 | 87.47 | 86.24 |######################| ^ (0.3%, 9.97, 2.4, 0%) 
0 >> 14/20 <<   Training | 0.6512 (33, 11, 26, 30, 14) | 1.148 | 1.310 | 87.79 | 86.14 |---------------------|
0             Validation | 0.6517 (33, 11, 26, 30, 14) | 1.146 | 1.309 | 87.55 | 86.25 |######################| ^ (0.4%, 9.84, 2.0, 0%) 
0 >> 15/20 <<   Training | 0.6517 (34, 11, 25, 30, 14) | 1.131 | 1.293 | 87.94 | 86.10 |---------------------|
0             Validation | 0.6529 (34, 11, 25, 30, 14) | 1.126 | 1.289 | 87.65 | 86.20 |######################| ^ (0.4%, 9.41, 1.9, 1%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6494 (33, 12, 25, 30, 14) | 1.151 | 1.314 | 87.92 | 86.20 |---------------------|
0             Validation | 0.6504 (33, 12, 25, 29, 14) | 1.148 | 1.309 | 87.61 | 86.30 |######################| ^ (0.4%, 9.52, 1.9, 1%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6491 (33, 11, 25, 30, 14) | 1.157 | 1.322 | 87.92 | 86.22 |----------------------|
0             Validation | 0.6501 (33, 11, 26, 29, 14) | 1.155 | 1.321 | 87.60 | 86.32 |#######################| ^ (0.3%, 9.63, 1.9, 1%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6489 (34, 11, 25, 29, 14) | 1.158 | 1.323 | 87.95 | 86.23 |----------------------|
0             Validation | 0.6500 (34, 11, 25, 29, 14) | 1.155 | 1.320 | 87.63 | 86.32 |#######################| ^ (0.3%, 9.60, 1.8, 1%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6489 (34, 11, 25, 29, 14) | 1.159 | 1.323 | 87.94 | 86.23 |----------------------|
0             Validation | 0.6500 (34, 11, 25, 29, 14) | 1.157 | 1.322 | 87.62 | 86.32 |#######################| ^ (0.3%, 9.62, 1.9, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6489 (34, 11, 25, 29, 14) | 1.159 | 1.323 | 87.94 | 86.23 |----------------------|
0             Validation | 0.6500 (34, 11, 25, 29, 14) | 1.157 | 1.322 | 87.62 | 86.32 |#######################| ^ (0.3%, 9.62, 1.9, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_lr0.01_epochs20_offset0_epoch20.pkl
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7135 (37, 11, 25, 28, 12) | 0.665 | 0.843 | 85.85 | 82.90 ||
0             Validation | 0.7117 (37, 11, 25, 28, 12) | 0.666 | 0.840 | 85.81 | 83.17 || ^ (0.8%, 9.15, 1.8, 1%) 0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6937 (41, 11, 23, 26, 12) | 0.412 | 0.482 | 87.55 | 84.15 |-|
0             Validation | 0.6928 (42, 11, 23, 26, 13) | 0.431 | 0.503 | 87.78 | 84.25 |##| ^ (0.4%, 11.40, 1.7, 2%) 0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6937 (41, 11, 23, 26, 12) | 0.412 | 0.482 | 87.55 | 84.15 |-|
0             Validation | 0.6928 (42, 11, 23, 26, 13) | 0.431 | 0.503 | 87.78 | 84.25 |##| ^ (0.4%, 11.40, 1.7, 2%) 0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6937 (41, 11, 23, 26, 12) | 0.412 | 0.482 | 87.55 | 84.15 |-|
0             Validation | 0.6928 (42, 11, 23, 26, 13) | 0.431 | 0.503 | 87.78 | 84.25 |##| ^ (0.4%, 11.40, 1.7, 2%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1008 batches)
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6937 (41, 11, 23, 26, 12) | 0.412 | 0.482 | 87.55 | 84.15 |-|
0             Validation | 0.6928 (42, 11, 23, 26, 13) | 0.431 | 0.503 | 87.78 | 84.25 |##| ^ (0.4%, 11.40, 1.7, 2%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1008 batches)
0 >>  2/20 <<   Training | 0.6637 (30, 11, 25, 31, 17) | 0.543 | 0.623 | 88.37 | 85.55 |---------------|
0             Validation | 0.6633 (31, 11, 25, 31, 17) | 0.561 | 0.642 | 88.38 | 85.58 |###############| ^ (0.3%, 10.74, 1.9, 1%) 
0 >>  3/20 <<   Training | 0.6673 (31, 11, 24, 32, 17) | 0.488 | 0.566 | 88.16 | 85.34 |-------------|
0             Validation | 0.6674 (31, 11, 24, 32, 17) | 0.534 | 0.617 | 88.07 | 85.43 |##############| ^ (0.4%, 10.93, 2.9, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (504 batches)
0 >>  4/20 <<   Training | 0.6606 (39, 12, 23, 26, 15) | 0.538 | 0.617 | 88.85 | 85.68 |----------------|
0             Validation | 0.6614 (39, 12, 23, 25, 16) | 0.567 | 0.650 | 88.77 | 85.73 |#################| ^ (0.3%, 9.67, 1.7, 2%) 
0 >>  5/20 <<   Training | 0.6558 (32, 11, 27, 29, 15) | 0.556 | 0.635 | 88.67 | 85.81 |------------------|
0             Validation | 0.6569 (32, 11, 27, 29, 16) | 0.561 | 0.640 | 88.52 | 85.84 |##################| ^ (0.3%, 10.65, 1.5, 5%) 
0 >>  6/20 <<   Training | 0.6563 (33, 12, 27, 27, 16) | 0.544 | 0.622 | 88.85 | 85.73 |-----------------|
0             Validation | 0.6560 (33, 12, 27, 27, 16) | 0.542 | 0.622 | 88.85 | 85.84 |##################| ^ (0.4%, 9.47, 1.7, 2%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (252 batches)
0 >>  7/20 <<   Training | 0.6564 (33, 12, 23, 31, 16) | 0.529 | 0.608 | 88.89 | 85.79 |-----------------|
0             Validation | 0.6569 (33, 12, 23, 31, 16) | 0.563 | 0.646 | 88.68 | 85.87 |##################| ^ (0.3%, 9.80, 1.7, 2%) 
0 >>  8/20 <<   Training | 0.6504 (33, 12, 24, 29, 16) | 0.561 | 0.642 | 89.25 | 85.90 |------------------|
0             Validation | 0.6506 (33, 12, 24, 29, 16) | 0.589 | 0.674 | 89.05 | 85.97 |###################| ^ (0.4%, 10.32, 1.2, 20%) 
0 >>  9/20 <<   Training | 0.6526 (33, 11, 24, 29, 17) | 0.565 | 0.646 | 89.20 | 85.82 |------------------|
0             Validation | 0.6526 (33, 11, 24, 29, 17) | 0.586 | 0.670 | 89.06 | 85.92 |###################| ^ (0.4%, 10.44, 1.7, 2%) 
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6937 (41, 11, 23, 26, 12) | 0.290 | 0.610 | 87.55 | 84.15 |-|
0             Validation | 0.6928 (42, 11, 23, 26, 13) | 0.288 | 0.607 | 87.78 | 84.25 |##| ^ (0.4%, 11.40, 1.7, 2%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1008 batches)
0 >>  2/20 <<   Training | 0.6637 (30, 11, 25, 31, 17) | 0.568 | 1.170 | 88.37 | 85.55 |---------------|
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6937 (41, 11, 23, 26, 12) | 0.440 | 0.515 | 87.55 | 84.15 |-|
0             Validation | 0.6928 (42, 11, 23, 26, 13) | 0.441 | 0.516 | 87.78 | 84.25 |##| ^ (0.4%, 11.40, 1.7, 2%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1008 batches)
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6937 (41, 11, 23, 26, 12) | 0.588 | 0.687 | 87.55 | 84.15 |-|
0             Validation | 0.6928 (42, 11, 23, 26, 13) | 0.595 | 0.695 | 87.78 | 84.25 |##| ^ (0.4%, 11.40, 1.7, 2%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1008 batches)
0 >>  2/20 <<   Training | 0.6637 (30, 11, 25, 31, 17) | 0.787 | 0.902 | 88.37 | 85.55 |---------------|
0             Validation | 0.6633 (31, 11, 25, 31, 17) | 0.793 | 0.907 | 88.38 | 85.58 |###############| ^ (0.3%, 10.74, 1.9, 1%) 
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6937 (41, 11, 23, 26, 12) | 0.741 | 0.863 | 87.55 | 84.15 |-|
0             Validation | 0.6928 (42, 11, 23, 26, 13) | 0.747 | 0.870 | 87.78 | 84.25 |##| ^ (0.4%, 11.40, 1.7, 2%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1008 batches)
0 >>  2/20 <<   Training | 0.6637 (30, 11, 25, 31, 17) | 0.929 | 1.063 | 88.37 | 85.55 |---------------|
0             Validation | 0.6633 (31, 11, 25, 31, 17) | 0.934 | 1.067 | 88.38 | 85.58 |###############| ^ (0.3%, 10.74, 1.9, 1%) 
0 >>  3/20 <<   Training | 0.6673 (31, 11, 24, 32, 17) | 0.877 | 1.013 | 88.16 | 85.34 |-------------|
0             Validation | 0.6674 (31, 11, 24, 32, 17) | 0.906 | 1.045 | 88.07 | 85.43 |##############| ^ (0.4%, 10.93, 2.9, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (504 batches)
0 >>  4/20 <<   Training | 0.6606 (39, 12, 23, 26, 15) | 0.884 | 1.015 | 88.85 | 85.68 |----------------|
0             Validation | 0.6614 (39, 12, 23, 25, 16) | 0.893 | 1.024 | 88.77 | 85.73 |#################| ^ (0.3%, 9.67, 1.7, 2%) 
0 >>  5/20 <<   Training | 0.6558 (32, 11, 27, 29, 15) | 0.945 | 1.080 | 88.67 | 85.81 |------------------|
0             Validation | 0.6569 (32, 11, 27, 29, 16) | 0.946 | 1.081 | 88.52 | 85.84 |##################| ^ (0.3%, 10.65, 1.5, 5%) 
0 >>  6/20 <<   Training | 0.6563 (33, 12, 27, 27, 16) | 0.915 | 1.047 | 88.85 | 85.73 |-----------------|
0             Validation | 0.6560 (33, 12, 27, 27, 16) | 0.918 | 1.049 | 88.85 | 85.84 |##################| ^ (0.4%, 9.47, 1.7, 2%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (252 batches)
0 >>  7/20 <<   Training | 0.6564 (33, 12, 23, 31, 16) | 0.914 | 1.047 | 88.89 | 85.79 |-----------------|
0             Validation | 0.6569 (33, 12, 23, 31, 16) | 0.928 | 1.063 | 88.68 | 85.87 |##################| ^ (0.3%, 9.80, 1.7, 2%) 
0 >>  8/20 <<   Training | 0.6504 (33, 12, 24, 29, 16) | 0.939 | 1.074 | 89.25 | 85.90 |------------------|
0             Validation | 0.6506 (33, 12, 24, 29, 16) | 0.945 | 1.079 | 89.05 | 85.97 |###################| ^ (0.4%, 10.32, 1.2, 20%) 
0 >>  9/20 <<   Training | 0.6526 (33, 11, 24, 29, 17) | 0.936 | 1.071 | 89.20 | 85.82 |------------------|
0             Validation | 0.6526 (33, 11, 24, 29, 17) | 0.964 | 1.101 | 89.06 | 85.92 |###################| ^ (0.4%, 10.44, 1.7, 2%) 
0 >> 10/20 <<   Training | 0.6494 (34, 12, 25, 27, 16) | 0.948 | 1.084 | 89.22 | 86.00 |--------------------|
0             Validation | 0.6499 (34, 12, 25, 27, 16) | 0.962 | 1.100 | 89.06 | 86.07 |####################| ^ (0.4%, 10.61, 1.5, 5%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (126 batches)
0 >> 11/20 <<   Training | 0.6500 (38, 11, 24, 27, 15) | 0.923 | 1.056 | 89.33 | 86.03 |--------------------|
0             Validation | 0.6513 (38, 11, 25, 26, 15) | 0.937 | 1.072 | 89.12 | 86.07 |####################| ^ (0.3%, 10.28, 1.4, 9%) 
0 >> 12/20 <<   Training | 0.6480 (34, 12, 25, 28, 16) | 0.954 | 1.091 | 89.30 | 86.06 |--------------------|
0             Validation | 0.6489 (34, 12, 25, 28, 16) | 0.962 | 1.099 | 89.09 | 86.11 |#####################| ^ (0.4%, 9.96, 2.3, 0%) 
0 >> 13/20 <<   Training | 0.6479 (32, 12, 25, 30, 17) | 0.970 | 1.107 | 89.46 | 86.06 |--------------------|
0             Validation | 0.6488 (32, 12, 25, 30, 17) | 0.964 | 1.102 | 89.26 | 86.10 |####################| ^ (0.3%, 10.43, 1.5, 5%) 
0 >> 14/20 <<   Training | 0.6473 (34, 11, 23, 29, 17) | 0.957 | 1.094 | 89.45 | 86.05 |--------------------|
0             Validation | 0.6482 (34, 11, 23, 29, 17) | 0.962 | 1.100 | 89.24 | 86.09 |####################| ^ (0.3%, 10.57, 1.7, 2%) 
0 >> 15/20 <<   Training | 0.6469 (35, 11, 23, 29, 16) | 0.952 | 1.088 | 89.51 | 86.05 |--------------------|
0             Validation | 0.6477 (35, 11, 23, 29, 17) | 0.960 | 1.098 | 89.29 | 86.10 |####################| ^ (0.3%, 10.11, 1.5, 5%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6458 (33, 12, 24, 30, 16) | 0.966 | 1.104 | 89.46 | 86.12 |---------------------|
0             Validation | 0.6468 (33, 12, 24, 30, 17) | 0.968 | 1.106 | 89.24 | 86.16 |#####################| ^ (0.3%, 10.52, 1.6, 4%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6455 (33, 11, 25, 29, 16) | 0.971 | 1.110 | 89.48 | 86.13 |---------------------|
0             Validation | 0.6463 (33, 11, 25, 29, 17) | 0.982 | 1.122 | 89.26 | 86.18 |#####################| ^ (0.3%, 10.61, 1.7, 2%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6453 (34, 12, 24, 29, 16) | 0.966 | 1.104 | 89.49 | 86.13 |---------------------|
0             Validation | 0.6462 (34, 12, 24, 29, 16) | 0.966 | 1.103 | 89.27 | 86.18 |#####################| ^ (0.3%, 10.57, 1.6, 3%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6453 (34, 12, 24, 29, 16) | 0.967 | 1.105 | 89.48 | 86.13 |---------------------|
0             Validation | 0.6462 (34, 12, 24, 29, 16) | 0.969 | 1.106 | 89.27 | 86.18 |#####################| ^ (0.3%, 10.59, 1.8, 1%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6453 (34, 12, 24, 29, 16) | 0.969 | 1.107 | 89.49 | 86.13 |---------------------|
0             Validation | 0.6462 (34, 12, 24, 29, 16) | 0.969 | 1.107 | 89.27 | 86.18 |#####################| ^ (0.3%, 10.59, 1.7, 2%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6924 (38, 10, 23, 29, 14) | 0.808 | 0.928 | 87.78 | 84.55 |-----|
0             Validation | 0.6882 (38, 10, 23, 29, 14) | 0.824 | 0.946 | 87.73 | 84.76 |#######| ^ (0.6%, 10.87, 2.0, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1007 batches)
0 >>  2/20 <<   Training | 0.6732 (31, 11, 25, 31, 17) | 0.880 | 1.009 | 87.89 | 85.19 |-----------|
0             Validation | 0.6690 (31, 11, 24, 32, 17) | 0.891 | 1.022 | 87.78 | 85.39 |#############| ^ (0.6%, 12.21, 2.4, 0%) 
0 >>  3/20 <<   Training | 0.6668 (34, 12, 26, 28, 16) | 0.901 | 1.031 | 88.43 | 85.26 |------------|
0             Validation | 0.6637 (34, 12, 25, 28, 16) | 0.916 | 1.049 | 88.31 | 85.40 |##############| ^ (0.4%, 14.87, 2.5, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (503 batches)
0 >>  4/20 <<   Training | 0.6590 (36, 11, 25, 28, 15) | 0.917 | 1.049 | 88.62 | 85.65 |----------------|
0             Validation | 0.6556 (36, 11, 25, 28, 15) | 0.925 | 1.057 | 88.53 | 85.82 |##################| ^ (0.5%, 12.98, 2.8, 0%) 
0 >>  5/20 <<   Training | 0.6579 (32, 11, 24, 30, 17) | 0.933 | 1.067 | 88.98 | 85.70 |-----------------|
0             Validation | 0.6550 (33, 11, 23, 30, 17) | 0.955 | 1.090 | 88.76 | 85.89 |##################| ^ (0.6%, 13.37, 2.7, 0%) 
0 >>  6/20 <<   Training | 0.6562 (34, 11, 25, 27, 17) | 0.936 | 1.070 | 88.94 | 85.72 |-----------------|
0             Validation | 0.6530 (34, 11, 25, 27, 17) | 0.951 | 1.087 | 88.80 | 85.90 |##################| ^ (0.6%, 14.06, 2.3, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (251 batches)
0 >>  7/20 <<   Training | 0.6536 (35, 11, 25, 28, 16) | 0.936 | 1.069 | 89.14 | 85.79 |-----------------|
0             Validation | 0.6509 (36, 11, 24, 28, 16) | 0.949 | 1.083 | 88.93 | 85.97 |###################| ^ (0.5%, 12.27, 2.2, 0%) 
0 >>  8/20 <<   Training | 0.6546 (33, 11, 26, 30, 15) | 0.918 | 1.049 | 89.01 | 85.74 |-----------------|
0             Validation | 0.6515 (33, 11, 26, 30, 15) | 0.934 | 1.068 | 88.83 | 85.92 |###################| ^ (0.5%, 12.83, 2.2, 0%) 
0 >>  9/20 <<   Training | 0.6531 (36, 11, 24, 28, 16) | 0.929 | 1.061 | 89.25 | 85.80 |------------------|
0             Validation | 0.6503 (36, 11, 23, 28, 16) | 0.947 | 1.083 | 89.05 | 85.99 |###################| ^ (0.5%, 13.13, 2.2, 0%) 
0 >> 10/20 <<   Training | 0.6564 (37, 12, 22, 28, 16) | 0.912 | 1.043 | 89.40 | 85.79 |-----------------|
0             Validation | 0.6535 (37, 12, 22, 28, 16) | 0.931 | 1.064 | 89.15 | 85.98 |###################| ^ (0.6%, 12.79, 2.6, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (125 batches)
0 >> 11/20 <<   Training | 0.6519 (35, 12, 25, 27, 16) | 0.944 | 1.079 | 89.04 | 85.88 |------------------|
0             Validation | 0.6487 (35, 12, 25, 27, 16) | 0.962 | 1.102 | 88.88 | 86.06 |####################| ^ (0.6%, 12.58, 1.8, 1%) 
0 >> 12/20 <<   Training | 0.6509 (34, 11, 25, 28, 16) | 0.947 | 1.082 | 89.23 | 85.88 |------------------|
0             Validation | 0.6479 (34, 11, 24, 28, 17) | 0.971 | 1.109 | 89.03 | 86.06 |####################| ^ (0.6%, 13.83, 2.9, 0%) 
0 >> 13/20 <<   Training | 0.6502 (34, 12, 25, 28, 16) | 0.953 | 1.089 | 89.32 | 85.90 |------------------|
0             Validation | 0.6473 (34, 12, 24, 28, 16) | 0.965 | 1.103 | 89.07 | 86.08 |####################| ^ (0.5%, 13.08, 2.4, 0%) 
0 >> 14/20 <<   Training | 0.6517 (35, 11, 26, 27, 16) | 0.947 | 1.082 | 89.05 | 85.87 |------------------|
0             Validation | 0.6486 (35, 11, 25, 27, 16) | 0.966 | 1.106 | 88.86 | 86.06 |####################| ^ (0.5%, 12.27, 2.7, 0%) 
0 >> 15/20 <<   Training | 0.6519 (35, 11, 25, 28, 16) | 0.942 | 1.076 | 89.16 | 85.80 |------------------|
0             Validation | 0.6494 (35, 11, 24, 28, 16) | 0.944 | 1.079 | 88.97 | 85.96 |###################| ^ (0.5%, 12.57, 2.2, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6487 (34, 11, 25, 28, 16) | 0.958 | 1.094 | 89.29 | 85.95 |-------------------|
0             Validation | 0.6455 (34, 11, 25, 28, 16) | 0.982 | 1.123 | 89.10 | 86.14 |#####################| ^ (0.6%, 13.34, 2.2, 0%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6481 (34, 11, 25, 29, 16) | 0.963 | 1.100 | 89.34 | 85.98 |-------------------|
0             Validation | 0.6451 (34, 11, 25, 29, 16) | 0.981 | 1.122 | 89.14 | 86.15 |#####################| ^ (0.6%, 13.09, 1.9, 1%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6479 (34, 11, 25, 29, 16) | 0.961 | 1.098 | 89.37 | 85.98 |-------------------|
0             Validation | 0.6450 (34, 11, 24, 29, 16) | 0.985 | 1.127 | 89.17 | 86.16 |#####################| ^ (0.6%, 13.07, 2.1, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6479 (34, 11, 25, 29, 16) | 0.960 | 1.097 | 89.37 | 85.98 |-------------------|
0             Validation | 0.6450 (34, 11, 24, 29, 16) | 0.984 | 1.126 | 89.17 | 86.16 |#####################| ^ (0.6%, 13.07, 2.1, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6479 (34, 11, 25, 29, 16) | 0.960 | 1.097 | 89.37 | 85.98 |-------------------|
0             Validation | 0.6450 (34, 11, 24, 29, 16) | 0.985 | 1.127 | 89.17 | 86.16 |#####################| ^ (0.6%, 13.07, 2.1, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6824 (36, 12, 23, 29, 15) | 0.830 | 0.965 | 87.58 | 84.77 |-------|
0             Validation | 0.6799 (36, 12, 23, 29, 15) | 0.841 | 0.980 | 87.80 | 84.86 |########| ^ (0.3%, 13.55, 0.8, 66%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (961 batches)
0 >>  2/20 <<   Training | 0.6665 (33, 12, 26, 29, 14) | 0.904 | 1.050 | 88.27 | 85.13 |-----------|
0             Validation | 0.6655 (33, 12, 26, 29, 14) | 0.919 | 1.073 | 88.41 | 85.13 |###########| ^ (0.2%, 12.75, 1.2, 20%) 
0 >>  3/20 <<   Training | 0.6615 (34, 11, 25, 30, 15) | 0.947 | 1.098 | 88.52 | 85.46 |--------------|
0             Validation | 0.6608 (34, 11, 24, 30, 15) | 0.953 | 1.107 | 88.54 | 85.47 |##############| ^ (0.2%, 13.76, 1.2, 22%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (480 batches)
0 >>  4/20 <<   Training | 0.6575 (35, 11, 23, 29, 16) | 0.934 | 1.082 | 89.06 | 85.57 |---------------|
0             Validation | 0.6569 (35, 11, 23, 29, 17) | 0.943 | 1.096 | 89.22 | 85.55 |###############| ^ (0.2%, 13.69, 1.8, 1%) 
0 >>  5/20 <<   Training | 0.6584 (32, 11, 28, 29, 15) | 0.980 | 1.137 | 88.67 | 85.55 |---------------|
0             Validation | 0.6581 (32, 11, 27, 29, 16) | 0.987 | 1.147 | 88.79 | 85.53 |###############| ^ (0.2%, 14.47, 1.8, 1%) 
0 >>  6/20 <<   Training | 0.6592 (30, 12, 27, 30, 15) | 0.982 | 1.141 | 88.67 | 85.51 |---------------|
0             Validation | 0.6579 (30, 12, 27, 31, 15) | 0.973 | 1.136 | 88.90 | 85.52 |###############| ^ (0.2%, 13.60, 1.5, 5%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (240 batches)
0 >>  7/20 <<   Training | 0.6529 (34, 12, 25, 29, 16) | 0.984 | 1.140 | 89.07 | 85.71 |-----------------|
0             Validation | 0.6527 (34, 11, 25, 29, 16) | 0.975 | 1.132 | 89.18 | 85.69 |################| ^ (0.2%, 14.45, 1.7, 2%) 
0 >>  8/20 <<   Training | 0.6525 (35, 12, 23, 29, 16) | 0.971 | 1.125 | 89.36 | 85.82 |------------------|
0             Validation | 0.6524 (35, 12, 23, 29, 16) | 0.973 | 1.130 | 89.42 | 85.78 |#################| ^ (0.2%, 15.52, 1.4, 7%) 
0 >>  9/20 <<   Training | 0.6514 (34, 11, 25, 29, 16) | 0.993 | 1.151 | 89.32 | 85.81 |------------------|
0             Validation | 0.6515 (34, 11, 25, 29, 16) | 1.000 | 1.162 | 89.42 | 85.79 |#################| ^ (0.2%, 13.79, 1.4, 10%) 
0 >> 10/20 <<   Training | 0.6533 (36, 12, 24, 28, 15) | 0.971 | 1.126 | 89.22 | 85.77 |-----------------|
0             Validation | 0.6535 (36, 12, 24, 28, 15) | 0.974 | 1.133 | 89.29 | 85.71 |#################| ^ (0.2%, 15.19, 1.3, 13%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (120 batches)
0 >> 11/20 <<   Training | 0.6503 (36, 11, 24, 29, 15) | 0.991 | 1.148 | 89.29 | 85.87 |------------------|
0             Validation | 0.6502 (36, 11, 24, 29, 15) | 0.994 | 1.155 | 89.40 | 85.83 |##################| ^ (0.2%, 14.75, 1.8, 1%) 
0 >> 12/20 <<   Training | 0.6497 (33, 12, 25, 30, 15) | 1.000 | 1.158 | 89.23 | 85.93 |-------------------|
0             Validation | 0.6498 (33, 12, 25, 31, 15) | 1.003 | 1.166 | 89.31 | 85.87 |##################| ^ (0.2%, 14.34, 2.3, 0%) 
0 >> 13/20 <<   Training | 0.6501 (33, 11, 25, 31, 15) | 1.007 | 1.166 | 89.21 | 85.88 |------------------|
0             Validation | 0.6508 (33, 11, 25, 31, 16) | 1.003 | 1.164 | 89.27 | 85.82 |##################| ^ (0.2%, 14.43, 1.6, 3%) 
0 >> 14/20 <<   Training | 0.6502 (34, 11, 27, 28, 15) | 0.997 | 1.157 | 89.03 | 85.90 |-------------------|
0             Validation | 0.6506 (34, 11, 26, 28, 15) | 1.000 | 1.163 | 89.11 | 85.85 |##################| ^ (0.2%, 13.94, 1.6, 4%) 
0 >> 15/20 <<   Training | 0.6516 (32, 11, 27, 29, 15) | 1.001 | 1.161 | 89.04 | 85.88 |------------------|
0             Validation | 0.6523 (32, 11, 27, 29, 15) | 1.007 | 1.171 | 89.07 | 85.82 |##################| ^ (0.2%, 14.38, 1.8, 1%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6481 (35, 11, 24, 29, 15) | 0.997 | 1.155 | 89.34 | 85.97 |-------------------|
0             Validation | 0.6485 (35, 11, 24, 29, 15) | 0.994 | 1.155 | 89.40 | 85.91 |###################| ^ (0.2%, 15.04, 1.7, 2%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6476 (34, 11, 25, 29, 15) | 1.011 | 1.171 | 89.34 | 85.98 |-------------------|
0             Validation | 0.6481 (34, 11, 25, 29, 15) | 1.010 | 1.173 | 89.43 | 85.91 |###################| ^ (0.2%, 14.69, 2.6, 0%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6474 (34, 11, 25, 29, 15) | 1.009 | 1.169 | 89.39 | 85.99 |-------------------|
0             Validation | 0.6479 (34, 11, 25, 30, 15) | 1.001 | 1.163 | 89.47 | 85.92 |###################| ^ (0.2%, 14.76, 2.1, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6474 (34, 11, 25, 29, 15) | 1.008 | 1.168 | 89.39 | 85.99 |-------------------|
0             Validation | 0.6479 (34, 11, 25, 29, 15) | 1.000 | 1.162 | 89.46 | 85.92 |###################| ^ (0.2%, 14.75, 2.1, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6474 (34, 11, 25, 29, 15) | 1.008 | 1.168 | 89.39 | 85.99 |-------------------|
0             Validation | 0.6479 (34, 11, 25, 29, 15) | 1.000 | 1.162 | 89.46 | 85.92 |###################| ^ (0.2%, 14.75, 2.2, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6853 (34, 10, 23, 30, 15) | 0.858 | 1.003 | 87.96 | 84.37 |---|
0             Validation | 0.6835 (34, 10, 23, 30, 15) | 0.873 | 1.018 | 87.73 | 84.51 |#####| ^ (0.5%, 15.75, 1.3, 14%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (936 batches)
0 >>  2/20 <<   Training | 0.6711 (36, 11, 25, 27, 14) | 0.930 | 1.084 | 88.25 | 85.17 |-----------|
0             Validation | 0.6691 (36, 11, 25, 28, 13) | 0.944 | 1.097 | 88.03 | 85.36 |#############| ^ (0.6%, 14.16, 1.1, 26%) 
0 >>  3/20 <<   Training | 0.6658 (35, 11, 25, 28, 14) | 0.933 | 1.088 | 88.48 | 85.18 |-----------|
0             Validation | 0.6632 (35, 11, 25, 28, 14) | 0.963 | 1.120 | 88.38 | 85.38 |#############| ^ (0.6%, 13.51, 1.7, 2%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (468 batches)
0 >>  4/20 <<   Training | 0.6619 (36, 11, 23, 28, 14) | 0.945 | 1.100 | 89.01 | 85.40 |-------------|
0             Validation | 0.6592 (36, 11, 23, 28, 14) | 0.966 | 1.122 | 88.89 | 85.59 |###############| ^ (0.6%, 13.08, 1.6, 4%) 
0 >>  5/20 <<   Training | 0.6599 (33, 11, 25, 28, 15) | 0.985 | 1.149 | 89.01 | 85.39 |-------------|
0             Validation | 0.6579 (33, 11, 25, 29, 15) | 1.006 | 1.171 | 88.86 | 85.57 |###############| ^ (0.6%, 13.34, 1.5, 5%) 
0 >>  6/20 <<   Training | 0.6598 (31, 12, 26, 30, 14) | 0.985 | 1.147 | 89.00 | 85.45 |--------------|
0             Validation | 0.6578 (32, 12, 25, 30, 14) | 1.005 | 1.168 | 88.78 | 85.64 |################| ^ (0.6%, 13.73, 1.5, 6%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (234 batches)
0 >>  7/20 <<   Training | 0.6563 (35, 11, 25, 28, 15) | 1.000 | 1.166 | 89.21 | 85.58 |---------------|
0             Validation | 0.6547 (35, 11, 25, 28, 15) | 1.029 | 1.197 | 89.02 | 85.76 |#################| ^ (0.6%, 12.22, 1.7, 2%) 
0 >>  8/20 <<   Training | 0.6584 (36, 12, 22, 28, 15) | 0.977 | 1.138 | 89.18 | 85.56 |---------------|
0             Validation | 0.6568 (36, 12, 22, 28, 15) | 0.993 | 1.157 | 88.94 | 85.75 |#################| ^ (0.6%, 14.01, 1.5, 5%) 
0 >>  9/20 <<   Training | 0.6564 (34, 11, 23, 30, 15) | 0.993 | 1.157 | 89.15 | 85.58 |---------------|
0             Validation | 0.6546 (34, 11, 23, 30, 15) | 1.014 | 1.179 | 88.95 | 85.77 |#################| ^ (0.6%, 12.93, 0.8, 63%) 
0 >> 10/20 <<   Training | 0.6555 (35, 11, 23, 28, 15) | 0.989 | 1.151 | 89.25 | 85.61 |----------------|
0             Validation | 0.6540 (35, 11, 23, 29, 15) | 1.003 | 1.166 | 88.99 | 85.79 |#################| ^ (0.6%, 13.14, 2.6, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (117 batches)
0 >> 11/20 <<   Training | 0.6553 (35, 11, 25, 28, 13) | 0.988 | 1.152 | 89.27 | 85.65 |----------------|
0             Validation | 0.6536 (35, 11, 25, 28, 13) | 1.019 | 1.185 | 89.10 | 85.83 |##################| ^ (0.6%, 13.70, 0.9, 57%) 
0 >> 12/20 <<   Training | 0.6546 (33, 11, 25, 28, 15) | 0.998 | 1.162 | 89.28 | 85.66 |----------------|
0             Validation | 0.6531 (33, 11, 25, 28, 15) | 1.030 | 1.196 | 89.03 | 85.83 |##################| ^ (0.5%, 13.19, 1.3, 15%) 
0 >> 13/20 <<   Training | 0.6553 (35, 11, 26, 27, 14) | 0.987 | 1.151 | 89.27 | 85.60 |----------------|
0             Validation | 0.6538 (35, 11, 25, 27, 14) | 1.013 | 1.179 | 89.03 | 85.78 |#################| ^ (0.6%, 13.21, 1.0, 38%) 
0 >> 14/20 <<   Training | 0.6547 (31, 12, 24, 30, 16) | 1.000 | 1.164 | 89.43 | 85.67 |----------------|
0             Validation | 0.6534 (31, 12, 24, 30, 16) | 1.034 | 1.200 | 89.17 | 85.84 |##################| ^ (0.5%, 12.53, 1.5, 5%) 
0 >> 15/20 <<   Training | 0.6550 (35, 11, 22, 30, 15) | 0.986 | 1.148 | 89.38 | 85.63 |----------------|
0             Validation | 0.6536 (35, 11, 22, 30, 15) | 1.005 | 1.167 | 89.13 | 85.82 |##################| ^ (0.5%, 13.19, 1.4, 8%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6530 (33, 11, 26, 28, 15) | 1.018 | 1.186 | 89.30 | 85.71 |-----------------|
0             Validation | 0.6517 (33, 11, 26, 28, 15) | 1.048 | 1.220 | 89.06 | 85.89 |##################| ^ (0.5%, 13.25, 1.3, 15%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6521 (34, 11, 24, 29, 15) | 1.017 | 1.185 | 89.42 | 85.74 |-----------------|
0             Validation | 0.6508 (34, 11, 24, 29, 15) | 1.040 | 1.209 | 89.18 | 85.92 |###################| ^ (0.5%, 13.19, 1.4, 8%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6520 (34, 11, 24, 29, 15) | 1.013 | 1.181 | 89.41 | 85.75 |-----------------|
0             Validation | 0.6507 (34, 11, 24, 29, 15) | 1.041 | 1.211 | 89.16 | 85.92 |###################| ^ (0.5%, 13.22, 1.3, 15%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6520 (34, 11, 24, 29, 15) | 1.012 | 1.179 | 89.42 | 85.75 |-----------------|
0             Validation | 0.6507 (34, 11, 24, 29, 15) | 1.042 | 1.212 | 89.16 | 85.92 |###################| ^ (0.5%, 13.21, 1.4, 10%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6520 (34, 11, 24, 29, 15) | 1.012 | 1.180 | 89.42 | 85.75 |-----------------|
0             Validation | 0.6507 (34, 11, 24, 29, 15) | 1.041 | 1.211 | 89.16 | 85.92 |###################| ^ (0.5%, 13.21, 1.4, 8%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6806 (38, 10, 26, 26, 13) | 0.998 | 1.167 | 87.84 | 84.72 |-------|
0             Validation | 0.6792 (38, 10, 26, 26, 12) | 1.016 | 1.193 | 88.12 | 84.72 |#######| ^ (0.2%, 10.27, 2.0, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (930 batches)
0 >>  2/20 <<   Training | 0.6704 (33, 11, 31, 25, 12) | 1.061 | 1.241 | 88.05 | 85.07 |----------|
0             Validation | 0.6697 (33, 11, 31, 25, 12) | 1.077 | 1.264 | 88.27 | 85.05 |##########| ^ (0.3%, 9.66, 1.1, 24%) 
0 >>  3/20 <<   Training | 0.6642 (34, 11, 28, 26, 13) | 1.110 | 1.294 | 88.22 | 85.35 |-------------|
0             Validation | 0.6636 (34, 11, 28, 26, 13) | 1.118 | 1.309 | 88.42 | 85.33 |#############| ^ (0.2%, 9.29, 1.2, 20%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (465 batches)
0 >>  4/20 <<   Training | 0.6618 (35, 11, 26, 26, 14) | 1.116 | 1.301 | 88.64 | 85.38 |-------------|
0             Validation | 0.6609 (35, 11, 26, 26, 14) | 1.127 | 1.319 | 88.81 | 85.38 |#############| ^ (0.2%, 10.02, 1.3, 16%) 
0 >>  5/20 <<   Training | 0.6608 (32, 11, 30, 26, 13) | 1.164 | 1.358 | 88.70 | 85.42 |--------------|
0             Validation | 0.6601 (32, 11, 30, 26, 13) | 1.181 | 1.384 | 88.84 | 85.43 |##############| ^ (0.2%, 10.74, 0.9, 50%) 
0 >>  6/20 <<   Training | 0.6612 (33, 11, 25, 29, 14) | 1.144 | 1.334 | 88.77 | 85.44 |--------------|
0             Validation | 0.6602 (33, 11, 25, 29, 14) | 1.168 | 1.367 | 89.02 | 85.47 |##############| ^ (0.2%, 10.18, 1.4, 10%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (232 batches)
0 >>  7/20 <<   Training | 0.6568 (35, 11, 25, 27, 13) | 1.133 | 1.321 | 88.96 | 85.60 |----------------|
0             Validation | 0.6557 (35, 11, 25, 27, 13) | 1.157 | 1.354 | 89.20 | 85.61 |################| ^ (0.3%, 9.71, 1.0, 36%) 
0 >>  8/20 <<   Training | 0.6550 (34, 11, 27, 27, 13) | 1.153 | 1.344 | 89.00 | 85.64 |----------------|
0             Validation | 0.6543 (34, 11, 27, 27, 13) | 1.168 | 1.365 | 89.20 | 85.63 |################| ^ (0.3%, 9.46, 1.3, 16%) 
0 >>  9/20 <<   Training | 0.6556 (34, 11, 27, 27, 13) | 1.170 | 1.364 | 88.97 | 85.65 |----------------|
0             Validation | 0.6550 (34, 11, 27, 27, 13) | 1.187 | 1.388 | 89.10 | 85.67 |################| ^ (0.2%, 9.73, 1.0, 39%) 
0 >> 10/20 <<   Training | 0.6554 (36, 11, 26, 27, 12) | 1.148 | 1.336 | 89.01 | 85.66 |----------------|
0             Validation | 0.6546 (36, 11, 26, 27, 12) | 1.177 | 1.377 | 89.20 | 85.67 |################| ^ (0.2%, 9.83, 1.4, 8%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (116 batches)
0 >> 11/20 <<   Training | 0.6534 (33, 11, 27, 27, 13) | 1.177 | 1.371 | 89.04 | 85.72 |-----------------|
0             Validation | 0.6526 (33, 11, 27, 27, 13) | 1.191 | 1.391 | 89.21 | 85.72 |#################| ^ (0.2%, 10.46, 1.4, 9%) 
0 >> 12/20 <<   Training | 0.6545 (31, 11, 28, 28, 14) | 1.184 | 1.379 | 88.97 | 85.70 |----------------|
0             Validation | 0.6535 (31, 11, 28, 28, 14) | 1.204 | 1.408 | 89.19 | 85.70 |#################| ^ (0.2%, 9.64, 1.1, 28%) 
0 >> 13/20 <<   Training | 0.6556 (36, 11, 25, 26, 13) | 1.147 | 1.336 | 89.08 | 85.63 |----------------|
0             Validation | 0.6545 (36, 11, 25, 26, 13) | 1.155 | 1.352 | 89.27 | 85.64 |################| ^ (0.2%, 9.94, 0.8, 66%) 
0 >> 14/20 <<   Training | 0.6538 (34, 11, 26, 27, 14) | 1.163 | 1.355 | 88.96 | 85.72 |-----------------|
0             Validation | 0.6530 (34, 11, 26, 28, 14) | 1.175 | 1.374 | 89.17 | 85.72 |#################| ^ (0.2%, 9.67, 0.7, 81%) 
0 >> 15/20 <<   Training | 0.6565 (38, 11, 27, 24, 13) | 1.136 | 1.323 | 89.13 | 85.70 |----------------|
0             Validation | 0.6555 (38, 11, 27, 24, 13) | 1.156 | 1.351 | 89.31 | 85.70 |#################| ^ (0.2%, 10.04, 1.5, 5%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6522 (34, 11, 27, 27, 13) | 1.177 | 1.371 | 89.12 | 85.76 |-----------------|
0             Validation | 0.6513 (34, 11, 27, 27, 13) | 1.201 | 1.404 | 89.30 | 85.78 |#################| ^ (0.2%, 9.79, 0.8, 62%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6515 (34, 11, 27, 27, 13) | 1.180 | 1.375 | 89.17 | 85.79 |-----------------|
0             Validation | 0.6506 (34, 11, 27, 27, 13) | 1.197 | 1.400 | 89.37 | 85.80 |#################| ^ (0.2%, 9.55, 1.2, 22%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6515 (33, 11, 27, 27, 13) | 1.189 | 1.385 | 89.16 | 85.79 |-----------------|
0             Validation | 0.6506 (33, 11, 27, 27, 13) | 1.202 | 1.405 | 89.35 | 85.80 |#################| ^ (0.2%, 9.60, 1.0, 45%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6514 (34, 11, 27, 27, 13) | 1.183 | 1.377 | 89.17 | 85.79 |-----------------|
0             Validation | 0.6506 (34, 11, 27, 27, 13) | 1.202 | 1.405 | 89.36 | 85.80 |#################| ^ (0.2%, 9.57, 0.9, 55%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6514 (34, 11, 27, 27, 13) | 1.183 | 1.378 | 89.16 | 85.79 |-----------------|
0             Validation | 0.6506 (34, 11, 27, 27, 13) | 1.202 | 1.405 | 89.36 | 85.80 |#################| ^ (0.2%, 9.57, 0.9, 53%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6935 (39, 10, 26, 25, 12) | 1.018 | 1.199 | 87.89 | 84.16 |-|
0             Validation | 0.6909 (39, 10, 26, 26, 12) | 1.028 | 1.209 | 87.93 | 84.29 |##| ^ (0.4%, 10.21, 0.6, 88%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (936 batches)
0 >>  2/20 <<   Training | 0.6701 (36, 11, 26, 26, 13) | 1.068 | 1.252 | 88.45 | 85.09 |----------|
0             Validation | 0.6670 (36, 11, 26, 26, 13) | 1.084 | 1.267 | 88.45 | 85.28 |############| ^ (0.6%, 10.35, 1.3, 14%) 
0 >>  3/20 <<   Training | 0.6655 (31, 11, 29, 28, 13) | 1.165 | 1.359 | 88.59 | 85.31 |-------------|
0             Validation | 0.6632 (31, 11, 29, 28, 13) | 1.181 | 1.376 | 88.60 | 85.45 |##############| ^ (0.4%, 11.68, 0.5, 92%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (468 batches)
0 >>  4/20 <<   Training | 0.6657 (28, 11, 30, 28, 14) | 1.146 | 1.338 | 88.66 | 85.40 |-------------|
0             Validation | 0.6628 (28, 11, 30, 28, 14) | 1.164 | 1.356 | 88.64 | 85.58 |###############| ^ (0.5%, 12.48, 1.5, 6%) 
0 >>  5/20 <<   Training | 0.6603 (33, 11, 28, 27, 14) | 1.153 | 1.347 | 89.01 | 85.37 |-------------|
0             Validation | 0.6576 (33, 11, 28, 27, 14) | 1.171 | 1.366 | 88.96 | 85.55 |###############| ^ (0.5%, 10.66, 1.9, 1%) 
0 >>  6/20 <<   Training | 0.6578 (35, 11, 26, 27, 13) | 1.162 | 1.357 | 88.92 | 85.52 |---------------|
0             Validation | 0.6552 (35, 11, 26, 27, 13) | 1.173 | 1.368 | 88.88 | 85.71 |#################| ^ (0.5%, 10.70, 1.1, 25%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (234 batches)
0 >>  7/20 <<   Training | 0.6557 (33, 11, 27, 28, 14) | 1.164 | 1.359 | 89.07 | 85.60 |---------------|
0             Validation | 0.6535 (33, 11, 27, 28, 14) | 1.190 | 1.386 | 89.02 | 85.78 |#################| ^ (0.5%, 10.34, 1.3, 16%) 
0 >>  8/20 <<   Training | 0.6548 (34, 11, 27, 27, 13) | 1.165 | 1.359 | 89.15 | 85.63 |----------------|
0             Validation | 0.6524 (34, 11, 27, 27, 13) | 1.184 | 1.380 | 89.11 | 85.82 |##################| ^ (0.5%, 10.41, 1.1, 28%) 
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6970 (37, 12, 21, 29, 14) | 0.800 | 0.938 | 88.33 | 84.11 |-|
0             Validation | 0.6953 (37, 12, 21, 29, 14) | 0.813 | 0.950 | 88.27 | 84.24 |##| ^ (0.4%, 10.86, 1.2, 16%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (936 batches)
0 >>  2/20 <<   Training | 0.6753 (30, 11, 24, 32, 15) | 0.880 | 1.027 | 88.84 | 84.93 |---------|
0             Validation | 0.6743 (30, 11, 24, 32, 15) | 0.887 | 1.033 | 88.62 | 85.06 |##########| ^ (0.5%, 10.55, 1.0, 35%) 
0 >>  3/20 <<   Training | 0.6699 (32, 11, 23, 31, 15) | 0.874 | 1.021 | 89.14 | 85.04 |----------|
0             Validation | 0.6690 (32, 11, 23, 31, 15) | 0.885 | 1.030 | 88.86 | 85.18 |###########| ^ (0.5%, 10.73, 1.4, 8%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (468 batches)
0 >>  4/20 <<   Training | 0.6663 (32, 12, 23, 31, 16) | 0.908 | 1.060 | 89.27 | 85.22 |------------|
0             Validation | 0.6653 (32, 12, 23, 31, 16) | 0.921 | 1.071 | 89.10 | 85.33 |#############| ^ (0.4%, 10.92, 1.4, 8%) 
0 >>  5/20 <<   Training | 0.6663 (34, 11, 22, 31, 15) | 0.913 | 1.065 | 89.52 | 85.30 |------------|
0             Validation | 0.6651 (34, 11, 22, 31, 15) | 0.926 | 1.080 | 89.38 | 85.42 |##############| ^ (0.4%, 11.06, 1.0, 35%) 
0 >>  6/20 <<   Training | 0.6678 (34, 11, 22, 31, 15) | 0.892 | 1.042 | 89.28 | 85.24 |------------|
0             Validation | 0.6667 (34, 11, 22, 31, 15) | 0.901 | 1.049 | 89.17 | 85.37 |#############| ^ (0.4%, 10.98, 1.1, 28%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (234 batches)
0 >>  7/20 <<   Training | 0.6632 (32, 12, 23, 31, 15) | 0.919 | 1.071 | 89.58 | 85.45 |--------------|
0             Validation | 0.6624 (32, 12, 23, 31, 15) | 0.939 | 1.093 | 89.39 | 85.55 |###############| ^ (0.4%, 10.52, 1.3, 15%) 
0 >>  8/20 <<   Training | 0.6640 (32, 11, 23, 31, 16) | 0.916 | 1.067 | 89.44 | 85.48 |--------------|
0             Validation | 0.6629 (32, 11, 22, 31, 16) | 0.932 | 1.084 | 89.24 | 85.61 |################| ^ (0.4%, 10.97, 1.4, 9%) 
0 >>  9/20 <<   Training | 0.6647 (33, 12, 22, 31, 16) | 0.926 | 1.078 | 89.59 | 85.50 |---------------|
0             Validation | 0.6639 (33, 11, 22, 31, 16) | 0.945 | 1.099 | 89.42 | 85.61 |################| ^ (0.3%, 10.46, 1.2, 16%) 
0 >> 10/20 <<   Training | 0.6691 (35, 12, 20, 31, 15) | 0.905 | 1.053 | 89.22 | 85.47 |--------------|
0             Validation | 0.6685 (35, 12, 20, 31, 15) | 0.914 | 1.061 | 88.98 | 85.58 |###############| ^ (0.4%, 11.15, 0.7, 75%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (117 batches)
0 >> 11/20 <<   Training | 0.6604 (34, 11, 23, 31, 14) | 0.913 | 1.064 | 89.55 | 85.55 |---------------|
0             Validation | 0.6594 (34, 11, 23, 31, 14) | 0.930 | 1.082 | 89.33 | 85.66 |################| ^ (0.4%, 10.92, 0.5, 94%) 
0 >> 12/20 <<   Training | 0.6638 (30, 11, 24, 32, 16) | 0.939 | 1.094 | 89.43 | 85.53 |---------------|
0             Validation | 0.6629 (30, 11, 24, 32, 16) | 0.953 | 1.108 | 89.21 | 85.65 |################| ^ (0.4%, 10.95, 1.2, 20%) 
0 >> 13/20 <<   Training | 0.6609 (33, 11, 22, 31, 16) | 0.946 | 1.102 | 89.67 | 85.61 |----------------|
0             Validation | 0.6602 (33, 11, 22, 31, 16) | 0.959 | 1.115 | 89.44 | 85.71 |#################| ^ (0.4%, 11.00, 0.8, 67%) 
0 >> 14/20 <<   Training | 0.6600 (33, 11, 23, 31, 15) | 0.922 | 1.073 | 89.75 | 85.52 |---------------|
0             Validation | 0.6590 (33, 11, 23, 31, 15) | 0.943 | 1.095 | 89.46 | 85.64 |################| ^ (0.4%, 10.94, 1.0, 39%) 
0 >> 15/20 <<   Training | 0.6608 (36, 11, 22, 30, 14) | 0.926 | 1.078 | 89.78 | 85.55 |---------------|
0             Validation | 0.6600 (36, 11, 22, 30, 14) | 0.941 | 1.095 | 89.61 | 85.64 |################| ^ (0.4%, 11.07, 1.3, 12%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6592 (32, 11, 22, 32, 15) | 0.948 | 1.104 | 89.77 | 85.66 |----------------|
0             Validation | 0.6584 (32, 11, 22, 32, 15) | 0.967 | 1.125 | 89.51 | 85.78 |#################| ^ (0.4%, 11.06, 1.1, 29%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6581 (33, 11, 23, 31, 15) | 0.938 | 1.093 | 89.79 | 85.64 |----------------|
0             Validation | 0.6574 (33, 11, 23, 31, 15) | 0.955 | 1.109 | 89.52 | 85.76 |#################| ^ (0.4%, 10.91, 0.7, 75%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6584 (33, 11, 22, 31, 15) | 0.942 | 1.097 | 89.79 | 85.66 |----------------|
0             Validation | 0.6577 (33, 11, 22, 31, 15) | 0.959 | 1.115 | 89.53 | 85.77 |#################| ^ (0.4%, 10.94, 1.0, 37%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6584 (33, 11, 22, 31, 15) | 0.943 | 1.098 | 89.79 | 85.66 |----------------|
0             Validation | 0.6577 (33, 11, 22, 31, 15) | 0.961 | 1.116 | 89.53 | 85.77 |#################| ^ (0.4%, 10.95, 1.0, 45%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6585 (33, 11, 22, 31, 15) | 0.943 | 1.098 | 89.79 | 85.66 |----------------|
0             Validation | 0.6577 (33, 11, 22, 31, 15) | 0.961 | 1.117 | 89.53 | 85.77 |#################| ^ (0.4%, 10.95, 0.9, 46%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6873 (39, 12, 22, 32, 16) | 0.787 | 0.924 | 88.63 | 84.09 ||
0             Validation | 0.6865 (39, 12, 22, 32, 16) | 0.815 | 0.952 | 88.44 | 84.22 |##| ^ (0.5%, 12.90, 0.8, 67%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (936 batches)
0 >>  2/20 <<   Training | 0.6695 (33, 10, 25, 36, 17) | 0.839 | 0.982 | 88.96 | 84.90 |--------|
0             Validation | 0.6682 (33, 10, 25, 36, 17) | 0.852 | 0.992 | 88.75 | 85.05 |##########| ^ (0.5%, 11.33, 1.8, 1%) 
0 >>  3/20 <<   Training | 0.6639 (35, 11, 26, 34, 16) | 0.831 | 0.972 | 89.17 | 84.81 |--------|
0             Validation | 0.6624 (35, 11, 26, 34, 15) | 0.831 | 0.971 | 88.99 | 84.98 |#########| ^ (0.6%, 12.92, 1.1, 28%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (468 batches)
0 >>  4/20 <<   Training | 0.6573 (34, 11, 25, 33, 17) | 0.889 | 1.036 | 89.23 | 85.21 |------------|
0             Validation | 0.6561 (34, 11, 24, 33, 17) | 0.892 | 1.038 | 89.03 | 85.36 |#############| ^ (0.5%, 13.60, 1.1, 28%) 
0 >>  5/20 <<   Training | 0.6564 (36, 11, 24, 33, 17) | 0.863 | 1.008 | 89.49 | 85.26 |------------|
0             Validation | 0.6547 (36, 11, 24, 33, 17) | 0.872 | 1.016 | 89.34 | 85.45 |##############| ^ (0.6%, 11.44, 0.7, 81%) 
0 >>  6/20 <<   Training | 0.6576 (35, 10, 25, 34, 17) | 0.885 | 1.034 | 89.28 | 85.20 |------------|
0             Validation | 0.6565 (35, 10, 25, 34, 17) | 0.897 | 1.045 | 89.13 | 85.35 |#############| ^ (0.5%, 11.54, 1.0, 39%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (234 batches)
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6834 (38, 10, 33, 32, 17) | 0.678 | 0.802 | 88.71 | 83.60 ||
0             Validation | 0.6806 (38, 10, 33, 33, 17) | 0.690 | 0.814 | 88.61 | 83.80 || ^ (0.7%, 23.90, 1.9, 1%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (936 batches)
0 >>  2/20 <<   Training | 0.6670 (38,  9, 33, 34, 17) | 0.674 | 0.797 | 89.31 | 84.18 |-|
0             Validation | 0.6650 (38,  9, 32, 34, 17) | 0.687 | 0.807 | 89.15 | 84.38 |###| ^ (0.7%, 20.96, 1.6, 4%) 
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6834 (38, 10, 33, 32, 17) | 0.707 | 0.833 | 88.71 | 83.37 ||
0             Validation | 0.6806 (38, 10, 33, 33, 17) | 0.721 | 0.847 | 88.61 | 83.50 || ^ (0.5%, 23.90, 1.9, 1%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (936 batches)
0 >>  2/20 <<   Training | 0.6670 (38,  9, 33, 34, 17) | 0.767 | 0.899 | 89.31 | 84.49 |----|
0             Validation | 0.6650 (38,  9, 32, 34, 17) | 0.782 | 0.912 | 89.15 | 84.63 |######| ^ (0.5%, 20.96, 1.6, 4%) 
0 >>  3/20 <<   Training | 0.6697 (35,  9, 35, 34, 17) | 0.777 | 0.911 | 89.06 | 84.65 |------|
0             Validation | 0.6681 (35,  9, 35, 34, 17) | 0.803 | 0.938 | 88.93 | 84.79 |#######| ^ (0.4%, 25.34, 2.1, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (468 batches)
0 >>  4/20 <<   Training | 0.6583 (34,  9, 35, 34, 18) | 0.821 | 0.960 | 89.59 | 84.81 |--------|
0             Validation | 0.6570 (34,  9, 35, 34, 18) | 0.832 | 0.970 | 89.40 | 84.91 |#########| ^ (0.4%, 27.03, 1.6, 4%) 
0 >>  5/20 <<   Training | 0.6579 (37,  9, 33, 34, 18) | 0.769 | 0.900 | 89.65 | 85.01 |----------|
0             Validation | 0.6567 (37,  9, 33, 34, 18) | 0.792 | 0.924 | 89.53 | 85.11 |###########| ^ (0.4%, 24.32, 1.6, 3%) 
0 >>  6/20 <<   Training | 0.6589 (34,  9, 33, 36, 18) | 0.812 | 0.950 | 89.10 | 85.11 |-----------|
0             Validation | 0.6576 (34,  9, 33, 36, 18) | 0.834 | 0.974 | 89.00 | 85.20 |###########| ^ (0.4%, 23.17, 2.6, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (234 batches)
0 >>  7/20 <<   Training | 0.6577 (34,  9, 35, 34, 18) | 0.799 | 0.934 | 89.77 | 85.04 |----------|
0             Validation | 0.6564 (34,  9, 35, 34, 18) | 0.824 | 0.961 | 89.66 | 85.15 |###########| ^ (0.4%, 26.31, 1.2, 22%) 
0 >>  8/20 <<   Training | 0.6593 (35,  9, 34, 34, 19) | 0.794 | 0.930 | 89.35 | 85.11 |-----------|
0             Validation | 0.6584 (35,  9, 34, 34, 19) | 0.822 | 0.961 | 89.26 | 85.18 |###########| ^ (0.4%, 27.67, 1.3, 15%) 
0 >>  9/20 <<   Training | 0.6571 (35,  9, 34, 34, 18) | 0.827 | 0.966 | 89.65 | 85.13 |-----------|
0             Validation | 0.6560 (35,  9, 34, 34, 18) | 0.836 | 0.974 | 89.52 | 85.23 |############| ^ (0.4%, 25.47, 1.8, 1%) 
0 >> 10/20 <<   Training | 0.6568 (35,  9, 33, 33, 19) | 0.806 | 0.941 | 89.51 | 85.09 |----------|
0             Validation | 0.6558 (35,  9, 33, 33, 19) | 0.829 | 0.968 | 89.34 | 85.19 |###########| ^ (0.4%, 27.25, 1.4, 9%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (117 batches)
0 >> 11/20 <<   Training | 0.6552 (33,  9, 37, 35, 17) | 0.791 | 0.925 | 89.65 | 85.15 |-----------|
0             Validation | 0.6540 (33,  9, 37, 35, 17) | 0.812 | 0.947 | 89.50 | 85.24 |############| ^ (0.4%, 26.84, 1.9, 1%) 
0 >> 12/20 <<   Training | 0.6522 (36,  9, 35, 33, 18) | 0.818 | 0.957 | 89.67 | 85.23 |------------|
0             Validation | 0.6511 (36,  9, 35, 33, 18) | 0.840 | 0.980 | 89.55 | 85.31 |#############| ^ (0.4%, 25.95, 1.6, 4%) 
0 >> 13/20 <<   Training | 0.6548 (35,  9, 33, 35, 18) | 0.837 | 0.978 | 89.83 | 85.17 |-----------|
0             Validation | 0.6540 (35,  9, 33, 35, 18) | 0.867 | 1.012 | 89.67 | 85.26 |############| ^ (0.4%, 28.12, 1.1, 25%) 
0 >> 14/20 <<   Training | 0.6530 (36,  9, 33, 34, 18) | 0.815 | 0.953 | 89.92 | 85.14 |-----------|
0             Validation | 0.6521 (36,  9, 33, 34, 18) | 0.837 | 0.977 | 89.74 | 85.22 |############| ^ (0.4%, 24.33, 1.8, 1%) 
0 >> 15/20 <<   Training | 0.6516 (38,  9, 34, 33, 17) | 0.799 | 0.935 | 89.93 | 85.11 |-----------|
0             Validation | 0.6507 (37,  9, 34, 33, 17) | 0.820 | 0.958 | 89.79 | 85.18 |###########| ^ (0.4%, 26.75, 2.1, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6522 (35,  9, 33, 35, 18) | 0.816 | 0.955 | 89.85 | 85.26 |------------|
0             Validation | 0.6513 (35,  9, 33, 35, 18) | 0.844 | 0.985 | 89.72 | 85.34 |#############| ^ (0.4%, 25.86, 1.4, 9%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6521 (35,  9, 34, 34, 18) | 0.822 | 0.962 | 89.96 | 85.28 |------------|
0             Validation | 0.6513 (35,  9, 34, 34, 18) | 0.845 | 0.986 | 89.80 | 85.35 |#############| ^ (0.4%, 25.18, 1.6, 3%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6517 (36,  9, 34, 34, 18) | 0.822 | 0.962 | 89.94 | 85.28 |------------|
0             Validation | 0.6508 (36,  9, 34, 34, 18) | 0.846 | 0.988 | 89.79 | 85.35 |#############| ^ (0.4%, 25.47, 1.2, 21%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6519 (35,  9, 34, 34, 18) | 0.824 | 0.964 | 89.93 | 85.28 |------------|
0             Validation | 0.6510 (35,  9, 34, 34, 18) | 0.846 | 0.987 | 89.78 | 85.36 |#############| ^ (0.4%, 25.57, 1.3, 12%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6519 (35,  9, 34, 34, 18) | 0.824 | 0.964 | 89.93 | 85.28 |------------|
0             Validation | 0.6511 (35,  9, 34, 34, 18) | 0.848 | 0.989 | 89.78 | 85.36 |#############| ^ (0.4%, 25.56, 1.4, 7%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6811 (38, 10, 18, 35, 20) | 0.880 | 1.035 | 88.38 | 84.55 |-----|
0             Validation | 0.6794 (38, 10, 18, 35, 20) | 0.897 | 1.050 | 88.16 | 84.69 |######| ^ (0.5%, 10.70, 1.5, 5%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (936 batches)
0 >>  2/20 <<   Training | 0.6635 (39, 11, 19, 32, 20) | 0.893 | 1.042 | 88.38 | 85.26 |------------|
0             Validation | 0.6615 (39, 11, 19, 32, 20) | 0.910 | 1.060 | 88.10 | 85.45 |##############| ^ (0.6%, 13.35, 1.0, 45%) 
0 >>  3/20 <<   Training | 0.6547 (38, 11, 20, 33, 20) | 0.906 | 1.057 | 88.93 | 85.20 |-----------|
0             Validation | 0.6522 (38, 11, 20, 33, 20) | 0.926 | 1.077 | 88.69 | 85.41 |##############| ^ (0.6%, 12.44, 1.2, 18%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (468 batches)
0 >>  4/20 <<   Training | 0.6545 (36, 11, 19, 34, 21) | 0.943 | 1.099 | 88.92 | 85.46 |--------------|
0             Validation | 0.6520 (36, 11, 19, 34, 21) | 0.957 | 1.113 | 88.71 | 85.66 |################| ^ (0.6%, 13.76, 0.9, 57%) 
0 >>  5/20 <<   Training | 0.6524 (39, 11, 18, 32, 21) | 0.923 | 1.077 | 89.33 | 85.52 |---------------|
0             Validation | 0.6503 (39, 11, 18, 32, 21) | 0.962 | 1.119 | 89.17 | 85.72 |#################| ^ (0.6%, 13.10, 0.9, 50%) 
0 >>  6/20 <<   Training | 0.6553 (36, 10, 18, 35, 21) | 0.933 | 1.089 | 89.13 | 85.46 |--------------|
0             Validation | 0.6534 (36, 10, 18, 35, 21) | 0.954 | 1.110 | 89.00 | 85.66 |################| ^ (0.6%, 13.15, 1.5, 5%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (234 batches)
0 >>  7/20 <<   Training | 0.6492 (38, 11, 18, 33, 21) | 0.919 | 1.075 | 89.50 | 85.57 |---------------|
0             Validation | 0.6476 (39, 11, 18, 33, 21) | 0.945 | 1.101 | 89.31 | 85.74 |#################| ^ (0.5%, 12.85, 0.8, 62%) 
0 >>  8/20 <<   Training | 0.6497 (37, 11, 19, 34, 21) | 0.940 | 1.099 | 89.47 | 85.53 |---------------|
0             Validation | 0.6478 (37, 11, 19, 34, 21) | 0.970 | 1.129 | 89.27 | 85.73 |#################| ^ (0.6%, 12.57, 1.0, 42%) 
0 >>  9/20 <<   Training | 0.6540 (40, 11, 17, 33, 20) | 0.934 | 1.090 | 89.53 | 85.66 |----------------|
0             Validation | 0.6524 (40, 11, 17, 33, 20) | 0.955 | 1.112 | 89.38 | 85.84 |##################| ^ (0.6%, 12.96, 1.2, 21%) 
0 >> 10/20 <<   Training | 0.6543 (40, 11, 17, 32, 21) | 0.927 | 1.082 | 89.46 | 85.63 |----------------|
0             Validation | 0.6527 (40, 11, 17, 32, 21) | 0.957 | 1.113 | 89.28 | 85.83 |##################| ^ (0.6%, 13.10, 1.4, 9%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (117 batches)
0 >> 11/20 <<   Training | 0.6515 (41, 11, 18, 33, 19) | 0.918 | 1.071 | 89.43 | 85.60 |----------------|
0             Validation | 0.6498 (41, 11, 17, 33, 20) | 0.940 | 1.094 | 89.28 | 85.80 |#################| ^ (0.6%, 12.38, 1.2, 21%) 
0 >> 12/20 <<   Training | 0.6459 (38, 11, 19, 33, 20) | 0.945 | 1.105 | 89.37 | 85.58 |---------------|
0             Validation | 0.6442 (38, 11, 19, 33, 20) | 0.972 | 1.132 | 89.18 | 85.76 |#################| ^ (0.6%, 12.27, 0.9, 51%) 
0 >> 13/20 <<   Training | 0.6520 (40, 11, 17, 33, 21) | 0.940 | 1.099 | 89.51 | 85.66 |----------------|
0             Validation | 0.6506 (40, 11, 17, 33, 21) | 0.977 | 1.137 | 89.29 | 85.85 |##################| ^ (0.6%, 12.48, 1.4, 10%) 
0 >> 14/20 <<   Training | 0.6473 (38, 11, 19, 33, 21) | 0.972 | 1.135 | 89.47 | 85.71 |-----------------|
0             Validation | 0.6458 (38, 11, 18, 34, 21) | 0.985 | 1.147 | 89.26 | 85.90 |###################| ^ (0.6%, 11.93, 1.5, 5%) 
0 >> 15/20 <<   Training | 0.6474 (39, 11, 18, 33, 20) | 0.942 | 1.100 | 89.49 | 85.68 |----------------|
0             Validation | 0.6460 (39, 11, 18, 34, 20) | 0.979 | 1.139 | 89.36 | 85.86 |##################| ^ (0.5%, 12.32, 1.4, 10%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6466 (39, 11, 18, 33, 20) | 0.942 | 1.099 | 89.51 | 85.72 |-----------------|
0             Validation | 0.6452 (39, 11, 18, 33, 20) | 0.964 | 1.121 | 89.34 | 85.90 |###################| ^ (0.5%, 12.29, 0.7, 79%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6461 (39, 11, 18, 32, 20) | 0.955 | 1.115 | 89.55 | 85.75 |-----------------|
0             Validation | 0.6446 (39, 11, 18, 33, 21) | 0.980 | 1.140 | 89.35 | 85.93 |###################| ^ (0.6%, 12.12, 0.7, 79%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6466 (39, 11, 18, 33, 20) | 0.954 | 1.113 | 89.56 | 85.75 |-----------------|
0             Validation | 0.6451 (39, 11, 18, 33, 20) | 0.982 | 1.143 | 89.38 | 85.93 |###################| ^ (0.6%, 12.14, 0.7, 75%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6465 (39, 11, 18, 33, 20) | 0.957 | 1.117 | 89.56 | 85.75 |-----------------|
0             Validation | 0.6451 (39, 11, 18, 33, 20) | 0.983 | 1.144 | 89.37 | 85.94 |###################| ^ (0.6%, 12.15, 0.6, 87%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6466 (39, 11, 18, 33, 20) | 0.957 | 1.117 | 89.56 | 85.75 |-----------------|
0             Validation | 0.6451 (39, 11, 18, 33, 20) | 0.983 | 1.144 | 89.37 | 85.94 |###################| ^ (0.6%, 12.15, 0.6, 86%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
