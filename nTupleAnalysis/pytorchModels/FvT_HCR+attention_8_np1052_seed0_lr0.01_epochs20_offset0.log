0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.8852 (24, 32,  7, 20) | 0.946 | 10.107 | 83.48 | 67.82 |----------------------------|
0             Validation | 0.8870 (24, 32,  7, 20) | 0.945 | 4.419 | 83.38 | 67.33 |#######################| ^ (2.7%, 2.58, 1.3, 16%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1337 batches)
0 >>  2/20 <<   Training | 0.8760 (25, 32,  7, 19) | 0.990 | 5.183 | 83.93 | 68.28 |--------------------------------|
0             Validation | 0.8783 (25, 32,  7, 19) | 0.988 | 3.665 | 83.84 | 67.85 |############################| ^ (2.3%, 3.82, 1.6, 4%) 
0 >>  3/20 <<   Training | 0.8751 (25, 29,  7, 21) | 0.979 | 9.122 | 84.30 | 68.88 |--------------------------------------|
0             Validation | 0.8768 (25, 29,  7, 21) | 0.978 | 6.314 | 84.21 | 68.59 |###################################| ^ (1.5%, 3.82, 2.2, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (668 batches)
0 >>  4/20 <<   Training | 0.8671 (27, 32,  7, 18) | 1.067 | 5.176 | 84.87 | 68.67 |------------------------------------|
0             Validation | 0.8695 (27, 32,  7, 18) | 1.065 | 3.088 | 84.76 | 68.25 |################################| ^ (2.3%, 5.75, 1.3, 12%) 
0 >>  5/20 <<   Training | 0.8716 (26, 28,  7, 21) | 1.050 | 4.596 | 84.84 | 68.75 |-------------------------------------|
0             Validation | 0.8737 (26, 28,  7, 21) | 1.048 | 3.330 | 84.75 | 68.37 |#################################| ^ (2.1%, 5.47, 1.7, 2%) 
0 >>  6/20 <<   Training | 0.8665 (26, 31,  7, 19) | 1.013 | 2.822 | 84.76 | 68.82 |--------------------------------------|
0             Validation | 0.8692 (26, 31,  7, 19) | 1.011 | 2.360 | 84.65 | 68.34 |#################################| ^ (2.6%, 4.67, 1.2, 20%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (334 batches)
0 >>  7/20 <<   Training | 0.8640 (26, 30,  7, 20) | 1.017 | 1.912 | 85.08 | 69.38 |-------------------------------------------|
0             Validation | 0.8665 (26, 30,  7, 20) | 1.016 | 1.595 | 84.98 | 68.96 |#######################################| ^ (2.2%, 4.32, 1.3, 11%) 
0 >>  8/20 <<   Training | 0.8636 (25, 31,  7, 20) | 0.959 | 3.748 | 85.09 | 69.85 |------------------------------------------------|
0             Validation | 0.8663 (25, 31,  7, 20) | 0.958 | 3.075 | 84.97 | 69.41 |############################################| ^ (2.2%, 4.13, 2.3, 0%) 
0 >>  9/20 <<   Training | 0.8627 (25, 31,  7, 19) | 1.008 | 2.473 | 85.10 | 69.95 |-------------------------------------------------|
0             Validation | 0.8653 (25, 31,  7, 19) | 1.006 | 1.929 | 84.98 | 69.48 |############################################| ^ (2.4%, 4.13, 1.3, 13%) 
0 >> 10/20 <<   Training | 0.8632 (26, 31,  7, 19) | 1.040 | 3.371 | 85.09 | 70.03 |--------------------------------------------------|
0             Validation | 0.8659 (26, 31,  7, 19) | 1.039 | 2.164 | 84.98 | 69.53 |#############################################| ^ (2.5%, 3.47, 2.4, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (167 batches)
0 >> 11/20 <<   Training | 0.8638 (27, 31,  7, 18) | 1.099 | 9.639 | 85.21 | 70.06 |--------------------------------------------------|
0             Validation | 0.8665 (27, 31,  7, 18) | 1.097 | 4.579 | 85.10 | 69.58 |#############################################| ^ (2.4%, 3.76, 1.5, 5%) 
0 >> 12/20 <<   Training | 0.8624 (25, 33,  7, 19) | 0.927 | 10.382 | 85.16 | 69.99 |-------------------------------------------------|
0             Validation | 0.8653 (25, 33,  7, 19) | 0.926 | 7.034 | 85.05 | 69.53 |#############################################| ^ (2.3%, 4.07, 2.0, 0%) 
0 >> 13/20 <<   Training | 0.8629 (24, 34,  7, 18) | 0.916 | 8.813 | 85.15 | 69.89 |------------------------------------------------|
0             Validation | 0.8659 (24, 34,  7, 18) | 0.914 | 5.620 | 85.03 | 69.37 |###########################################| ^ (2.6%, 3.88, 1.7, 2%) 
0 >> 14/20 <<   Training | 0.8623 (25, 31,  7, 19) | 1.004 | 0.780 | 85.12 | 69.74 |-----------------------------------------------|
0             Validation | 0.8653 (26, 31,  7, 19) | 1.003 | 1.199 | 84.99 | 69.23 |##########################################| ^ (2.6%, 3.80, 2.1, 0%) 
0 >> 15/20 <<   Training | 0.8625 (24, 33,  7, 19) | 0.931 | 6.116 | 85.13 | 69.99 |-------------------------------------------------|
0             Validation | 0.8653 (24, 32,  7, 19) | 0.930 | 3.688 | 85.02 | 69.50 |#############################################| ^ (2.5%, 3.82, 3.1, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.8610 (25, 32,  7, 19) | 1.010 | 1.609 | 85.24 | 70.15 |---------------------------------------------------|
0             Validation | 0.8640 (26, 32,  7, 19) | 1.008 | 1.295 | 85.12 | 69.63 |##############################################| ^ (2.6%, 3.89, 1.2, 21%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.8605 (26, 31,  7, 19) | 1.022 | 1.847 | 85.27 | 70.06 |--------------------------------------------------|
0             Validation | 0.8636 (26, 31,  7, 19) | 1.021 | 1.759 | 85.14 | 69.54 |#############################################| ^ (2.6%, 3.95, 1.4, 8%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.8605 (25, 32,  7, 19) | 1.002 | 1.290 | 85.26 | 70.08 |--------------------------------------------------|
0             Validation | 0.8636 (26, 32,  7, 19) | 1.001 | 1.888 | 85.14 | 69.56 |#############################################| ^ (2.6%, 3.97, 1.3, 14%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.8605 (25, 32,  7, 19) | 1.002 | 1.065 | 85.26 | 70.07 |--------------------------------------------------|
0             Validation | 0.8635 (26, 32,  7, 19) | 1.000 | 1.734 | 85.14 | 69.55 |#############################################| ^ (2.6%, 3.97, 1.4, 10%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.8605 (25, 32,  7, 19) | 1.001 | 1.123 | 85.26 | 70.08 |--------------------------------------------------|
0             Validation | 0.8635 (26, 32,  7, 19) | 1.000 | 1.632 | 85.14 | 69.55 |#############################################| ^ (2.6%, 3.97, 1.4, 9%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset0_epoch20_before_finetuning.pkl
Run Finetuning
0 >> 20/20 <<   Training | 0.8605 (25, 32,  7, 19) | 1.002 | 1.220 | 85.26 | 70.07 |--------------------------------------------------|
0             Validation | 0.8635 (26, 32,  7, 19) | 1.001 | 1.596 | 85.14 | 69.55 |#############################################| ^ (2.6%, 3.97, 1.3, 15%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
Fri Apr 15 14:15:48 2022
Using weight: mcPseudoTagWeight for classifier: FvT
Add true class labels to data
Add true class labels to ttbar MC
concatenate data and ttbar dataframes
add encoded target: d4=0, d3=1, t4=2, t3=3
2016 ---------------------------------------------
nd4 =   22557, wd4 =  22557.0, <w> = 1.000
nd3 = 1080290, wd3 =  59314.2, <w> = 0.055
nt4 =   24964, wt4 =   1078.9, <w> = 0.043
nt3 =  365641, wt3 =   9206.6, <w> = 0.025
2017 ---------------------------------------------
nd4 =   19573, wd4 =  19573.0, <w> = 1.000
nd3 =  484439, wd3 =  29448.0, <w> = 0.061
nt4 =   47278, wt4 =   1522.8, <w> = 0.032
nt3 =  546564, wt3 =   6362.3, <w> = 0.012
2018 ---------------------------------------------
nd4 =   26575, wd4 =  26575.0, <w> = 1.000
nd3 =  831699, wd3 =  53355.4, <w> = 0.064
nt4 =   56634, wt4 =   2092.5, <w> = 0.037
nt3 =  603708, wt3 =   9210.2, <w> = 0.015
All ----------------------------------------------
nd4 =   68705, wd4 =  68705.0, <w> = 1.000
nd3 = 2396428, wd3 = 142117.6, <w> = 0.059
nt4 =  128876, wt4 =   4694.2, <w> = 0.036
nt3 = 1515913, wt3 =  24779.1, <w> = 0.016
wtn =   -142.3
Normalization = wd4_SB/(wd3_SB-wt3_SB+wt4_SB)
              = 68705/(76704-10337+2321)
              = 1.000 +/- 0.004 (0.007 validation stat uncertainty, norm should converge to about this precision)
loaded die loss inside SR: 0.589803
loaded die loss: 0.828956
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset0.log
Setup training/validation tensors
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.8843 (26, 32,  7, 19) | 1.036 | 18.080 | 83.20 | 68.06 |------------------------------|
0             Validation | 0.8867 (26, 31,  7, 19) | 1.044 | 7.414 | 83.18 | 67.86 |############################| ^ (1.4%, 2.16, 2.6, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1338 batches)
0 >>  2/20 <<   Training | 0.8756 (25, 33,  7, 18) | 0.991 | 2.473 | 84.06 | 68.56 |-----------------------------------|
0             Validation | 0.8779 (25, 33,  7, 18) | 0.999 | 1.507 | 84.01 | 68.49 |##################################| ^ (0.7%, 2.63, 3.1, 0%) 
0 >>  3/20 <<   Training | 0.8860 (26, 27,  7, 23) | 1.113 | 5.216 | 84.25 | 67.14 |---------------------|
0             Validation | 0.8887 (26, 27,  7, 23) | 1.121 | 2.753 | 84.17 | 66.96 |###################| ^ (1.3%, 2.69, 3.2, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (669 batches)
0 >>  4/20 <<   Training | 0.8687 (26, 30,  7, 20) | 1.043 | 2.290 | 84.67 | 69.23 |------------------------------------------|
0             Validation | 0.8717 (26, 30,  7, 20) | 1.052 | 1.508 | 84.61 | 69.01 |########################################| ^ (1.3%, 2.88, 2.7, 0%) 
0 >>  5/20 <<   Training | 0.8686 (24, 33,  7, 19) | 0.927 | 1.417 | 84.75 | 69.83 |------------------------------------------------|
0             Validation | 0.8718 (24, 33,  7, 19) | 0.934 | 1.350 | 84.67 | 69.54 |#############################################| ^ (1.5%, 4.08, 3.6, 0%) 
0 >>  6/20 <<   Training | 0.8684 (25, 31,  7, 20) | 0.965 | 3.950 | 84.75 | 69.85 |------------------------------------------------|
0             Validation | 0.8717 (25, 31,  7, 20) | 0.973 | 2.656 | 84.66 | 69.59 |#############################################| ^ (1.4%, 3.54, 3.9, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (334 batches)
0 >>  7/20 <<   Training | 0.8671 (25, 34,  6, 17) | 0.981 | 2.342 | 84.91 | 69.95 |-------------------------------------------------|
0             Validation | 0.8707 (26, 33,  6, 17) | 0.990 | 1.858 | 84.80 | 69.65 |##############################################| ^ (1.5%, 3.17, 4.0, 0%) 
0 >>  8/20 <<   Training | 0.8761 (25, 28,  8, 22) | 1.012 | 1.277 | 84.91 | 69.03 |----------------------------------------|
0             Validation | 0.8797 (25, 28,  8, 22) | 1.019 | 1.209 | 84.81 | 68.70 |####################################| ^ (1.8%, 3.14, 3.1, 0%) 
0 >>  9/20 <<   Training | 0.8640 (25, 33,  7, 18) | 0.973 | 2.650 | 84.94 | 69.66 |----------------------------------------------|
0             Validation | 0.8680 (25, 32,  7, 18) | 0.982 | 2.301 | 84.82 | 69.30 |###########################################| ^ (1.8%, 3.95, 3.1, 0%) 
0 >> 10/20 <<   Training | 0.8641 (26, 33,  7, 18) | 1.000 | 1.156 | 85.03 | 69.88 |------------------------------------------------|
0             Validation | 0.8682 (26, 33,  7, 18) | 1.009 | 1.356 | 84.90 | 69.46 |############################################| ^ (2.1%, 4.19, 3.0, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (167 batches)
0 >> 11/20 <<   Training | 0.8626 (25, 31,  7, 20) | 0.976 | 1.819 | 85.04 | 69.75 |-----------------------------------------------|
0             Validation | 0.8667 (25, 31,  7, 20) | 0.984 | 1.520 | 84.91 | 69.36 |###########################################| ^ (2.0%, 3.99, 3.1, 0%) 
0 >> 12/20 <<   Training | 0.8633 (26, 31,  7, 19) | 1.018 | 4.983 | 85.15 | 69.46 |--------------------------------------------|
0             Validation | 0.8675 (26, 30,  7, 19) | 1.028 | 3.761 | 85.01 | 69.10 |#########################################| ^ (1.8%, 3.82, 2.9, 0%) 
0 >> 13/20 <<   Training | 0.8638 (26, 31,  6, 19) | 1.069 | 4.457 | 85.10 | 69.96 |-------------------------------------------------|
0             Validation | 0.8680 (27, 31,  7, 19) | 1.078 | 1.819 | 84.97 | 69.51 |#############################################| ^ (2.3%, 3.78, 3.2, 0%) 
0 >> 14/20 <<   Training | 0.8621 (26, 31,  7, 19) | 1.043 | 4.337 | 85.10 | 69.95 |-------------------------------------------------|
0             Validation | 0.8662 (26, 31,  7, 19) | 1.051 | 1.608 | 84.97 | 69.55 |#############################################| ^ (2.0%, 4.10, 3.1, 0%) 
0 >> 15/20 <<   Training | 0.8615 (25, 31,  7, 19) | 1.011 | 2.458 | 85.14 | 69.78 |-----------------------------------------------|
0             Validation | 0.8657 (26, 31,  7, 19) | 1.019 | 1.472 | 85.01 | 69.37 |###########################################| ^ (2.1%, 3.76, 3.2, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.8615 (26, 32,  7, 18) | 0.998 | 1.837 | 85.22 | 70.03 |--------------------------------------------------|
0             Validation | 0.8660 (26, 32,  7, 18) | 1.007 | 1.985 | 85.09 | 69.58 |#############################################| ^ (2.2%, 4.01, 3.4, 0%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.8605 (26, 32,  7, 19) | 1.009 | 1.564 | 85.21 | 69.90 |-------------------------------------------------|
0             Validation | 0.8651 (26, 31,  7, 19) | 1.018 | 1.326 | 85.07 | 69.46 |############################################| ^ (2.3%, 3.92, 3.3, 0%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.8605 (26, 32,  7, 19) | 1.008 | 1.193 | 85.21 | 69.92 |-------------------------------------------------|
0             Validation | 0.8651 (26, 32,  7, 19) | 1.016 | 1.233 | 85.07 | 69.47 |############################################| ^ (2.3%, 3.95, 3.3, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.8605 (25, 32,  7, 19) | 1.002 | 1.506 | 85.22 | 69.92 |-------------------------------------------------|
0             Validation | 0.8650 (26, 31,  7, 19) | 1.010 | 1.233 | 85.08 | 69.47 |############################################| ^ (2.3%, 3.95, 3.3, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.8605 (25, 32,  7, 19) | 1.002 | 1.467 | 85.22 | 69.92 |-------------------------------------------------|
0             Validation | 0.8650 (26, 31,  7, 19) | 1.010 | 1.177 | 85.08 | 69.47 |############################################| ^ (2.3%, 3.95, 3.3, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset0_epoch20_before_finetuning.pkl
Run Finetuning
0 >> 20/20 <<   Training | 0.8605 (25, 32,  7, 19) | 1.000 | 1.562 | 85.22 | 69.91 |-------------------------------------------------|
0             Validation | 0.8650 (26, 31,  7, 19) | 1.008 | 1.200 | 85.08 | 69.46 |############################################| ^ (2.3%, 3.96, 3.2, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

0 >> DONE <<
0 Minimum Loss = 0.860488
Mon Apr 25 22:28:26 2022
Using weight: mcPseudoTagWeight for classifier: FvT
Add true class labels to data
Add true class labels to ttbar MC
concatenate data and ttbar dataframes
add encoded target: d4=0, d3=1, t4=2, t3=3
2016 ---------------------------------------------
nd4 =   33303, wd4 =  33303.0, <w> = 1.000
nd3 = 1545810, wd3 =  75998.2, <w> = 0.049
nt4 =   34931, wt4 =   1488.9, <w> = 0.043
nt3 =  464056, wt3 =  10340.1, <w> = 0.022
2017 ---------------------------------------------
nd4 =   27207, wd4 =  27207.0, <w> = 1.000
nd3 =  642531, wd3 =  35456.4, <w> = 0.055
nt4 =   65736, wt4 =   2075.0, <w> = 0.032
nt3 =  693073, wt3 =   7105.7, <w> = 0.010
2018 ---------------------------------------------
nd4 =   36667, wd4 =  36667.0, <w> = 1.000
nd3 = 1090687, wd3 =  63725.1, <w> = 0.058
nt4 =   79211, wt4 =   2860.7, <w> = 0.036
nt3 =  760219, wt3 =  10223.0, <w> = 0.013
All ----------------------------------------------
nd4 =   97177, wd4 =  97177.0, <w> = 1.000
nd3 = 3279028, wd3 = 175179.7, <w> = 0.053
nt4 =  179878, wt4 =   6424.6, <w> = 0.036
nt3 = 1917348, wt3 =  27668.8, <w> = 0.014
wtn =   -155.6
Normalization = wd4_SB/(wd3_SB-wt3_SB+wt4_SB)
              = 97177/(106760-13238+3646)
              = 1.000 +/- 0.003 (0.006 validation stat uncertainty, norm should converge to about this precision)
loaded die loss inside SR: 0.590595
loaded die loss: 0.848897
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset0.log
Setup training/validation tensors
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.8850 (27, 33,  7, 17) | 0.915 | 22.455 | 84.07 | 68.02 |------------------------------|
0             Validation | 0.8872 (27, 33,  7, 17) | 0.926 | 12.121 | 84.04 | 67.92 |#############################| ^ (0.6%, 3.60, 2.3, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1781 batches)
0 >>  2/20 <<   Training | 0.8796 (26, 31,  8, 20) | 0.890 | 40.549 | 84.50 | 67.36 |-----------------------|
0             Validation | 0.8823 (27, 31,  8, 20) | 0.900 | 21.260 | 84.47 | 67.18 |#####################| ^ (1.1%, 2.58, 2.2, 0%) 
0 >>  3/20 <<   Training | 0.8756 (26, 32,  8, 19) | 0.917 | 6.796 | 84.69 | 67.61 |--------------------------|
0             Validation | 0.8787 (26, 32,  8, 19) | 0.927 | 4.561 | 84.65 | 67.33 |#######################| ^ (1.7%, 2.86, 2.5, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (890 batches)
0 >>  4/20 <<   Training | 0.8712 (27, 35,  7, 17) | 0.940 | 3.166 | 84.87 | 68.30 |---------------------------------|
0             Validation | 0.8742 (27, 34,  7, 17) | 0.951 | 2.815 | 84.82 | 68.02 |##############################| ^ (1.6%, 3.15, 1.8, 1%) 
0 >>  5/20 <<   Training | 0.8694 (28, 32,  7, 18) | 1.055 | 8.613 | 85.03 | 68.07 |------------------------------|
0             Validation | 0.8725 (28, 31,  7, 18) | 1.066 | 4.256 | 84.99 | 67.80 |###########################| ^ (1.7%, 3.00, 2.7, 0%) 
0 >>  6/20 <<   Training | 0.8694 (27, 32,  7, 19) | 0.972 | 3.968 | 85.09 | 68.33 |---------------------------------|
0             Validation | 0.8725 (27, 32,  7, 19) | 0.982 | 1.986 | 85.06 | 68.04 |##############################| ^ (1.6%, 3.43, 2.4, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (445 batches)
0 >>  7/20 <<   Training | 0.8699 (28, 34,  6, 17) | 0.972 | 15.432 | 84.98 | 68.50 |----------------------------------|
0             Validation | 0.8732 (28, 33,  7, 17) | 0.983 | 9.583 | 84.94 | 68.22 |################################| ^ (1.5%, 3.88, 2.0, 0%) 
0 >>  8/20 <<   Training | 0.8700 (28, 34,  7, 16) | 1.011 | 2.142 | 85.12 | 68.51 |-----------------------------------|
0             Validation | 0.8734 (28, 34,  7, 16) | 1.022 | 1.141 | 85.08 | 68.13 |###############################| ^ (2.1%, 3.28, 2.4, 0%) 
0 >>  9/20 <<   Training | 0.8676 (27, 34,  7, 17) | 0.979 | 2.209 | 85.18 | 68.70 |------------------------------------|
0             Validation | 0.8708 (28, 34,  7, 17) | 0.990 | 1.289 | 85.13 | 68.38 |#################################| ^ (1.7%, 3.36, 2.3, 0%) 
0 >> 10/20 <<   Training | 0.8747 (27, 30,  8, 20) | 0.980 | 16.564 | 85.21 | 67.92 |-----------------------------|
0             Validation | 0.8785 (28, 29,  8, 20) | 0.991 | 10.219 | 85.20 | 67.56 |#########################| ^ (2.1%, 3.29, 3.0, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (222 batches)
0 >> 11/20 <<   Training | 0.8664 (28, 33,  7, 17) | 1.042 | 5.469 | 85.25 | 68.60 |-----------------------------------|
0             Validation | 0.8698 (28, 33,  7, 17) | 1.053 | 2.647 | 85.21 | 68.23 |################################| ^ (2.0%, 3.30, 2.4, 0%) 
0 >> 12/20 <<   Training | 0.8673 (27, 34,  7, 17) | 0.939 | 1.785 | 85.19 | 68.73 |-------------------------------------|
0             Validation | 0.8706 (27, 34,  7, 17) | 0.949 | 1.752 | 85.15 | 68.37 |#################################| ^ (2.0%, 3.89, 2.2, 0%) 
0 >> 13/20 <<   Training | 0.8678 (25, 34,  7, 19) | 0.875 | 1.285 | 85.25 | 68.68 |------------------------------------|
0             Validation | 0.8712 (25, 34,  7, 19) | 0.885 | 1.241 | 85.20 | 68.31 |#################################| ^ (2.0%, 3.30, 2.4, 0%) 
0 >> 14/20 <<   Training | 0.8657 (28, 32,  7, 17) | 1.031 | 2.523 | 85.31 | 68.75 |-------------------------------------|
0             Validation | 0.8693 (29, 32,  7, 17) | 1.043 | 1.297 | 85.27 | 68.39 |#################################| ^ (1.9%, 3.52, 1.9, 1%) 
0 >> 15/20 <<   Training | 0.8664 (29, 32,  7, 17) | 1.084 | 4.082 | 85.35 | 68.63 |------------------------------------|
0             Validation | 0.8701 (29, 31,  7, 17) | 1.096 | 2.179 | 85.31 | 68.24 |################################| ^ (2.1%, 3.26, 2.5, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.8648 (29, 32,  7, 17) | 1.047 | 2.046 | 85.36 | 68.74 |-------------------------------------|
0             Validation | 0.8684 (29, 32,  7, 17) | 1.059 | 1.423 | 85.33 | 68.35 |#################################| ^ (2.1%, 3.44, 2.2, 0%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.8643 (28, 32,  7, 17) | 1.013 | 1.551 | 85.37 | 68.73 |-------------------------------------|
0             Validation | 0.8681 (28, 32,  7, 17) | 1.024 | 1.157 | 85.33 | 68.33 |#################################| ^ (2.2%, 3.62, 2.9, 0%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.8642 (28, 33,  7, 18) | 1.002 | 1.728 | 85.36 | 68.74 |-------------------------------------|
0             Validation | 0.8679 (28, 32,  7, 18) | 1.013 | 1.519 | 85.33 | 68.33 |#################################| ^ (2.2%, 3.61, 3.2, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.8642 (28, 33,  7, 18) | 0.999 | 1.723 | 85.36 | 68.74 |-------------------------------------|
0             Validation | 0.8679 (28, 32,  7, 17) | 1.010 | 1.648 | 85.33 | 68.33 |#################################| ^ (2.2%, 3.62, 3.1, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.8642 (28, 33,  7, 18) | 0.998 | 1.837 | 85.36 | 68.74 |-------------------------------------|
0             Validation | 0.8679 (28, 32,  7, 18) | 1.009 | 1.628 | 85.33 | 68.33 |#################################| ^ (2.2%, 3.62, 3.0, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset0_epoch20_before_finetuning.pkl
Run Finetuning
0 >> 20/20 <<   Training | 0.8642 (28, 33,  7, 18) | 0.996 | 1.442 | 85.36 | 68.74 |-------------------------------------|
0             Validation | 0.8679 (28, 32,  7, 18) | 1.007 | 1.405 | 85.32 | 68.33 |#################################| ^ (2.2%, 3.64, 3.1, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

0 >> DONE <<
0 Minimum Loss = 0.864223
