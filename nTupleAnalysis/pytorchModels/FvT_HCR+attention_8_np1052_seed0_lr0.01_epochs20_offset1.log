1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >>  1/20 <<   Training | 0.8848 (26, 29,  7, 21) | 1.072 | 5.103 | 83.51 | 68.97 |---------------------------------------|
1             Validation | 0.8846 (26, 29,  7, 21) | 1.058 | 3.698 | 83.55 | 68.95 |#######################################| ^ (1.1%, 2.93, 1.5, 5%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1337 batches)
1 >>  2/20 <<   Training | 0.8754 (25, 32,  7, 19) | 0.956 | 1.803 | 84.04 | 68.96 |---------------------------------------|
1             Validation | 0.8756 (24, 32,  7, 19) | 0.943 | 1.091 | 84.10 | 68.83 |######################################| ^ (1.0%, 3.84, 2.2, 0%) 
1 >>  3/20 <<   Training | 0.8720 (26, 30,  7, 20) | 1.052 | 1.485 | 84.44 | 68.80 |-------------------------------------|
1             Validation | 0.8725 (26, 30,  7, 20) | 1.038 | 1.276 | 84.48 | 68.65 |####################################| ^ (0.9%, 4.55, 2.0, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (668 batches)
1 >>  4/20 <<   Training | 0.8704 (26, 30,  7, 20) | 1.015 | 3.722 | 84.58 | 68.41 |----------------------------------|
1             Validation | 0.8712 (25, 30,  7, 20) | 1.002 | 3.070 | 84.64 | 68.20 |###############################| ^ (1.2%, 3.90, 1.7, 2%) 
1 >>  5/20 <<   Training | 0.8676 (25, 31,  7, 20) | 0.998 | 2.033 | 84.91 | 69.08 |----------------------------------------|
1             Validation | 0.8679 (25, 31,  7, 20) | 0.985 | 2.547 | 84.99 | 68.93 |#######################################| ^ (1.2%, 3.39, 1.7, 1%) 
1 >>  6/20 <<   Training | 0.8661 (25, 32,  7, 19) | 1.001 | 2.625 | 84.80 | 69.01 |----------------------------------------|
1             Validation | 0.8666 (25, 32,  7, 19) | 0.988 | 1.591 | 84.86 | 68.81 |######################################| ^ (1.2%, 4.19, 1.9, 1%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (334 batches)
1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >>  1/20 <<   Training | 0.8848 (26, 29,  7, 21) | 1.072 | 5.103 | 83.51 | 68.97 |---------------------------------------|
1             Validation | 0.8846 (26, 29,  7, 21) | 1.058 | 3.698 | 83.55 | 68.95 |#######################################| ^ (1.1%, 2.93, 1.5, 5%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1337 batches)
1 >>  2/20 <<   Training | 0.8754 (25, 32,  7, 19) | 0.956 | 1.803 | 84.04 | 68.96 |---------------------------------------|
1             Validation | 0.8756 (24, 32,  7, 19) | 0.943 | 1.091 | 84.10 | 68.83 |######################################| ^ (1.0%, 3.84, 2.2, 0%) 
1 >>  3/20 <<   Training | 0.8720 (26, 30,  7, 20) | 1.052 | 1.485 | 84.44 | 68.80 |-------------------------------------|
1             Validation | 0.8725 (26, 30,  7, 20) | 1.038 | 1.276 | 84.48 | 68.65 |####################################| ^ (0.9%, 4.55, 2.0, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (668 batches)
1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >>  1/20 <<   Training | 0.8848 (26, 29,  7, 21) | 1.072 | 5.103 | 83.51 | 68.97 |---------------------------------------|
1             Validation | 0.8846 (26, 29,  7, 21) | 1.058 | 3.698 | 83.55 | 68.95 |#######################################| ^ (1.1%, 2.93, 1.5, 5%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1337 batches)
1 >>  2/20 <<   Training | 0.8754 (25, 32,  7, 19) | 0.956 | 1.803 | 84.04 | 68.96 |---------------------------------------|
1             Validation | 0.8756 (24, 32,  7, 19) | 0.943 | 1.091 | 84.10 | 68.83 |######################################| ^ (1.0%, 3.84, 2.2, 0%) 
1 >>  3/20 <<   Training | 0.8720 (26, 30,  7, 20) | 1.052 | 1.485 | 84.44 | 68.80 |-------------------------------------|
1             Validation | 0.8725 (26, 30,  7, 20) | 1.038 | 1.276 | 84.48 | 68.65 |####################################| ^ (0.9%, 4.55, 2.0, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (668 batches)
1 >>  4/20 <<   Training | 0.8704 (26, 30,  7, 20) | 1.015 | 3.722 | 84.58 | 68.41 |----------------------------------|
1             Validation | 0.8712 (25, 30,  7, 20) | 1.002 | 3.070 | 84.64 | 68.20 |###############################| ^ (1.2%, 3.90, 1.7, 2%) 
1 >>  5/20 <<   Training | 0.8676 (25, 31,  7, 20) | 0.998 | 2.033 | 84.91 | 69.08 |----------------------------------------|
1             Validation | 0.8679 (25, 31,  7, 20) | 0.985 | 2.547 | 84.99 | 68.93 |#######################################| ^ (1.2%, 3.39, 1.7, 1%) 
1 >>  6/20 <<   Training | 0.8661 (25, 32,  7, 19) | 1.001 | 2.625 | 84.80 | 69.01 |----------------------------------------|
1             Validation | 0.8666 (25, 32,  7, 19) | 0.988 | 1.591 | 84.86 | 68.81 |######################################| ^ (1.2%, 4.19, 1.9, 1%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (334 batches)
1 >>  7/20 <<   Training | 0.8662 (26, 32,  7, 18) | 1.011 | 8.486 | 84.90 | 69.46 |--------------------------------------------|
1             Validation | 0.8669 (26, 32,  7, 18) | 0.998 | 5.283 | 84.96 | 69.21 |##########################################| ^ (1.4%, 4.35, 2.1, 0%) 
1 >>  8/20 <<   Training | 0.8646 (26, 31,  7, 19) | 1.017 | 1.104 | 84.96 | 69.55 |---------------------------------------------|
1             Validation | 0.8654 (25, 31,  7, 20) | 1.005 | 1.168 | 85.01 | 69.35 |###########################################| ^ (1.1%, 3.78, 2.4, 0%) 
1 >>  9/20 <<   Training | 0.8646 (25, 32,  7, 19) | 0.984 | 1.225 | 85.01 | 69.71 |-----------------------------------------------|
1             Validation | 0.8655 (25, 32,  7, 19) | 0.971 | 1.775 | 85.06 | 69.47 |############################################| ^ (1.5%, 4.06, 2.1, 0%) 
1 >> 10/20 <<   Training | 0.8642 (25, 32,  7, 19) | 0.977 | 1.081 | 84.90 | 69.34 |-------------------------------------------|
1             Validation | 0.8648 (25, 32,  7, 19) | 0.965 | 1.209 | 84.96 | 69.18 |#########################################| ^ (1.2%, 4.24, 1.7, 2%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (167 batches)
1 >> 11/20 <<   Training | 0.8635 (25, 33,  7, 18) | 0.984 | 2.846 | 85.05 | 69.59 |---------------------------------------------|
1             Validation | 0.8645 (25, 33,  7, 18) | 0.971 | 2.522 | 85.09 | 69.33 |###########################################| ^ (1.4%, 4.55, 1.9, 1%) 
1 >> 12/20 <<   Training | 0.8646 (27, 32,  7, 18) | 1.043 | 3.506 | 85.09 | 69.61 |----------------------------------------------|
1             Validation | 0.8655 (26, 32,  7, 18) | 1.030 | 2.445 | 85.14 | 69.36 |###########################################| ^ (1.3%, 4.52, 2.4, 0%) 
1 >> 13/20 <<   Training | 0.8630 (26, 31,  7, 19) | 1.057 | 3.284 | 85.06 | 69.59 |---------------------------------------------|
1             Validation | 0.8639 (26, 31,  7, 19) | 1.043 | 1.803 | 85.12 | 69.31 |###########################################| ^ (1.5%, 4.29, 2.0, 0%) 
1 >> 14/20 <<   Training | 0.8628 (26, 31,  7, 19) | 1.033 | 1.277 | 85.10 | 69.78 |-----------------------------------------------|
1             Validation | 0.8639 (26, 31,  7, 19) | 1.019 | 1.679 | 85.16 | 69.48 |############################################| ^ (1.6%, 4.22, 2.0, 0%) 
1 >> 15/20 <<   Training | 0.8632 (25, 33,  7, 18) | 0.964 | 3.746 | 85.05 | 69.77 |-----------------------------------------------|
1             Validation | 0.8643 (25, 33,  7, 18) | 0.951 | 2.694 | 85.09 | 69.52 |#############################################| ^ (1.4%, 4.51, 1.7, 2%) 
Decay learning rate: 0.010000 -> 0.002500
1 >> 16/20 <<   Training | 0.8619 (25, 32,  7, 19) | 0.973 | 1.815 | 85.13 | 70.08 |--------------------------------------------------|
1             Validation | 0.8632 (25, 32,  7, 19) | 0.960 | 1.813 | 85.18 | 69.79 |###############################################| ^ (1.5%, 4.71, 2.4, 0%) 
Decay learning rate: 0.002500 -> 0.000625
1 >> 17/20 <<   Training | 0.8615 (26, 31,  7, 19) | 1.023 | 1.008 | 85.17 | 69.89 |------------------------------------------------|
1             Validation | 0.8626 (26, 32,  7, 19) | 1.010 | 1.032 | 85.22 | 69.59 |#############################################| ^ (1.6%, 4.71, 2.0, 0%) 
Decay learning rate: 0.000625 -> 0.000156
1 >> 18/20 <<   Training | 0.8614 (25, 32,  7, 19) | 1.001 | 1.045 | 85.17 | 69.93 |-------------------------------------------------|
1             Validation | 0.8626 (25, 32,  7, 19) | 0.988 | 1.080 | 85.22 | 69.63 |##############################################| ^ (1.6%, 4.70, 2.0, 0%) 
Decay learning rate: 0.000156 -> 0.000039
1 >> 19/20 <<   Training | 0.8614 (25, 32,  7, 19) | 1.002 | 1.226 | 85.17 | 69.92 |-------------------------------------------------|
1             Validation | 0.8626 (25, 32,  7, 19) | 0.989 | 1.195 | 85.22 | 69.62 |##############################################| ^ (1.6%, 4.70, 2.0, 0%) 
Decay learning rate: 0.000039 -> 0.000010
1 >> 20/20 <<   Training | 0.8614 (25, 32,  7, 19) | 1.002 | 1.085 | 85.17 | 69.92 |-------------------------------------------------|
1             Validation | 0.8626 (25, 32,  7, 19) | 0.989 | 1.266 | 85.22 | 69.62 |##############################################| ^ (1.6%, 4.69, 2.0, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset1_epoch20_before_finetuning.pkl
Run Finetuning
1 >> 20/20 <<   Training | 0.8614 (25, 32,  7, 19) | 1.003 | 1.216 | 85.17 | 69.91 |-------------------------------------------------|
1             Validation | 0.8626 (25, 32,  7, 19) | 0.990 | 1.174 | 85.22 | 69.61 |##############################################| ^ (1.6%, 4.69, 1.9, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset1_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
Fri Apr 15 14:23:08 2022
Using weight: mcPseudoTagWeight for classifier: FvT
Add true class labels to data
Add true class labels to ttbar MC
concatenate data and ttbar dataframes
add encoded target: d4=0, d3=1, t4=2, t3=3
2016 ---------------------------------------------
nd4 =   22557, wd4 =  22557.0, <w> = 1.000
nd3 = 1080290, wd3 =  59314.2, <w> = 0.055
nt4 =   24964, wt4 =   1078.9, <w> = 0.043
nt3 =  365641, wt3 =   9206.6, <w> = 0.025
2017 ---------------------------------------------
nd4 =   19573, wd4 =  19573.0, <w> = 1.000
nd3 =  484439, wd3 =  29448.0, <w> = 0.061
nt4 =   47278, wt4 =   1522.8, <w> = 0.032
nt3 =  546564, wt3 =   6362.3, <w> = 0.012
2018 ---------------------------------------------
nd4 =   26575, wd4 =  26575.0, <w> = 1.000
nd3 =  831699, wd3 =  53355.4, <w> = 0.064
nt4 =   56634, wt4 =   2092.5, <w> = 0.037
nt3 =  603708, wt3 =   9210.2, <w> = 0.015
All ----------------------------------------------
nd4 =   68705, wd4 =  68705.0, <w> = 1.000
nd3 = 2396428, wd3 = 142117.6, <w> = 0.059
nt4 =  128876, wt4 =   4694.2, <w> = 0.036
nt3 = 1515913, wt3 =  24779.1, <w> = 0.016
wtn =   -142.3
Normalization = wd4_SB/(wd3_SB-wt3_SB+wt4_SB)
              = 68705/(76704-10337+2321)
              = 1.000 +/- 0.004 (0.007 validation stat uncertainty, norm should converge to about this precision)
loaded die loss inside SR: 0.589803
loaded die loss: 0.828956
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset1.log
Setup training/validation tensors
1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >>  1/20 <<   Training | 0.8885 (28, 32,  7, 17) | 1.089 | 6.292 | 83.25 | 67.55 |-------------------------|
1             Validation | 0.8883 (27, 32,  7, 17) | 1.086 | 4.788 | 83.30 | 67.57 |#########################| ^ (0.8%, 2.28, 1.9, 1%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1337 batches)
1 >>  2/20 <<   Training | 0.8752 (26, 32,  7, 18) | 1.002 | 2.506 | 84.03 | 68.05 |------------------------------|
1             Validation | 0.8755 (26, 32,  7, 18) | 1.001 | 1.780 | 84.06 | 67.95 |#############################| ^ (0.8%, 3.64, 1.7, 2%) 
1 >>  3/20 <<   Training | 0.8725 (23, 33,  7, 19) | 0.850 | 8.452 | 84.20 | 68.90 |--------------------------------------|
1             Validation | 0.8725 (23, 33,  7, 19) | 0.847 | 5.316 | 84.26 | 68.94 |#######################################| ^ (0.7%, 2.96, 2.4, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (668 batches)
1 >>  4/20 <<   Training | 0.8712 (25, 30,  7, 21) | 1.008 | 5.278 | 84.56 | 69.35 |-------------------------------------------|
1             Validation | 0.8716 (25, 30,  7, 21) | 1.007 | 2.568 | 84.61 | 69.25 |##########################################| ^ (0.8%, 2.90, 1.9, 1%) 
1 >>  5/20 <<   Training | 0.8690 (25, 32,  7, 19) | 0.981 | 1.798 | 84.49 | 69.01 |----------------------------------------|
1             Validation | 0.8694 (25, 32,  7, 19) | 0.979 | 1.962 | 84.54 | 68.93 |#######################################| ^ (0.8%, 2.98, 1.9, 0%) 
1 >>  6/20 <<   Training | 0.8684 (25, 33,  7, 18) | 0.971 | 4.560 | 84.63 | 69.52 |---------------------------------------------|
1             Validation | 0.8689 (25, 33,  7, 18) | 0.969 | 3.474 | 84.66 | 69.47 |############################################| ^ (0.8%, 3.10, 1.4, 7%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (334 batches)
1 >>  7/20 <<   Training | 0.8665 (25, 30,  7, 20) | 1.017 | 1.043 | 84.75 | 69.44 |--------------------------------------------|
1             Validation | 0.8671 (25, 30,  7, 20) | 1.015 | 1.484 | 84.79 | 69.30 |###########################################| ^ (0.8%, 3.28, 1.8, 1%) 
1 >>  8/20 <<   Training | 0.8656 (25, 32,  7, 19) | 0.979 | 2.906 | 84.75 | 69.60 |---------------------------------------------|
1             Validation | 0.8663 (25, 32,  7, 19) | 0.977 | 2.546 | 84.79 | 69.45 |############################################| ^ (0.8%, 3.48, 1.8, 1%) 
1 >>  9/20 <<   Training | 0.8668 (24, 34,  7, 18) | 0.934 | 2.836 | 84.69 | 69.76 |-----------------------------------------------|
1             Validation | 0.8673 (24, 34,  7, 18) | 0.933 | 1.218 | 84.73 | 69.66 |##############################################| ^ (0.7%, 3.30, 1.8, 1%) 
1 >> 10/20 <<   Training | 0.8671 (25, 30,  7, 20) | 1.020 | 2.541 | 84.82 | 69.22 |------------------------------------------|
1             Validation | 0.8680 (25, 30,  7, 20) | 1.018 | 1.450 | 84.86 | 69.00 |########################################| ^ (1.2%, 3.18, 2.2, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (167 batches)
1 >> 11/20 <<   Training | 0.8656 (24, 33,  7, 18) | 0.933 | 2.835 | 84.76 | 69.63 |----------------------------------------------|
1             Validation | 0.8663 (25, 33,  7, 18) | 0.931 | 2.045 | 84.81 | 69.49 |############################################| ^ (0.9%, 3.38, 1.8, 1%) 
1 >> 12/20 <<   Training | 0.8670 (27, 32,  7, 18) | 1.059 | 1.028 | 84.83 | 69.40 |-------------------------------------------|
1             Validation | 0.8678 (27, 32,  7, 18) | 1.057 | 0.969 | 84.89 | 69.23 |##########################################| ^ (0.9%, 3.36, 1.9, 1%) 
1 >> 13/20 <<   Training | 0.8644 (25, 32,  7, 19) | 0.946 | 2.821 | 84.74 | 69.45 |--------------------------------------------|
1             Validation | 0.8653 (25, 32,  7, 19) | 0.944 | 2.696 | 84.78 | 69.26 |##########################################| ^ (1.0%, 3.42, 1.5, 6%) 
1 >> 14/20 <<   Training | 0.8643 (25, 32,  7, 19) | 0.961 | 2.056 | 84.89 | 69.91 |-------------------------------------------------|
1             Validation | 0.8655 (25, 32,  7, 19) | 0.959 | 1.487 | 84.93 | 69.66 |##############################################| ^ (1.2%, 3.42, 2.2, 0%) 
1 >> 15/20 <<   Training | 0.8640 (26, 31,  7, 19) | 1.023 | 1.132 | 84.92 | 69.52 |---------------------------------------------|
1             Validation | 0.8650 (26, 31,  7, 19) | 1.022 | 1.205 | 84.95 | 69.32 |###########################################| ^ (1.0%, 3.25, 1.5, 5%) 
Decay learning rate: 0.010000 -> 0.002500
1 >> 16/20 <<   Training | 0.8632 (26, 32,  7, 19) | 1.012 | 1.126 | 84.94 | 69.74 |-----------------------------------------------|
1             Validation | 0.8643 (26, 32,  7, 19) | 1.010 | 0.747 | 84.97 | 69.54 |#############################################| ^ (1.1%, 3.63, 1.5, 6%) 
Decay learning rate: 0.002500 -> 0.000625
1 >> 17/20 <<   Training | 0.8628 (26, 31,  7, 19) | 1.010 | 0.922 | 84.94 | 69.70 |-----------------------------------------------|
1             Validation | 0.8639 (26, 31,  7, 19) | 1.008 | 0.947 | 84.98 | 69.47 |############################################| ^ (1.2%, 3.59, 2.4, 0%) 
Decay learning rate: 0.000625 -> 0.000156
1 >> 18/20 <<   Training | 0.8627 (25, 32,  7, 19) | 1.005 | 0.887 | 84.95 | 69.71 |-----------------------------------------------|
1             Validation | 0.8638 (26, 32,  7, 19) | 1.003 | 0.771 | 84.98 | 69.48 |############################################| ^ (1.2%, 3.59, 2.1, 0%) 
Decay learning rate: 0.000156 -> 0.000039
1 >> 19/20 <<   Training | 0.8627 (25, 32,  7, 19) | 0.996 | 0.965 | 84.95 | 69.72 |-----------------------------------------------|
1             Validation | 0.8638 (25, 32,  7, 19) | 0.994 | 0.925 | 84.98 | 69.49 |############################################| ^ (1.2%, 3.60, 2.0, 0%) 
Decay learning rate: 0.000039 -> 0.000010
1 >> 20/20 <<   Training | 0.8627 (25, 32,  7, 19) | 0.998 | 0.802 | 84.95 | 69.72 |-----------------------------------------------|
1             Validation | 0.8638 (25, 32,  7, 19) | 0.995 | 1.020 | 84.98 | 69.48 |############################################| ^ (1.2%, 3.60, 2.0, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset1_epoch20_before_finetuning.pkl
Run Finetuning
1 >> 20/20 <<   Training | 0.8627 (25, 32,  7, 19) | 0.995 | 0.874 | 84.95 | 69.72 |-----------------------------------------------|
1             Validation | 0.8638 (25, 32,  7, 19) | 0.993 | 0.835 | 84.98 | 69.49 |############################################| ^ (1.2%, 3.61, 2.0, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset1_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

1 >> DONE <<
1 Minimum Loss = 0.862720
Tue Apr 26 01:43:14 2022
Using weight: mcPseudoTagWeight for classifier: FvT
Add true class labels to data
Add true class labels to ttbar MC
concatenate data and ttbar dataframes
add encoded target: d4=0, d3=1, t4=2, t3=3
2016 ---------------------------------------------
nd4 =   33303, wd4 =  33303.0, <w> = 1.000
nd3 = 1545810, wd3 =  75998.2, <w> = 0.049
nt4 =   34931, wt4 =   1488.9, <w> = 0.043
nt3 =  464056, wt3 =  10340.1, <w> = 0.022
2017 ---------------------------------------------
nd4 =   27207, wd4 =  27207.0, <w> = 1.000
nd3 =  642531, wd3 =  35456.4, <w> = 0.055
nt4 =   65736, wt4 =   2075.0, <w> = 0.032
nt3 =  693073, wt3 =   7105.7, <w> = 0.010
2018 ---------------------------------------------
nd4 =   36667, wd4 =  36667.0, <w> = 1.000
nd3 = 1090687, wd3 =  63725.1, <w> = 0.058
nt4 =   79211, wt4 =   2860.7, <w> = 0.036
nt3 =  760219, wt3 =  10223.0, <w> = 0.013
All ----------------------------------------------
nd4 =   97177, wd4 =  97177.0, <w> = 1.000
nd3 = 3279028, wd3 = 175179.7, <w> = 0.053
nt4 =  179878, wt4 =   6424.6, <w> = 0.036
nt3 = 1917348, wt3 =  27668.8, <w> = 0.014
wtn =   -155.6
Normalization = wd4_SB/(wd3_SB-wt3_SB+wt4_SB)
              = 97177/(106760-13238+3646)
              = 1.000 +/- 0.003 (0.006 validation stat uncertainty, norm should converge to about this precision)
loaded die loss inside SR: 0.590595
loaded die loss: 0.848897
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset1.log
Setup training/validation tensors
1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >>  1/20 <<   Training | 0.8882 (30, 30,  7, 18) | 1.191 | 25.111 | 83.88 | 66.23 |------------|
1             Validation | 0.8877 (30, 30,  7, 18) | 1.187 | 14.237 | 83.84 | 66.45 |##############| ^ (1.5%, 2.45, 1.6, 3%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1782 batches)
1 >>  2/20 <<   Training | 0.8786 (29, 30,  7, 19) | 1.113 | 1.247 | 84.24 | 67.07 |--------------------|
1             Validation | 0.8788 (29, 30,  7, 19) | 1.109 | 1.069 | 84.19 | 67.13 |#####################| ^ (0.7%, 2.82, 0.9, 47%) 
1 >>  3/20 <<   Training | 0.8741 (27, 32,  7, 18) | 0.984 | 6.928 | 84.62 | 67.72 |---------------------------|
1             Validation | 0.8751 (27, 32,  7, 18) | 0.981 | 4.055 | 84.56 | 67.65 |##########################| ^ (0.7%, 5.03, 1.1, 30%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (891 batches)
1 >>  4/20 <<   Training | 0.8701 (27, 34,  7, 18) | 0.952 | 3.907 | 84.98 | 68.22 |--------------------------------|
1             Validation | 0.8705 (27, 34,  7, 18) | 0.949 | 2.595 | 84.95 | 68.25 |################################| ^ (0.7%, 3.44, 1.5, 5%) 
1 >>  5/20 <<   Training | 0.8701 (28, 32,  7, 18) | 1.040 | 8.576 | 84.96 | 68.10 |-------------------------------|
1             Validation | 0.8707 (28, 32,  7, 18) | 1.037 | 4.054 | 84.91 | 68.09 |##############################| ^ (0.9%, 3.30, 1.1, 29%) 
1 >>  6/20 <<   Training | 0.8716 (27, 34,  7, 17) | 0.990 | 5.769 | 84.95 | 68.09 |------------------------------|
1             Validation | 0.8721 (27, 34,  7, 17) | 0.987 | 3.070 | 84.92 | 68.12 |###############################| ^ (0.8%, 3.60, 1.3, 11%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (445 batches)
1 >>  7/20 <<   Training | 0.8688 (27, 31,  8, 19) | 0.983 | 1.461 | 85.19 | 68.37 |---------------------------------|
1             Validation | 0.8694 (27, 31,  8, 19) | 0.978 | 1.126 | 85.15 | 68.35 |#################################| ^ (0.8%, 3.99, 1.1, 27%) 
1 >>  8/20 <<   Training | 0.8682 (28, 32,  7, 18) | 1.037 | 6.915 | 85.18 | 67.93 |-----------------------------|
1             Validation | 0.8686 (28, 32,  7, 18) | 1.034 | 3.643 | 85.14 | 67.96 |#############################| ^ (0.8%, 4.48, 0.9, 51%) 
1 >>  9/20 <<   Training | 0.8676 (27, 32,  7, 18) | 0.994 | 1.286 | 85.17 | 68.40 |---------------------------------|
1             Validation | 0.8682 (27, 32,  7, 18) | 0.990 | 1.184 | 85.13 | 68.41 |##################################| ^ (0.8%, 4.77, 0.8, 65%) 
1 >> 10/20 <<   Training | 0.8680 (28, 33,  7, 17) | 0.978 | 1.937 | 85.20 | 68.16 |-------------------------------|
1             Validation | 0.8687 (28, 33,  7, 17) | 0.974 | 1.852 | 85.17 | 68.16 |###############################| ^ (1.0%, 4.36, 2.2, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (222 batches)
1 >> 11/20 <<   Training | 0.8687 (26, 32,  8, 19) | 0.944 | 1.301 | 85.28 | 68.08 |------------------------------|
1             Validation | 0.8694 (26, 32,  8, 19) | 0.940 | 1.446 | 85.24 | 68.02 |##############################| ^ (0.6%, 4.57, 1.0, 35%) 
1 >> 12/20 <<   Training | 0.8681 (25, 35,  7, 18) | 0.875 | 1.493 | 85.29 | 68.29 |--------------------------------|
1             Validation | 0.8687 (25, 35,  7, 18) | 0.871 | 1.598 | 85.26 | 68.29 |################################| ^ (0.6%, 4.73, 0.9, 49%) 
1 >> 13/20 <<   Training | 0.8663 (27, 32,  7, 18) | 0.998 | 1.839 | 85.33 | 68.39 |---------------------------------|
1             Validation | 0.8672 (27, 32,  7, 18) | 0.994 | 1.236 | 85.29 | 68.32 |#################################| ^ (0.6%, 5.16, 1.3, 14%) 
1 >> 14/20 <<   Training | 0.8667 (27, 32,  7, 19) | 0.976 | 1.352 | 85.36 | 68.22 |--------------------------------|
1             Validation | 0.8676 (27, 32,  7, 19) | 0.972 | 1.432 | 85.32 | 68.16 |###############################| ^ (0.7%, 4.79, 0.8, 66%) 
1 >> 15/20 <<   Training | 0.8662 (28, 33,  7, 18) | 1.005 | 0.995 | 85.28 | 68.26 |--------------------------------|
1             Validation | 0.8669 (28, 33,  7, 18) | 1.000 | 1.099 | 85.24 | 68.23 |################################| ^ (0.7%, 3.96, 1.5, 5%) 
Decay learning rate: 0.010000 -> 0.002500
1 >> 16/20 <<   Training | 0.8652 (28, 32,  7, 18) | 1.045 | 2.231 | 85.39 | 68.35 |---------------------------------|
1             Validation | 0.8661 (28, 32,  7, 18) | 1.041 | 1.146 | 85.35 | 68.30 |#################################| ^ (0.9%, 4.79, 0.9, 58%) 
Decay learning rate: 0.002500 -> 0.000625
1 >> 17/20 <<   Training | 0.8648 (28, 33,  7, 17) | 1.001 | 0.991 | 85.39 | 68.43 |----------------------------------|
1             Validation | 0.8657 (28, 33,  7, 17) | 0.997 | 1.064 | 85.34 | 68.38 |#################################| ^ (0.8%, 4.99, 0.9, 52%) 
Decay learning rate: 0.000625 -> 0.000156
1 >> 18/20 <<   Training | 0.8648 (28, 32,  7, 17) | 1.011 | 0.844 | 85.40 | 68.42 |----------------------------------|
1             Validation | 0.8656 (28, 33,  7, 17) | 1.006 | 0.920 | 85.36 | 68.38 |#################################| ^ (0.7%, 5.01, 0.9, 53%) 
Decay learning rate: 0.000156 -> 0.000039
1 >> 19/20 <<   Training | 0.8648 (28, 33,  7, 17) | 1.005 | 0.825 | 85.40 | 68.43 |----------------------------------|
1             Validation | 0.8656 (28, 33,  7, 17) | 1.001 | 0.947 | 85.36 | 68.38 |#################################| ^ (0.7%, 5.02, 1.0, 34%) 
Decay learning rate: 0.000039 -> 0.000010
1 >> 20/20 <<   Training | 0.8648 (28, 33,  7, 18) | 1.003 | 0.753 | 85.40 | 68.43 |----------------------------------|
1             Validation | 0.8656 (28, 33,  7, 18) | 0.999 | 0.886 | 85.36 | 68.38 |#################################| ^ (0.7%, 5.02, 1.0, 39%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset1_epoch20_before_finetuning.pkl
Run Finetuning
1 >> 20/20 <<   Training | 0.8648 (28, 33,  7, 18) | 0.998 | 1.050 | 85.40 | 68.43 |----------------------------------|
1             Validation | 0.8656 (28, 33,  7, 18) | 0.993 | 0.930 | 85.36 | 68.39 |#################################| ^ (0.7%, 5.03, 1.0, 40%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset1_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

1 >> DONE <<
1 Minimum Loss = 0.864786
