1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >>  1/20 <<   Training | 0.9547 (22, 29,  7, 18) | 1.061 | 5.896 | 82.62 | 68.32 |---------------------------------|
1             Validation | 0.9575 (23, 29,  7, 18) | 1.070 | 3.358 | 82.54 | 68.02 |##############################| ^ (1.8%, 3.52, 1.8, 1%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1269 batches)
1 >>  2/20 <<   Training | 0.9406 (22, 28,  7, 19) | 1.023 | 2.160 | 83.52 | 68.97 |---------------------------------------|
1             Validation | 0.9434 (22, 27,  7, 19) | 1.032 | 0.997 | 83.49 | 68.65 |####################################| ^ (1.8%, 3.45, 2.8, 0%) 
1 >>  3/20 <<   Training | 0.9382 (22, 28,  7, 18) | 1.034 | 2.375 | 83.91 | 68.69 |------------------------------------|
1             Validation | 0.9411 (22, 28,  7, 18) | 1.043 | 2.107 | 83.84 | 68.44 |##################################| ^ (1.4%, 3.14, 2.8, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (634 batches)
1 >>  4/20 <<   Training | 0.9412 (19, 27,  7, 21) | 0.892 | 11.378 | 83.96 | 69.51 |---------------------------------------------|
1             Validation | 0.9441 (20, 27,  7, 21) | 0.900 | 5.592 | 83.89 | 69.19 |#########################################| ^ (1.8%, 3.45, 2.7, 0%) 
1 >>  5/20 <<   Training | 0.9356 (21, 27,  7, 21) | 0.990 | 2.150 | 84.25 | 69.38 |-------------------------------------------|
1             Validation | 0.9386 (22, 27,  7, 21) | 0.999 | 1.648 | 84.19 | 69.04 |########################################| ^ (1.8%, 3.07, 2.2, 0%) 
1 >>  6/20 <<   Training | 0.9420 (21, 25,  7, 22) | 1.035 | 3.483 | 84.33 | 69.30 |------------------------------------------|
1             Validation | 0.9455 (22, 25,  7, 21) | 1.044 | 1.828 | 84.25 | 68.84 |######################################| ^ (2.4%, 3.63, 2.7, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (317 batches)
1 >>  7/20 <<   Training | 0.9321 (21, 28,  7, 20) | 0.975 | 2.857 | 84.43 | 70.00 |--------------------------------------------------|
1             Validation | 0.9355 (21, 28,  7, 19) | 0.984 | 1.271 | 84.35 | 69.54 |#############################################| ^ (2.4%, 3.53, 2.1, 0%) 
1 >>  8/20 <<   Training | 0.9297 (22, 28,  7, 19) | 1.014 | 1.455 | 84.54 | 70.20 |---------------------------------------------------|
1             Validation | 0.9330 (22, 28,  7, 19) | 1.024 | 1.230 | 84.47 | 69.77 |###############################################| ^ (2.1%, 3.71, 2.9, 0%) 
1 >>  9/20 <<   Training | 0.9289 (21, 28,  7, 19) | 0.978 | 1.381 | 84.54 | 70.34 |-----------------------------------------------------|
1             Validation | 0.9327 (21, 28,  7, 19) | 0.987 | 0.989 | 84.45 | 69.86 |################################################| ^ (2.4%, 3.62, 1.9, 1%) 
1 >> 10/20 <<   Training | 0.9286 (22, 28,  7, 19) | 1.017 | 1.360 | 84.65 | 70.41 |------------------------------------------------------|
1             Validation | 0.9322 (22, 27,  7, 19) | 1.027 | 1.450 | 84.59 | 69.98 |#################################################| ^ (2.2%, 3.53, 2.9, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (158 batches)
1 >> 11/20 <<   Training | 0.9278 (22, 28,  7, 19) | 1.024 | 1.850 | 84.68 | 70.50 |------------------------------------------------------|
1             Validation | 0.9315 (22, 28,  7, 18) | 1.034 | 1.893 | 84.62 | 70.03 |##################################################| ^ (2.3%, 3.63, 2.7, 0%) 
1 >> 12/20 <<   Training | 0.9275 (21, 29,  7, 19) | 0.978 | 1.759 | 84.70 | 70.41 |------------------------------------------------------|
1             Validation | 0.9312 (21, 28,  7, 19) | 0.987 | 1.096 | 84.63 | 69.94 |#################################################| ^ (2.3%, 3.47, 2.3, 0%) 
1 >> 13/20 <<   Training | 0.9284 (21, 30,  6, 18) | 0.944 | 3.543 | 84.70 | 70.79 |---------------------------------------------------------|
1             Validation | 0.9322 (21, 30,  6, 18) | 0.953 | 1.766 | 84.63 | 70.26 |####################################################| ^ (2.6%, 3.55, 2.0, 0%) 
1 >> 14/20 <<   Training | 0.9285 (20, 29,  7, 20) | 0.915 | 7.172 | 84.74 | 70.62 |--------------------------------------------------------|
1             Validation | 0.9322 (21, 28,  7, 19) | 0.923 | 3.650 | 84.67 | 70.14 |###################################################| ^ (2.3%, 3.37, 2.2, 0%) 
1 >> 15/20 <<   Training | 0.9279 (21, 28,  7, 19) | 0.951 | 3.067 | 84.71 | 70.50 |-------------------------------------------------------|
1             Validation | 0.9319 (21, 28,  7, 19) | 0.960 | 1.569 | 84.63 | 69.97 |#################################################| ^ (2.6%, 3.68, 2.2, 0%) 
Decay learning rate: 0.010000 -> 0.002500
1 >> 16/20 <<   Training | 0.9264 (22, 29,  7, 18) | 0.984 | 2.447 | 84.77 | 70.61 |--------------------------------------------------------|
1             Validation | 0.9305 (22, 29,  7, 18) | 0.992 | 2.019 | 84.69 | 70.09 |##################################################| ^ (2.5%, 3.67, 2.3, 0%) 
Decay learning rate: 0.002500 -> 0.000625
1 >> 17/20 <<   Training | 0.9259 (22, 28,  7, 19) | 1.005 | 0.982 | 84.76 | 70.64 |--------------------------------------------------------|
1             Validation | 0.9301 (22, 28,  7, 19) | 1.014 | 1.310 | 84.69 | 70.10 |##################################################| ^ (2.7%, 3.82, 2.3, 0%) 
Decay learning rate: 0.000625 -> 0.000156
1 >> 18/20 <<   Training | 0.9259 (21, 29,  7, 19) | 0.992 | 1.000 | 84.76 | 70.67 |--------------------------------------------------------|
1             Validation | 0.9300 (22, 28,  7, 18) | 1.001 | 1.103 | 84.69 | 70.13 |###################################################| ^ (2.7%, 3.81, 2.1, 0%) 
Decay learning rate: 0.000156 -> 0.000039
1 >> 19/20 <<   Training | 0.9258 (22, 29,  7, 19) | 0.996 | 0.966 | 84.77 | 70.67 |--------------------------------------------------------|
1             Validation | 0.9300 (22, 28,  7, 18) | 1.005 | 1.074 | 84.69 | 70.13 |###################################################| ^ (2.7%, 3.81, 2.1, 0%) 
Decay learning rate: 0.000039 -> 0.000010
1 >> 20/20 <<   Training | 0.9258 (22, 29,  7, 19) | 0.997 | 0.896 | 84.77 | 70.67 |--------------------------------------------------------|
1             Validation | 0.9300 (22, 28,  7, 18) | 1.006 | 1.128 | 84.69 | 70.13 |###################################################| ^ (2.7%, 3.81, 2.1, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_lr0.01_epochs20_offset1_epoch20_before_finetuning.pkl
Run Finetuning
1 >> 20/20 <<   Training | 0.9258 (22, 28,  7, 19) | 0.999 | 1.134 | 84.77 | 70.67 |--------------------------------------------------------|
1             Validation | 0.9300 (22, 28,  7, 18) | 1.008 | 1.083 | 84.69 | 70.12 |###################################################| ^ (2.7%, 3.80, 2.0, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_lr0.01_epochs20_offset1_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >>  1/20 <<   Training | 0.8844 (22, 30,  8, 22) | 0.881 | 16.557 | 83.04 | 68.47 |----------------------------------|
1             Validation | 0.8862 (22, 30,  8, 22) | 0.876 | 10.220 | 83.02 | 68.15 |###############################| ^ (1.9%, 4.14, 1.7, 2%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1216 batches)
1 >>  2/20 <<   Training | 0.8727 (25, 31,  7, 20) | 1.020 | 2.844 | 83.56 | 67.88 |----------------------------|
1             Validation | 0.8755 (25, 31,  7, 20) | 1.013 | 2.785 | 83.52 | 67.28 |######################| ^ (3.4%, 3.97, 1.4, 8%) 
1 >>  3/20 <<   Training | 0.8744 (26, 33,  7, 17) | 1.045 | 4.061 | 83.91 | 68.42 |----------------------------------|
1             Validation | 0.8767 (26, 32,  7, 17) | 1.037 | 2.968 | 83.91 | 67.91 |#############################| ^ (2.8%, 3.39, 1.7, 2%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (608 batches)
1 >>  4/20 <<   Training | 0.8697 (23, 31,  7, 21) | 0.890 | 18.282 | 84.04 | 68.85 |--------------------------------------|
1             Validation | 0.8727 (23, 31,  7, 21) | 0.884 | 12.517 | 84.01 | 68.28 |################################| ^ (3.0%, 3.30, 2.0, 0%) 
1 >>  5/20 <<   Training | 0.8730 (23, 29,  7, 22) | 0.948 | 5.938 | 84.13 | 67.73 |---------------------------|
1             Validation | 0.8755 (23, 29,  7, 22) | 0.941 | 4.893 | 84.08 | 67.23 |######################| ^ (2.9%, 2.92, 1.4, 10%) 
1 >>  6/20 <<   Training | 0.8673 (24, 33,  7, 18) | 0.986 | 1.025 | 84.28 | 69.69 |----------------------------------------------|
1             Validation | 0.8702 (24, 33,  7, 18) | 0.979 | 1.840 | 84.25 | 69.09 |########################################| ^ (3.1%, 2.87, 1.9, 1%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (304 batches)
1 >>  7/20 <<   Training | 0.8645 (26, 30,  7, 19) | 1.136 | 11.903 | 84.46 | 69.34 |-------------------------------------------|
1             Validation | 0.8671 (26, 30,  7, 19) | 1.128 | 5.721 | 84.42 | 68.73 |#####################################| ^ (3.2%, 3.06, 1.3, 14%) 
1 >>  8/20 <<   Training | 0.8624 (24, 31,  7, 20) | 0.979 | 2.509 | 84.47 | 69.25 |------------------------------------------|
1             Validation | 0.8652 (24, 31,  7, 20) | 0.972 | 2.852 | 84.42 | 68.71 |#####################################| ^ (2.9%, 3.16, 1.1, 26%) 
1 >>  9/20 <<   Training | 0.8669 (25, 33,  7, 17) | 1.018 | 4.065 | 84.51 | 69.91 |-------------------------------------------------|
1             Validation | 0.8701 (25, 33,  7, 18) | 1.010 | 3.288 | 84.47 | 69.28 |##########################################| ^ (3.2%, 3.12, 2.0, 0%) 
1 >> 10/20 <<   Training | 0.8618 (25, 31,  7, 19) | 1.049 | 2.527 | 84.58 | 69.53 |---------------------------------------------|
1             Validation | 0.8650 (25, 31,  7, 19) | 1.042 | 1.128 | 84.52 | 68.83 |######################################| ^ (3.6%, 3.66, 1.6, 3%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (152 batches)
1 >> 11/20 <<   Training | 0.8612 (24, 31,  7, 21) | 0.983 | 2.171 | 84.64 | 69.45 |--------------------------------------------|
1             Validation | 0.8643 (24, 31,  7, 21) | 0.976 | 1.436 | 84.58 | 68.79 |#####################################| ^ (3.4%, 3.27, 1.0, 35%) 
1 >> 12/20 <<   Training | 0.8611 (25, 31,  7, 19) | 1.052 | 2.945 | 84.63 | 69.66 |----------------------------------------------|
1             Validation | 0.8641 (25, 31,  7, 19) | 1.044 | 1.597 | 84.58 | 69.04 |########################################| ^ (3.1%, 3.43, 1.9, 0%) 
1 >> 13/20 <<   Training | 0.8625 (24, 30,  7, 21) | 0.975 | 1.423 | 84.59 | 69.62 |----------------------------------------------|
1             Validation | 0.8657 (24, 30,  7, 21) | 0.968 | 1.340 | 84.54 | 68.98 |#######################################| ^ (3.3%, 3.70, 1.4, 7%) 
1 >> 14/20 <<   Training | 0.8611 (23, 32,  7, 20) | 0.942 | 3.460 | 84.60 | 69.68 |----------------------------------------------|
1             Validation | 0.8643 (23, 32,  7, 20) | 0.936 | 3.417 | 84.56 | 69.00 |########################################| ^ (3.5%, 3.30, 1.7, 1%) 
1 >> 15/20 <<   Training | 0.8607 (25, 30,  7, 20) | 1.048 | 2.331 | 84.55 | 69.44 |--------------------------------------------|
1             Validation | 0.8639 (25, 30,  7, 20) | 1.040 | 2.203 | 84.50 | 68.81 |######################################| ^ (3.2%, 4.13, 1.6, 4%) 
Decay learning rate: 0.010000 -> 0.002500
1 >> 16/20 <<   Training | 0.8596 (24, 32,  7, 19) | 0.965 | 1.506 | 84.72 | 70.00 |-------------------------------------------------|
1             Validation | 0.8630 (24, 32,  7, 19) | 0.959 | 2.368 | 84.67 | 69.30 |###########################################| ^ (3.5%, 3.87, 1.7, 2%) 
Decay learning rate: 0.002500 -> 0.000625
1 >> 17/20 <<   Training | 0.8589 (24, 31,  7, 20) | 1.002 | 1.008 | 84.72 | 69.92 |-------------------------------------------------|
1             Validation | 0.8623 (25, 31,  7, 20) | 0.995 | 1.961 | 84.67 | 69.23 |##########################################| ^ (3.4%, 3.96, 1.8, 1%) 
Decay learning rate: 0.000625 -> 0.000156
1 >> 18/20 <<   Training | 0.8588 (24, 31,  7, 20) | 1.003 | 0.681 | 84.72 | 69.93 |-------------------------------------------------|
1             Validation | 0.8622 (24, 31,  7, 20) | 0.996 | 1.607 | 84.67 | 69.25 |##########################################| ^ (3.4%, 3.93, 1.8, 1%) 
Decay learning rate: 0.000156 -> 0.000039
1 >> 19/20 <<   Training | 0.8588 (24, 31,  7, 20) | 1.000 | 0.713 | 84.72 | 69.92 |-------------------------------------------------|
1             Validation | 0.8622 (24, 31,  7, 20) | 0.993 | 1.514 | 84.67 | 69.24 |##########################################| ^ (3.4%, 3.93, 1.7, 2%) 
Decay learning rate: 0.000039 -> 0.000010
1 >> 20/20 <<   Training | 0.8588 (24, 31,  7, 20) | 0.999 | 0.755 | 84.72 | 69.92 |-------------------------------------------------|
1             Validation | 0.8622 (24, 31,  7, 20) | 0.992 | 1.553 | 84.67 | 69.24 |##########################################| ^ (3.4%, 3.92, 1.8, 1%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_lr0.01_epochs20_offset1_epoch20_before_finetuning.pkl
Run Finetuning
1 >> 20/20 <<   Training | 0.8588 (24, 31,  7, 20) | 0.999 | 0.811 | 84.72 | 69.92 |-------------------------------------------------|
1             Validation | 0.8622 (24, 31,  7, 20) | 0.992 | 1.719 | 84.67 | 69.24 |##########################################| ^ (3.4%, 3.92, 1.8, 1%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_lr0.01_epochs20_offset1_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
