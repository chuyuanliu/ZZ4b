0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6637 (40, 11, 18, 32, 18) | 0.735 | 0.879 | 87.95 | 85.31 |-------------|
0             Validation | 0.6648 (40, 11, 18, 32, 19) | 0.745 | 0.887 | 88.10 | 85.42 |##############| ^ (0.4%, 10.81, 1.3, 15%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1032 batches)
0 >>  2/20 <<   Training | 0.6501 (38, 11, 19, 32, 20) | 0.826 | 0.973 | 87.87 | 86.10 |---------------------|
0             Validation | 0.6516 (38, 11, 19, 32, 20) | 0.856 | 1.004 | 88.11 | 86.20 |#####################| ^ (0.3%, 10.83, 0.8, 64%) 
0 >>  3/20 <<   Training | 0.6516 (42, 11, 19, 29, 18) | 0.813 | 0.959 | 87.89 | 86.10 |---------------------|
0             Validation | 0.6531 (42, 11, 19, 29, 18) | 0.827 | 0.972 | 88.03 | 86.19 |#####################| ^ (0.3%, 10.84, 1.3, 13%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (516 batches)
0 >>  4/20 <<   Training | 0.6433 (39, 11, 18, 31, 21) | 0.901 | 1.062 | 88.54 | 86.34 |-----------------------|
0             Validation | 0.6446 (39, 11, 18, 31, 21) | 0.936 | 1.098 | 88.74 | 86.43 |########################| ^ (0.3%, 9.77, 1.0, 42%) 
0 >>  5/20 <<   Training | 0.6456 (38, 10, 17, 33, 21) | 0.883 | 1.037 | 88.60 | 86.45 |------------------------|
0             Validation | 0.6470 (38, 10, 17, 33, 21) | 0.924 | 1.083 | 88.72 | 86.56 |#########################| ^ (0.4%, 10.39, 0.7, 74%) 
0 >>  6/20 <<   Training | 0.6444 (37, 11, 18, 32, 22) | 0.916 | 1.076 | 88.62 | 86.51 |-------------------------|
0             Validation | 0.6454 (37, 11, 18, 32, 22) | 0.960 | 1.121 | 88.75 | 86.62 |##########################| ^ (0.3%, 10.61, 0.7, 76%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (258 batches)
0 >>  7/20 <<   Training | 0.6390 (38, 11, 18, 32, 21) | 0.913 | 1.074 | 88.56 | 86.54 |-------------------------|
0             Validation | 0.6406 (38, 11, 18, 31, 21) | 0.936 | 1.096 | 88.69 | 86.61 |##########################| ^ (0.3%, 9.82, 0.9, 58%) 
0 >>  8/20 <<   Training | 0.6399 (40, 11, 18, 30, 19) | 0.870 | 1.028 | 88.58 | 86.49 |------------------------|
0             Validation | 0.6419 (40, 11, 18, 30, 19) | 0.893 | 1.050 | 88.78 | 86.56 |#########################| ^ (0.3%, 10.01, 1.1, 30%) 
0 >>  9/20 <<   Training | 0.6390 (37, 11, 18, 32, 20) | 0.914 | 1.075 | 88.70 | 86.50 |-------------------------|
0             Validation | 0.6405 (37, 11, 18, 32, 21) | 0.946 | 1.109 | 88.88 | 86.59 |#########################| ^ (0.4%, 9.75, 0.9, 53%) 
0 >> 10/20 <<   Training | 0.6421 (37, 11, 17, 34, 21) | 0.929 | 1.091 | 88.69 | 86.56 |-------------------------|
0             Validation | 0.6442 (37, 11, 17, 33, 21) | 0.962 | 1.126 | 88.76 | 86.63 |##########################| ^ (0.3%, 10.06, 1.5, 5%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (129 batches)
0 >> 11/20 <<   Training | 0.6393 (39, 11, 17, 31, 21) | 0.927 | 1.089 | 88.95 | 86.66 |--------------------------|
0             Validation | 0.6409 (39, 11, 17, 31, 21) | 0.956 | 1.120 | 89.10 | 86.74 |###########################| ^ (0.3%, 10.11, 0.7, 79%) 
0 >> 12/20 <<   Training | 0.6394 (40, 10, 18, 32, 20) | 0.925 | 1.086 | 88.90 | 86.59 |-------------------------|
0             Validation | 0.6411 (40, 10, 18, 32, 20) | 0.957 | 1.121 | 89.05 | 86.67 |##########################| ^ (0.3%, 10.05, 1.1, 32%) 
0 >> 13/20 <<   Training | 0.6408 (38, 11, 17, 32, 22) | 0.924 | 1.087 | 88.88 | 86.65 |--------------------------|
0             Validation | 0.6423 (38, 11, 17, 32, 22) | 0.966 | 1.131 | 88.98 | 86.74 |###########################| ^ (0.4%, 10.13, 0.8, 68%) 
0 >> 14/20 <<   Training | 0.6420 (41, 10, 17, 31, 19) | 0.907 | 1.065 | 88.92 | 86.63 |--------------------------|
0             Validation | 0.6438 (41, 10, 17, 31, 19) | 0.944 | 1.106 | 89.03 | 86.69 |##########################| ^ (0.3%, 9.81, 1.8, 1%) 
0 >> 15/20 <<   Training | 0.6366 (39, 11, 18, 31, 20) | 0.905 | 1.065 | 88.83 | 86.58 |-------------------------|
0             Validation | 0.6386 (38, 11, 18, 31, 20) | 0.929 | 1.089 | 88.99 | 86.65 |##########################| ^ (0.3%, 10.13, 1.5, 6%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6356 (38, 11, 18, 32, 20) | 0.935 | 1.098 | 88.95 | 86.70 |--------------------------|
0             Validation | 0.6374 (38, 11, 18, 32, 21) | 0.963 | 1.127 | 89.08 | 86.76 |###########################| ^ (0.3%, 10.26, 1.0, 43%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6354 (38, 11, 18, 32, 20) | 0.936 | 1.099 | 88.94 | 86.71 |---------------------------|
0             Validation | 0.6372 (38, 11, 18, 32, 20) | 0.962 | 1.126 | 89.06 | 86.77 |###########################| ^ (0.3%, 10.18, 1.3, 14%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6361 (38, 11, 18, 32, 21) | 0.935 | 1.098 | 88.97 | 86.73 |---------------------------|
0             Validation | 0.6378 (38, 11, 18, 32, 21) | 0.964 | 1.129 | 89.09 | 86.79 |###########################| ^ (0.3%, 10.17, 1.2, 23%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6355 (38, 11, 18, 32, 21) | 0.937 | 1.100 | 88.96 | 86.72 |---------------------------|
0             Validation | 0.6373 (38, 11, 18, 32, 21) | 0.964 | 1.129 | 89.08 | 86.78 |###########################| ^ (0.3%, 10.18, 1.4, 10%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6356 (38, 11, 18, 32, 20) | 0.934 | 1.097 | 88.96 | 86.72 |---------------------------|
0             Validation | 0.6374 (38, 11, 18, 32, 21) | 0.964 | 1.129 | 89.08 | 86.78 |###########################| ^ (0.3%, 10.18, 1.5, 6%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7061 (28, 12, 11, 19, 10) | 0.868 | 1.023 | 87.02 | 85.63 |----------------|
0             Validation | 0.7064 (28, 12, 11, 19, 10) | 0.880 | 1.032 | 87.10 | 85.75 |#################| ^ (0.4%, 4.21, 1.6, 3%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1032 batches)
0 >>  2/20 <<   Training | 0.6909 (26, 13, 11, 19, 11) | 0.919 | 1.083 | 87.37 | 86.12 |---------------------|
0             Validation | 0.6916 (26, 13, 11, 19, 11) | 0.945 | 1.109 | 87.56 | 86.18 |#####################| ^ (0.3%, 4.33, 1.6, 4%) 
0 >>  3/20 <<   Training | 0.6919 (27, 13, 11, 18, 11) | 0.939 | 1.105 | 87.25 | 86.19 |---------------------|
0             Validation | 0.6926 (27, 13, 11, 18, 11) | 0.968 | 1.134 | 87.25 | 86.26 |######################| ^ (0.4%, 4.18, 1.3, 11%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (516 batches)
0 >>  4/20 <<   Training | 0.6823 (24, 13, 11, 19, 13) | 1.001 | 1.177 | 87.87 | 86.55 |-------------------------|
0             Validation | 0.6828 (24, 13, 11, 19, 13) | 1.036 | 1.213 | 87.97 | 86.62 |##########################| ^ (0.3%, 4.07, 3.0, 0%) 
0 >>  5/20 <<   Training | 0.6815 (23, 12, 12, 20, 13) | 1.003 | 1.178 | 87.86 | 86.51 |-------------------------|
0             Validation | 0.6821 (23, 12, 12, 20, 13) | 1.043 | 1.221 | 87.97 | 86.60 |#########################| ^ (0.4%, 4.47, 1.7, 2%) 
0 >>  6/20 <<   Training | 0.6873 (22, 13, 10, 20, 14) | 0.985 | 1.157 | 87.95 | 86.55 |-------------------------|
0             Validation | 0.6876 (22, 13, 10, 20, 14) | 1.010 | 1.182 | 88.09 | 86.63 |##########################| ^ (0.4%, 4.38, 1.7, 2%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (258 batches)
0 >>  7/20 <<   Training | 0.6812 (25, 13, 11, 19, 12) | 0.982 | 1.155 | 88.11 | 86.53 |-------------------------|
0             Validation | 0.6822 (25, 13, 11, 19, 12) | 1.010 | 1.182 | 88.28 | 86.58 |#########################| ^ (0.3%, 4.07, 2.1, 0%) 
0 >>  8/20 <<   Training | 0.6836 (27, 13, 11, 18, 11) | 0.997 | 1.172 | 88.00 | 86.61 |--------------------------|
0             Validation | 0.6847 (27, 13, 11, 18, 11) | 1.027 | 1.202 | 88.14 | 86.66 |##########################| ^ (0.3%, 4.30, 2.5, 0%) 
0 >>  9/20 <<   Training | 0.6805 (23, 13, 11, 20, 13) | 1.015 | 1.192 | 87.96 | 86.59 |-------------------------|
0             Validation | 0.6816 (23, 13, 11, 20, 13) | 1.052 | 1.232 | 88.16 | 86.66 |##########################| ^ (0.3%, 4.19, 1.7, 2%) 
0 >> 10/20 <<   Training | 0.6840 (26, 13, 10, 19, 12) | 1.023 | 1.202 | 88.18 | 86.60 |-------------------------|
0             Validation | 0.6854 (26, 13, 10, 19, 12) | 1.054 | 1.233 | 88.27 | 86.63 |##########################| ^ (0.3%, 4.32, 2.0, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (129 batches)
0 >> 11/20 <<   Training | 0.6825 (25, 13, 10, 19, 13) | 1.032 | 1.212 | 88.33 | 86.77 |---------------------------|
0             Validation | 0.6835 (25, 13, 10, 19, 13) | 1.064 | 1.245 | 88.47 | 86.82 |############################| ^ (0.3%, 4.17, 1.9, 1%) 
0 >> 12/20 <<   Training | 0.6785 (23, 12, 11, 20, 13) | 1.032 | 1.212 | 88.29 | 86.72 |---------------------------|
0             Validation | 0.6794 (23, 12, 11, 20, 13) | 1.062 | 1.243 | 88.40 | 86.78 |###########################| ^ (0.3%, 4.33, 1.4, 10%) 
0 >> 13/20 <<   Training | 0.6825 (25, 13, 10, 18, 13) | 1.029 | 1.208 | 88.28 | 86.75 |---------------------------|
0             Validation | 0.6834 (25, 13, 10, 18, 13) | 1.068 | 1.250 | 88.41 | 86.82 |############################| ^ (0.3%, 4.34, 1.7, 2%) 
0 >> 14/20 <<   Training | 0.6832 (27, 12, 10, 19, 12) | 1.026 | 1.204 | 88.31 | 86.69 |--------------------------|
0             Validation | 0.6843 (27, 12, 10, 19, 12) | 1.062 | 1.242 | 88.47 | 86.73 |###########################| ^ (0.3%, 4.20, 2.1, 0%) 
0 >> 15/20 <<   Training | 0.6793 (26, 13, 11, 19, 12) | 1.019 | 1.197 | 88.21 | 86.68 |--------------------------|
0             Validation | 0.6805 (26, 13, 11, 19, 12) | 1.045 | 1.223 | 88.39 | 86.73 |###########################| ^ (0.3%, 4.17, 1.5, 6%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6772 (24, 13, 11, 20, 12) | 1.033 | 1.212 | 88.28 | 86.73 |---------------------------|
0             Validation | 0.6785 (24, 13, 11, 20, 12) | 1.062 | 1.242 | 88.43 | 86.78 |###########################| ^ (0.3%, 4.26, 1.4, 10%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6769 (24, 13, 11, 20, 12) | 1.030 | 1.209 | 88.29 | 86.77 |---------------------------|
0             Validation | 0.6781 (24, 13, 11, 20, 12) | 1.061 | 1.241 | 88.42 | 86.81 |############################| ^ (0.3%, 4.29, 1.6, 3%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6775 (25, 13, 11, 19, 12) | 1.033 | 1.213 | 88.32 | 86.78 |---------------------------|
0             Validation | 0.6787 (25, 13, 11, 19, 12) | 1.064 | 1.244 | 88.45 | 86.82 |############################| ^ (0.3%, 4.29, 1.6, 3%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6771 (24, 13, 11, 20, 12) | 1.032 | 1.212 | 88.32 | 86.77 |---------------------------|
0             Validation | 0.6783 (24, 13, 11, 20, 12) | 1.063 | 1.243 | 88.45 | 86.82 |############################| ^ (0.3%, 4.29, 1.6, 3%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6772 (24, 13, 11, 20, 12) | 1.032 | 1.211 | 88.32 | 86.77 |---------------------------|
0             Validation | 0.6784 (24, 13, 11, 20, 12) | 1.064 | 1.245 | 88.45 | 86.82 |############################| ^ (0.3%, 4.29, 1.4, 7%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
Thu Apr 14 10:45:18 2022
Using weight: weight for classifier: SvB
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 2396428
wDB 113735.127030
wDBn -29.981275
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 128876
wT 4694.203751
wTn -16.779144
Not obvious how to handle negative ttbar weights, for now remove them
nS 645566
nB 2524908
sum_wS 201.702339
sum_wB 118506.072474
nzz =   68867, wzz =   71.9, wzz_SR =   59.2
nzh =  541264, wzh =   92.5, wzh_SR =   77.9
nhh =   35435, whh =   37.3, whh_SR =   33.4
sum_wS_SR 170.521446
sum_wB_SR 50064.653301
sum_wSp_SR 203.140887
sum_wSn_SR 32.619440
Negative weight signal events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 263.065004
sigScaleZH 390.354672
sigScaleHH 609.145944
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.192666
Setup training/validation tensors
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
Thu Apr 14 10:59:33 2022
Using weight: weight for classifier: SvB
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 2396428
wDB 113735.127030
wDBn -29.981275
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 128876
wT 4694.203751
wTn -16.779144
nS 645566
nB 2525304
sum_wS 201.702339
sum_wB 118489.293330
nzz =   68867, wzz =   71.9, wzz_SR =   59.2
nzh =  541264, wzh =   92.5, wzh_SR =   77.9
nhh =   35435, whh =   37.3, whh_SR =   33.4
sum_wS_SR 170.521446
sum_wB_SR 50056.193175
sum_wSp_SR 203.140887
sum_wSn_SR 32.619440
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 263.031478
sigScaleZH 390.285279
sigScaleHH 609.010109
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.192394
Setup training/validation tensors
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6616 (38, 10, 18, 33, 20) | 0.867 | 1.021 | 88.23 | 86.01 |--------------------|
0             Validation | 0.6615 (38, 10, 18, 33, 20) | 0.889 | 1.043 | 88.27 | 86.10 |####################| ^ (0.3%, 12.09, 0.9, 56%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1032 batches)
0 >>  2/20 <<   Training | 0.6508 (41, 10, 18, 30, 20) | 0.878 | 1.032 | 88.20 | 86.21 |----------------------|
0             Validation | 0.6514 (41, 10, 18, 30, 20) | 0.905 | 1.062 | 88.39 | 86.29 |######################| ^ (0.3%, 10.02, 1.0, 42%) 
0 >>  3/20 <<   Training | 0.6473 (39, 11, 17, 32, 21) | 0.907 | 1.064 | 88.83 | 86.51 |-------------------------|
0             Validation | 0.6483 (39, 11, 17, 31, 21) | 0.933 | 1.094 | 88.81 | 86.58 |#########################| ^ (0.3%, 11.45, 1.2, 16%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (516 batches)
0 >>  4/20 <<   Training | 0.6436 (40, 11, 18, 32, 18) | 0.826 | 0.971 | 88.79 | 86.39 |-----------------------|
0             Validation | 0.6446 (40, 11, 18, 32, 18) | 0.832 | 0.975 | 88.86 | 86.44 |########################| ^ (0.4%, 11.40, 1.1, 29%) 
0 >>  5/20 <<   Training | 0.6432 (41, 11, 17, 30, 20) | 0.914 | 1.073 | 89.05 | 86.64 |--------------------------|
0             Validation | 0.6444 (41, 11, 17, 30, 20) | 0.925 | 1.084 | 89.15 | 86.68 |##########################| ^ (0.4%, 10.88, 1.4, 8%) 
0 >>  6/20 <<   Training | 0.6468 (42, 11, 16, 29, 20) | 0.915 | 1.076 | 89.11 | 86.58 |-------------------------|
0             Validation | 0.6484 (42, 11, 16, 29, 21) | 0.932 | 1.093 | 89.19 | 86.61 |##########################| ^ (0.4%, 10.53, 1.1, 29%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (258 batches)
0 >>  7/20 <<   Training | 0.6361 (38, 11, 18, 31, 21) | 0.941 | 1.104 | 89.09 | 86.68 |--------------------------|
0             Validation | 0.6375 (38, 11, 18, 31, 21) | 0.960 | 1.123 | 89.21 | 86.71 |###########################| ^ (0.4%, 11.34, 1.3, 12%) 
Thu Apr 14 13:18:27 2022
Using weight: weight for classifier: SvB
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 2396428
wDB 113735.127030
wDBn -29.981275
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 128876
wT 4694.203751
wTn -16.779144
nS 645566
nB 2525304
sum_wS 201.702339
sum_wB 118489.293330
nzz =   68867, wzz =   71.9, wzz_SR =   59.2
nzh =  541264, wzh =   92.5, wzh_SR =   77.9
nhh =   35435, whh =   37.3, whh_SR =   33.4
sum_wS_SR 170.521446
sum_wB_SR 50056.193175
sum_wSp_SR 203.140887
sum_wSn_SR 32.619440
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 263.031478
sigScaleZH 390.285279
sigScaleHH 609.010109
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.192394
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset0.log
Setup training/validation tensors
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6616 (38, 10, 18, 33, 20) | 0.867 | 1.021 | 88.23 | 86.01 |--------------------|
0             Validation | 0.6615 (38, 10, 18, 33, 20) | 0.889 | 1.043 | 88.27 | 86.10 |####################| ^ (0.3%, 12.09, 0.9, 56%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1032 batches)
0 >>  2/20 <<   Training | 0.6508 (41, 10, 18, 30, 20) | 0.878 | 1.032 | 88.20 | 86.21 |----------------------|
0             Validation | 0.6514 (41, 10, 18, 30, 20) | 0.905 | 1.062 | 88.39 | 86.29 |######################| ^ (0.3%, 10.02, 1.0, 42%) 
0 >>  3/20 <<   Training | 0.6473 (39, 11, 17, 32, 21) | 0.907 | 1.064 | 88.83 | 86.51 |-------------------------|
0             Validation | 0.6483 (39, 11, 17, 31, 21) | 0.933 | 1.094 | 88.81 | 86.58 |#########################| ^ (0.3%, 11.45, 1.2, 16%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (516 batches)
0 >>  4/20 <<   Training | 0.6436 (40, 11, 18, 32, 18) | 0.826 | 0.971 | 88.79 | 86.39 |-----------------------|
0             Validation | 0.6446 (40, 11, 18, 32, 18) | 0.832 | 0.975 | 88.86 | 86.44 |########################| ^ (0.4%, 11.40, 1.1, 29%) 
0 >>  5/20 <<   Training | 0.6432 (41, 11, 17, 30, 20) | 0.914 | 1.073 | 89.05 | 86.64 |--------------------------|
0             Validation | 0.6444 (41, 11, 17, 30, 20) | 0.925 | 1.084 | 89.15 | 86.68 |##########################| ^ (0.4%, 10.88, 1.4, 8%) 
0 >>  6/20 <<   Training | 0.6468 (42, 11, 16, 29, 20) | 0.915 | 1.076 | 89.11 | 86.58 |-------------------------|
0             Validation | 0.6484 (42, 11, 16, 29, 21) | 0.932 | 1.093 | 89.19 | 86.61 |##########################| ^ (0.4%, 10.53, 1.1, 29%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (258 batches)
0 >>  7/20 <<   Training | 0.6361 (38, 11, 18, 31, 21) | 0.941 | 1.104 | 89.09 | 86.68 |--------------------------|
0             Validation | 0.6375 (38, 11, 18, 31, 21) | 0.960 | 1.123 | 89.21 | 86.71 |###########################| ^ (0.4%, 11.34, 1.3, 12%) 
0 >>  8/20 <<   Training | 0.6359 (37, 11, 18, 32, 21) | 0.942 | 1.105 | 89.00 | 86.70 |---------------------------|
0             Validation | 0.6374 (37, 11, 18, 32, 21) | 0.965 | 1.130 | 89.17 | 86.72 |###########################| ^ (0.4%, 11.23, 1.1, 33%) 
0 >>  9/20 <<   Training | 0.6368 (39, 11, 18, 32, 20) | 0.947 | 1.111 | 89.07 | 86.71 |---------------------------|
0             Validation | 0.6383 (39, 11, 18, 32, 20) | 0.971 | 1.136 | 89.20 | 86.73 |###########################| ^ (0.4%, 11.92, 1.7, 2%) 
0 >> 10/20 <<   Training | 0.6373 (38, 11, 18, 32, 21) | 0.942 | 1.106 | 89.11 | 86.63 |--------------------------|
0             Validation | 0.6387 (38, 10, 18, 32, 21) | 0.963 | 1.128 | 89.19 | 86.66 |##########################| ^ (0.4%, 11.14, 1.8, 1%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (129 batches)
0 >> 11/20 <<   Training | 0.6392 (41, 11, 17, 31, 20) | 0.935 | 1.098 | 89.16 | 86.74 |---------------------------|
0             Validation | 0.6406 (41, 11, 17, 31, 20) | 0.953 | 1.118 | 89.35 | 86.77 |###########################| ^ (0.3%, 10.71, 1.5, 5%) 
0 >> 12/20 <<   Training | 0.6371 (38, 11, 17, 32, 21) | 0.942 | 1.106 | 89.23 | 86.64 |--------------------------|
0             Validation | 0.6384 (38, 11, 17, 32, 21) | 0.961 | 1.126 | 89.37 | 86.67 |##########################| ^ (0.4%, 11.20, 1.5, 5%) 
0 >> 13/20 <<   Training | 0.6374 (39, 11, 17, 33, 20) | 0.935 | 1.097 | 89.20 | 86.76 |---------------------------|
0             Validation | 0.6385 (39, 11, 17, 32, 20) | 0.964 | 1.129 | 89.37 | 86.79 |###########################| ^ (0.3%, 11.55, 1.1, 30%) 
0 >> 14/20 <<   Training | 0.6355 (35, 11, 18, 34, 21) | 0.972 | 1.140 | 89.20 | 86.76 |---------------------------|
0             Validation | 0.6367 (35, 11, 18, 34, 21) | 0.978 | 1.145 | 89.37 | 86.78 |###########################| ^ (0.4%, 11.34, 1.2, 19%) 
0 >> 15/20 <<   Training | 0.6387 (40, 11, 17, 31, 21) | 0.933 | 1.095 | 89.33 | 86.70 |---------------------------|
0             Validation | 0.6397 (40, 11, 17, 31, 21) | 0.965 | 1.130 | 89.50 | 86.74 |###########################| ^ (0.4%, 10.81, 2.6, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6334 (38, 11, 18, 33, 20) | 0.948 | 1.113 | 89.22 | 86.78 |---------------------------|
0             Validation | 0.6346 (38, 11, 18, 33, 20) | 0.970 | 1.136 | 89.40 | 86.81 |############################| ^ (0.4%, 11.55, 2.2, 0%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6328 (37, 11, 18, 32, 21) | 0.959 | 1.126 | 89.27 | 86.79 |---------------------------|
0             Validation | 0.6340 (37, 11, 18, 32, 21) | 0.982 | 1.149 | 89.43 | 86.81 |############################| ^ (0.4%, 11.71, 2.4, 0%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6329 (38, 11, 18, 32, 20) | 0.956 | 1.122 | 89.26 | 86.80 |----------------------------|
0             Validation | 0.6341 (38, 11, 18, 32, 21) | 0.978 | 1.145 | 89.43 | 86.83 |############################| ^ (0.4%, 11.69, 2.2, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6332 (38, 11, 18, 32, 20) | 0.958 | 1.124 | 89.27 | 86.81 |----------------------------|
0             Validation | 0.6344 (38, 11, 18, 32, 21) | 0.980 | 1.146 | 89.44 | 86.83 |############################| ^ (0.4%, 11.68, 2.2, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6332 (38, 11, 18, 32, 20) | 0.957 | 1.123 | 89.27 | 86.81 |----------------------------|
0             Validation | 0.6344 (38, 11, 18, 32, 21) | 0.978 | 1.145 | 89.44 | 86.84 |############################| ^ (0.4%, 11.68, 2.2, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

0 >> DONE <<
Mon Apr 18 10:04:41 2022
Using weight: weight for classifier: SvB
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 2396428
wDB 114777.240743
wDBn -53.981695
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 128876
wT 4694.203751
wTn -16.779144
nS 645566
nB 2525304
sum_wS 201.702339
sum_wB 119579.407884
nzz =   68867, wzz =   71.9, wzz_SR =   59.2
nzh =  541264, wzh =   92.5, wzh_SR =   77.9
nhh =   35435, whh =   37.3, whh_SR =   33.4
sum_wS_SR 170.521446
sum_wB_SR 50599.001540
sum_wSp_SR 203.140887
sum_wSn_SR 32.619440
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 264.795507
sigScaleZH 394.679756
sigScaleHH 619.280794
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.192839
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset0.log
Setup training/validation tensors
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6732 (45, 10, 15, 31, 19) | 0.775 | 0.920 | 88.23 | 85.73 |-----------------|
0             Validation | 0.6745 (44, 11, 15, 31, 19) | 0.770 | 0.911 | 87.92 | 85.81 |##################| ^ (0.3%, 11.74, 9.6, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1032 batches)
0 >>  2/20 <<   Training | 0.6490 (40, 11, 17, 31, 21) | 0.883 | 1.037 | 88.59 | 86.25 |----------------------|
0             Validation | 0.6513 (39, 11, 17, 31, 21) | 0.890 | 1.043 | 88.31 | 86.31 |#######################| ^ (0.2%, 12.49, 9.2, 0%) 
0 >>  3/20 <<   Training | 0.6459 (37, 11, 18, 34, 19) | 0.833 | 0.980 | 88.64 | 86.09 |--------------------|
0             Validation | 0.6487 (37, 11, 18, 34, 19) | 0.831 | 0.976 | 88.42 | 86.09 |####################| ^ (0.2%, 12.02, 9.7, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (516 batches)
0 >>  4/20 <<   Training | 0.6391 (39, 10, 18, 32, 20) | 0.881 | 1.036 | 88.96 | 86.43 |------------------------|
0             Validation | 0.6420 (38, 11, 18, 32, 20) | 0.872 | 1.023 | 88.62 | 86.47 |########################| ^ (0.2%, 13.16, 9.0, 0%) 
0 >>  5/20 <<   Training | 0.6458 (39, 10, 16, 33, 20) | 0.929 | 1.091 | 88.94 | 86.53 |-------------------------|
0             Validation | 0.6479 (38, 11, 16, 34, 20) | 0.938 | 1.100 | 88.65 | 86.62 |##########################| ^ (0.3%, 12.25, 9.2, 0%) 
0 >>  6/20 <<   Training | 0.6400 (34, 10, 18, 34, 23) | 0.981 | 1.152 | 89.09 | 86.60 |-------------------------|
0             Validation | 0.6435 (33, 11, 18, 35, 23) | 0.978 | 1.144 | 88.78 | 86.60 |##########################| ^ (0.2%, 11.93, 9.0, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (258 batches)
0 >>  7/20 <<   Training | 0.6357 (36, 11, 18, 33, 22) | 0.941 | 1.105 | 89.21 | 86.66 |--------------------------|
0             Validation | 0.6390 (36, 11, 18, 33, 22) | 0.950 | 1.112 | 88.83 | 86.67 |##########################| ^ (0.2%, 11.78, 8.6, 0%) 
0 >>  8/20 <<   Training | 0.6349 (37, 11, 18, 32, 21) | 0.952 | 1.119 | 89.22 | 86.70 |--------------------------|
0             Validation | 0.6378 (37, 11, 18, 33, 21) | 0.962 | 1.127 | 88.84 | 86.73 |###########################| ^ (0.2%, 12.04, 8.7, 0%) 
0 >>  9/20 <<   Training | 0.6376 (41, 11, 18, 31, 20) | 0.920 | 1.079 | 89.22 | 86.66 |--------------------------|
0             Validation | 0.6400 (40, 11, 18, 31, 20) | 0.911 | 1.066 | 88.90 | 86.71 |###########################| ^ (0.2%, 11.86, 9.3, 0%) 
0 >> 10/20 <<   Training | 0.6365 (38, 11, 18, 32, 21) | 0.930 | 1.092 | 89.31 | 86.53 |-------------------------|
0             Validation | 0.6396 (37, 11, 18, 32, 21) | 0.923 | 1.079 | 88.92 | 86.56 |#########################| ^ (0.2%, 11.01, 9.2, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (129 batches)
0 >> 11/20 <<   Training | 0.6414 (39, 11, 16, 32, 21) | 0.914 | 1.075 | 89.37 | 86.62 |--------------------------|
0             Validation | 0.6440 (39, 11, 16, 33, 21) | 0.920 | 1.078 | 89.00 | 86.67 |##########################| ^ (0.2%, 11.59, 9.5, 0%) 
0 >> 12/20 <<   Training | 0.6329 (39, 11, 18, 32, 19) | 0.890 | 1.047 | 89.19 | 86.63 |--------------------------|
0             Validation | 0.6354 (39, 11, 18, 32, 19) | 0.895 | 1.047 | 88.86 | 86.68 |##########################| ^ (0.3%, 11.82, 9.2, 0%) 
0 >> 13/20 <<   Training | 0.6347 (39, 11, 17, 32, 21) | 0.934 | 1.096 | 89.41 | 86.71 |---------------------------|
0             Validation | 0.6373 (38, 11, 17, 32, 21) | 0.957 | 1.118 | 89.03 | 86.75 |###########################| ^ (0.3%, 11.12, 9.0, 0%) 
0 >> 14/20 <<   Training | 0.6340 (38, 11, 18, 32, 21) | 0.947 | 1.113 | 89.33 | 86.74 |---------------------------|
0             Validation | 0.6371 (37, 11, 18, 33, 21) | 0.951 | 1.113 | 88.96 | 86.76 |###########################| ^ (0.2%, 11.16, 8.8, 0%) 
0 >> 15/20 <<   Training | 0.6340 (40, 10, 18, 31, 20) | 0.938 | 1.100 | 89.21 | 86.73 |---------------------------|
0             Validation | 0.6367 (39, 11, 18, 31, 20) | 0.937 | 1.096 | 88.85 | 86.75 |###########################| ^ (0.2%, 10.75, 8.9, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6321 (38, 11, 18, 32, 21) | 0.947 | 1.111 | 89.26 | 86.77 |---------------------------|
0             Validation | 0.6349 (37, 11, 18, 33, 21) | 0.958 | 1.119 | 88.90 | 86.81 |############################| ^ (0.2%, 11.80, 9.5, 0%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6323 (39, 11, 18, 32, 20) | 0.944 | 1.108 | 89.41 | 86.78 |---------------------------|
0             Validation | 0.6350 (38, 11, 18, 32, 20) | 0.953 | 1.114 | 89.03 | 86.82 |############################| ^ (0.2%, 11.63, 8.9, 0%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6324 (38, 11, 18, 32, 21) | 0.947 | 1.112 | 89.42 | 86.79 |---------------------------|
0             Validation | 0.6351 (38, 11, 18, 33, 21) | 0.961 | 1.123 | 89.04 | 86.83 |############################| ^ (0.2%, 11.64, 9.1, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6321 (38, 11, 18, 32, 21) | 0.949 | 1.114 | 89.41 | 86.79 |---------------------------|
0             Validation | 0.6348 (38, 11, 18, 33, 21) | 0.960 | 1.122 | 89.03 | 86.83 |############################| ^ (0.2%, 11.67, 8.8, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6320 (38, 11, 18, 32, 21) | 0.949 | 1.115 | 89.41 | 86.79 |---------------------------|
0             Validation | 0.6348 (38, 11, 18, 33, 21) | 0.959 | 1.121 | 89.03 | 86.83 |############################| ^ (0.2%, 11.68, 8.9, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

0 >> DONE <<
0 Minimum Loss = 0.632009
Mon May  2 09:55:55 2022
Using weight: weight for classifier: SvB
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 3279028
wDB 148604.957728
wDBn -54.520430
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 179878
wT 6424.617429
wTn -22.434503
nS 770238
nB 3458906
sum_wS 187.901514
sum_wB 155138.616017
nzz =  259880, wzz =   64.5, wzz_SR =   51.7
nzh =  473043, wzh =   84.0, wzh_SR =   69.4
nhh =   37315, whh =   39.4, whh_SR =   34.7
sum_wS_SR 155.770448
sum_wB_SR 57581.386117
sum_wSp_SR 183.921904
sum_wSn_SR 28.151456
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 322.752545
sigScaleZH 495.078449
sigScaleHH 738.114058
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.161672
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset0.log
Setup training/validation tensors
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6484 (39, 10, 18, 30, 19) | 0.713 | 0.855 | 86.76 | 87.40 |---------------------------------|
0             Validation | 0.6489 (39, 10, 18, 30, 19) | 0.723 | 0.866 | 87.16 | 87.46 |##################################| ^ (0.2%, 9.95, 5.1, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1376 batches)
0 >>  2/20 <<   Training | 0.6295 (38, 12, 16, 29, 21) | 0.807 | 0.965 | 88.83 | 88.08 |----------------------------------------|
0             Validation | 0.6305 (38, 12, 16, 29, 21) | 0.813 | 0.973 | 89.13 | 88.13 |#########################################| ^ (0.2%, 10.94, 5.8, 0%) 
0 >>  3/20 <<   Training | 0.6258 (37, 12, 17, 31, 20) | 0.765 | 0.915 | 88.53 | 88.02 |----------------------------------------|
0             Validation | 0.6265 (37, 12, 17, 31, 20) | 0.773 | 0.925 | 88.83 | 88.07 |########################################| ^ (0.1%, 12.25, 6.7, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (688 batches)
0 >>  4/20 <<   Training | 0.6238 (38, 12, 16, 31, 20) | 0.800 | 0.956 | 88.98 | 88.17 |-----------------------------------------|
0             Validation | 0.6248 (37, 12, 16, 31, 21) | 0.816 | 0.975 | 89.24 | 88.22 |##########################################| ^ (0.2%, 11.84, 6.2, 0%) 
Mon May  2 10:27:57 2022
Using weight: weight for classifier: SvB
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 3279028
wDB 148604.957728
wDBn -54.520430
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 179878
wT 6424.617429
wTn -22.434503
nS 770238
nB 3458906
sum_wS 187.901514
sum_wB 155138.616017
nzz =  259880, wzz =   64.5, wzz_SR =   51.7
nzh =  473043, wzh =   84.0, wzh_SR =   69.4
nhh =   37315, whh =   39.4, whh_SR =   34.7
sum_wS_SR 155.770448
sum_wB_SR 57581.386117
sum_wSp_SR 183.921904
sum_wSn_SR 28.151456
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 268.960454
sigScaleZH 412.565374
sigScaleHH 615.095048
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.099299
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset0.log
Setup training/validation tensors
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6625 (39, 11, 16, 27, 17) | 0.733 | 0.881 | 86.32 | 87.28 |--------------------------------|
0             Validation | 0.6629 (39, 11, 16, 27, 17) | 0.740 | 0.888 | 86.71 | 87.38 |#################################| ^ (0.3%, 8.20, 5.3, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1376 batches)
0 >>  2/20 <<   Training | 0.6343 (36, 13, 15, 27, 20) | 0.836 | 1.001 | 88.68 | 88.03 |----------------------------------------|
0             Validation | 0.6349 (35, 13, 15, 27, 20) | 0.841 | 1.004 | 89.01 | 88.11 |#########################################| ^ (0.2%, 9.23, 6.1, 0%) 
0 >>  3/20 <<   Training | 0.6338 (34, 12, 15, 30, 19) | 0.794 | 0.949 | 88.53 | 88.01 |----------------------------------------|
0             Validation | 0.6352 (34, 12, 15, 30, 19) | 0.798 | 0.953 | 88.77 | 88.07 |########################################| ^ (0.2%, 8.90, 4.9, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (688 batches)
0 >>  4/20 <<   Training | 0.6255 (35, 12, 15, 29, 19) | 0.846 | 1.010 | 88.85 | 88.18 |-----------------------------------------|
Mon May  2 11:40:45 2022
Using weight: weight for classifier: SvB
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 3279028
wDB 148604.957728
wDBn -54.520430
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 179878
wT 6424.617429
wTn -22.434503
nS 770238
nB 3458906
sum_wS 187.901514
sum_wB 155138.616017
nzz =  259880, wzz =   64.5, wzz_SR =   51.7
nzh =  473043, wzh =   84.0, wzh_SR =   69.4
nhh =   37315, whh =   39.4, whh_SR =   34.7
sum_wS_SR 155.770448
sum_wB_SR 57581.386117
sum_wSp_SR 183.921904
sum_wSn_SR 28.151456
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 268.960454
sigScaleZH 412.565374
sigScaleHH 615.095048
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.099299
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset0.log
Setup training/validation tensors
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) |   B   | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6625 (39, 11, 16, 27, 17) | 184.409 | 0.881 | 86.32 | 87.28 |--------------------------------|
0             Validation | 0.6629 (39, 11, 16, 27, 17) | 184.465 | 0.888 | 86.71 | 87.38 |#################################| ^ (0.3%, 8.20, 5.3, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1376 batches)
0 >>  2/20 <<   Training | 0.6343 (36, 13, 15, 27, 20) | 181.250 | 1.001 | 88.68 | 88.03 |----------------------------------------|
0             Validation | 0.6349 (35, 13, 15, 27, 20) | 184.711 | 1.004 | 89.01 | 88.11 |#########################################| ^ (0.2%, 9.23, 6.1, 0%) 
0 >>  3/20 <<   Training | 0.6338 (34, 12, 15, 30, 19) | 632.059 | 0.949 | 88.53 | 88.01 |----------------------------------------|
0             Validation | 0.6352 (34, 12, 15, 30, 19) | 628.051 | 0.953 | 88.77 | 88.07 |########################################| ^ (0.2%, 8.90, 4.9, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (688 batches)
Mon May  2 12:56:11 2022
Using weight: weight for classifier: SvB
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 3279028
wDB 148604.957728
wDBn -54.520430
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 179878
wT 6424.617429
wTn -22.434503
nS 770238
nB 3458906
sum_wS 187.901514
sum_wB 155138.616017
nzz =  259880, wzz =   64.5, wzz_SR =   51.7
nzh =  473043, wzh =   84.0, wzh_SR =   69.4
nhh =   37315, whh =   39.4, whh_SR =   34.7
sum_wS_SR 155.770448
sum_wB_SR 57581.386117
sum_wSp_SR 183.921904
sum_wSn_SR 28.151456
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 268.960454
sigScaleZH 412.565374
sigScaleHH 615.095048
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.099299
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset0.log
Setup training/validation tensors
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) |   B   | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6625 (39, 11, 16, 27, 17) |   184 | 0.881 | 86.32 | 87.28 |--------------------------------|
0             Validation | 0.6629 (39, 11, 16, 27, 17) |   184 | 0.888 | 86.71 | 87.38 |#################################| ^ (0.3%, 8.20, 5.3, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1376 batches)
Mon May  2 13:37:23 2022
Using weight: weight for classifier: SvB
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 3279028
wDB 148604.957728
wDBn -54.520430
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 179878
wT 6424.617429
wTn -22.434503
nS 770238
nB 3458906
sum_wS 187.901514
sum_wB 155138.616017
nzz =  259880, wzz =   64.5, wzz_SR =   51.7
nzh =  473043, wzh =   84.0, wzh_SR =   69.4
nhh =   37315, whh =   39.4, whh_SR =   34.7
sum_wS_SR 155.770448
sum_wB_SR 57581.386117
sum_wSp_SR 183.921904
sum_wSn_SR 28.151456
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 268.960454
sigScaleZH 412.565374
sigScaleHH 615.095048
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.099299
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset0.log
Setup training/validation tensors
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) |  S/B  | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6625 (39, 11, 16, 27, 17) | 9/184 | 0.881 | 86.32 | 87.28 |--------------------------------|
0             Validation | 0.6629 (39, 11, 16, 27, 17) | 9/184 | 0.888 | 86.71 | 87.38 |#################################| ^ (0.3%, 8.20, 5.3, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1376 batches)
0 >>  2/20 <<   Training | 0.6343 (36, 13, 15, 27, 20) | 11/181 | 1.001 | 88.68 | 88.03 |----------------------------------------|
0             Validation | 0.6349 (35, 13, 15, 27, 20) | 11/185 | 1.004 | 89.01 | 88.11 |#########################################| ^ (0.2%, 9.23, 6.1, 0%) 
0 >>  3/20 <<   Training | 0.6338 (34, 12, 15, 30, 19) | 5/ 84 | 0.949 | 88.53 | 88.01 |----------------------------------------|
0             Validation | 0.6352 (34, 12, 15, 30, 19) | 5/ 87 | 0.953 | 88.77 | 88.07 |########################################| ^ (0.2%, 8.90, 4.9, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (688 batches)
0 >>  4/20 <<   Training | 0.6255 (35, 12, 15, 29, 19) | 6/ 77 | 1.010 | 88.85 | 88.18 |-----------------------------------------|
0             Validation | 0.6270 (34, 12, 15, 29, 19) | 6/ 75 | 1.015 | 89.12 | 88.23 |##########################################| ^ (0.1%, 8.80, 5.8, 0%) 
0 >>  5/20 <<   Training | 0.6259 (35, 13, 15, 28, 20) | 10/144 | 1.045 | 88.93 | 88.26 |------------------------------------------|
0             Validation | 0.6274 (35, 13, 15, 28, 20) | 10/139 | 1.060 | 89.23 | 88.31 |###########################################| ^ (0.2%, 8.39, 4.9, 0%) 
0 >>  6/20 <<   Training | 0.6237 (32, 12, 16, 31, 19) | 5/ 67 | 1.032 | 88.68 | 88.14 |-----------------------------------------|
0             Validation | 0.6256 (31, 12, 16, 31, 19) | 5/ 63 | 1.044 | 88.94 | 88.19 |#########################################| ^ (0.2%, 8.40, 5.8, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (344 batches)
0 >>  7/20 <<   Training | 0.6205 (32, 12, 17, 30, 20) | 6/ 76 | 1.052 | 88.87 | 88.22 |------------------------------------------|
0             Validation | 0.6226 (32, 12, 17, 30, 20) | 6/ 71 | 1.063 | 89.14 | 88.25 |##########################################| ^ (0.1%, 8.12, 5.6, 0%) 
0 >>  8/20 <<   Training | 0.6219 (35, 12, 16, 28, 20) | 10/151 | 1.058 | 88.94 | 88.32 |-------------------------------------------|
0             Validation | 0.6233 (35, 12, 16, 28, 20) | 10/146 | 1.067 | 89.26 | 88.37 |###########################################| ^ (0.2%, 8.37, 5.2, 0%) 
0 >>  9/20 <<   Training | 0.6232 (33, 12, 15, 30, 20) | 6/ 74 | 1.059 | 89.05 | 88.33 |-------------------------------------------|
0             Validation | 0.6250 (33, 12, 15, 30, 20) | 6/ 71 | 1.073 | 89.31 | 88.38 |###########################################| ^ (0.2%, 8.75, 5.6, 0%) 
0 >> 10/20 <<   Training | 0.6201 (33, 12, 16, 29, 19) | 7/ 85 | 1.065 | 88.99 | 88.35 |-------------------------------------------|
0             Validation | 0.6220 (32, 13, 16, 29, 20) | 7/ 79 | 1.082 | 89.25 | 88.40 |############################################| ^ (0.2%, 8.64, 6.6, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (172 batches)
0 >> 11/20 <<   Training | 0.6206 (34, 11, 16, 29, 20) | 6/ 79 | 1.068 | 89.03 | 88.34 |-------------------------------------------|
0             Validation | 0.6225 (34, 12, 16, 29, 20) | 6/ 77 | 1.074 | 89.32 | 88.39 |###########################################| ^ (0.2%, 8.19, 6.0, 0%) 
0 >> 12/20 <<   Training | 0.6235 (34, 12, 15, 29, 21) | 8/ 99 | 1.075 | 89.13 | 88.40 |--------------------------------------------|
0             Validation | 0.6254 (33, 12, 15, 29, 21) | 8/ 96 | 1.081 | 89.43 | 88.44 |############################################| ^ (0.2%, 8.74, 5.7, 0%) 
0 >> 13/20 <<   Training | 0.6195 (34, 12, 16, 28, 19) | 11/185 | 1.034 | 89.03 | 88.30 |-------------------------------------------|
0             Validation | 0.6212 (34, 13, 16, 28, 19) | 11/179 | 1.043 | 89.31 | 88.36 |###########################################| ^ (0.2%, 8.57, 5.0, 0%) 
0 >> 14/20 <<   Training | 0.6222 (34, 13, 15, 29, 19) | 8/112 | 1.058 | 89.03 | 88.33 |-------------------------------------------|
0             Validation | 0.6240 (33, 13, 15, 30, 19) | 8/111 | 1.060 | 89.31 | 88.37 |###########################################| ^ (0.2%, 8.71, 5.7, 0%) 
0 >> 15/20 <<   Training | 0.6192 (32, 12, 16, 30, 20) | 8/100 | 1.075 | 88.96 | 88.36 |-------------------------------------------|
0             Validation | 0.6215 (32, 12, 16, 30, 20) | 7/ 94 | 1.087 | 89.25 | 88.40 |###########################################| ^ (0.1%, 8.65, 6.3, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6187 (34, 12, 16, 28, 20) | 8/111 | 1.077 | 89.10 | 88.41 |--------------------------------------------|
0             Validation | 0.6205 (34, 12, 16, 29, 20) | 8/106 | 1.086 | 89.38 | 88.46 |############################################| ^ (0.2%, 8.47, 5.8, 0%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6186 (34, 12, 16, 29, 20) | 9/116 | 1.083 | 89.08 | 88.42 |--------------------------------------------|
0             Validation | 0.6204 (34, 12, 16, 29, 20) | 9/114 | 1.088 | 89.38 | 88.47 |############################################| ^ (0.2%, 8.45, 5.8, 0%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6192 (34, 12, 16, 29, 19) | 9/121 | 1.079 | 89.11 | 88.41 |--------------------------------------------|
0             Validation | 0.6210 (34, 12, 15, 29, 19) | 9/122 | 1.078 | 89.40 | 88.46 |############################################| ^ (0.2%, 8.47, 5.8, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6190 (34, 12, 16, 28, 19) | 9/121 | 1.080 | 89.10 | 88.42 |--------------------------------------------|
0             Validation | 0.6208 (34, 12, 16, 29, 19) | 9/121 | 1.081 | 89.39 | 88.47 |############################################| ^ (0.2%, 8.46, 5.9, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6190 (34, 12, 16, 28, 19) | 9/120 | 1.079 | 89.10 | 88.42 |--------------------------------------------|
0             Validation | 0.6208 (34, 12, 16, 29, 20) | 9/119 | 1.083 | 89.40 | 88.47 |############################################| ^ (0.2%, 8.46, 5.9, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

0 >> DONE <<
0 Minimum Loss = 0.618554
Mon May  9 11:00:02 2022
Using weight: weight for classifier: SvB
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 3279028
wDB 147993.557827
wDBn -59.208815
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 179878
wT 6758.331864
wTn -23.581879
nS 940327
nB 3458906
sum_wS 167.472288
sum_wB 154870.307320
nzz =  323399, wzz =   57.4, wzz_SR =   46.2
nzh =  574169, wzh =   74.7, wzh_SR =   61.9
nhh =   42759, whh =   35.4, whh_SR =   31.2
sum_wS_SR 139.340355
sum_wB_SR 57305.889757
sum_wSp_SR 164.693187
sum_wSn_SR 25.352832
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 295.764663
sigScaleZH 457.493368
sigScaleHH 679.928873
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.099681
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset0.log
Setup training/validation tensors
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) |  S/B   | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6582 (38, 12, 17, 27, 16) |  6/120 | 0.829 | 86.01 | 87.53 |-----------------------------------|
0             Validation | 0.6580 (38, 12, 17, 27, 16) |  6/122 | 0.848 | 86.34 | 87.60 |####################################| ^ (0.2%, 9.59, 17.0, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1431 batches)
0 >>  2/20 <<   Training | 0.6285 (35, 12, 16, 28, 19) |  6/ 95 | 0.928 | 88.40 | 88.17 |-----------------------------------------|
0             Validation | 0.6283 (35, 12, 16, 28, 19) |  6/ 96 | 0.937 | 88.74 | 88.23 |##########################################| ^ (0.2%, 8.38, 15.8, 0%) 
0 >>  3/20 <<   Training | 0.6255 (35, 13, 16, 28, 18) | 10/237 | 0.876 | 88.71 | 88.27 |------------------------------------------|
0             Validation | 0.6254 (35, 13, 15, 28, 18) | 10/225 | 0.900 | 89.04 | 88.33 |###########################################| ^ (0.2%, 8.88, 16.3, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (715 batches)
0 >>  4/20 <<   Training | 0.6217 (33, 13, 16, 30, 20) |  7/109 | 0.957 | 88.80 | 88.44 |--------------------------------------------|
0             Validation | 0.6221 (33, 13, 15, 30, 20) |  7/107 | 0.976 | 89.14 | 88.47 |############################################| ^ (0.2%, 8.19, 16.7, 0%) 
0 >>  5/20 <<   Training | 0.6228 (33, 13, 16, 28, 20) |  9/130 | 0.979 | 88.95 | 88.48 |--------------------------------------------|
0             Validation | 0.6236 (33, 13, 15, 28, 20) |  9/126 | 1.001 | 89.24 | 88.51 |#############################################| ^ (0.2%, 8.48, 14.3, 0%) 
0 >>  6/20 <<   Training | 0.6259 (37, 13, 14, 28, 19) |  8/134 | 0.939 | 88.88 | 88.46 |--------------------------------------------|
0             Validation | 0.6258 (36, 13, 14, 28, 19) |  8/133 | 0.958 | 89.16 | 88.53 |#############################################| ^ (0.2%, 8.29, 14.1, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (357 batches)
0 >>  7/20 <<   Training | 0.6230 (38, 13, 15, 26, 18) | 12/230 | 0.919 | 88.96 | 88.48 |--------------------------------------------|
0             Validation | 0.6232 (37, 13, 15, 26, 18) | 11/222 | 0.935 | 89.32 | 88.51 |#############################################| ^ (0.1%, 8.38, 13.4, 0%) 
0 >>  8/20 <<   Training | 0.6177 (33, 13, 16, 28, 20) |  8/118 | 0.973 | 88.81 | 88.47 |--------------------------------------------|
0             Validation | 0.6186 (33, 13, 16, 28, 20) |  8/122 | 0.984 | 89.17 | 88.49 |############################################| ^ (0.1%, 8.29, 13.3, 0%) 
0 >>  9/20 <<   Training | 0.6181 (35, 13, 16, 28, 19) |  8/135 | 0.958 | 88.82 | 88.57 |---------------------------------------------|
0             Validation | 0.6185 (35, 13, 16, 28, 19) |  8/129 | 0.977 | 89.16 | 88.60 |##############################################| ^ (0.1%, 8.49, 13.0, 0%) 
0 >> 10/20 <<   Training | 0.6177 (33, 13, 16, 29, 20) |  8/118 | 0.990 | 88.93 | 88.54 |---------------------------------------------|
0             Validation | 0.6186 (33, 13, 16, 29, 20) |  8/114 | 1.014 | 89.31 | 88.57 |#############################################| ^ (0.1%, 8.72, 13.3, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (178 batches)
0 >> 11/20 <<   Training | 0.6194 (35, 13, 15, 29, 19) | 10/181 | 0.960 | 89.06 | 88.59 |---------------------------------------------|
0             Validation | 0.6196 (35, 13, 15, 29, 19) | 10/173 | 0.982 | 89.43 | 88.63 |##############################################| ^ (0.2%, 8.22, 13.5, 0%) 
0 >> 12/20 <<   Training | 0.6191 (35, 12, 15, 28, 20) |  9/158 | 0.969 | 89.15 | 88.63 |----------------------------------------------|
0             Validation | 0.6196 (35, 13, 15, 28, 20) |  9/152 | 0.992 | 89.52 | 88.67 |##############################################| ^ (0.2%, 8.29, 14.1, 0%) 
0 >> 13/20 <<   Training | 0.6187 (33, 13, 15, 29, 19) |  7/ 91 | 0.994 | 89.12 | 88.62 |----------------------------------------------|
0             Validation | 0.6190 (33, 13, 15, 29, 19) |  7/ 85 | 1.021 | 89.48 | 88.67 |##############################################| ^ (0.2%, 8.52, 14.3, 0%) 
0 >> 14/20 <<   Training | 0.6184 (36, 12, 15, 27, 19) | 12/218 | 0.951 | 89.09 | 88.58 |---------------------------------------------|
0             Validation | 0.6187 (35, 13, 15, 27, 19) | 12/201 | 0.984 | 89.45 | 88.63 |##############################################| ^ (0.2%, 8.74, 14.1, 0%) 
0 >> 15/20 <<   Training | 0.6166 (34, 13, 16, 28, 20) |  7/103 | 0.995 | 89.04 | 88.62 |----------------------------------------------|
0             Validation | 0.6170 (33, 13, 16, 28, 20) |  7/ 98 | 1.018 | 89.39 | 88.67 |##############################################| ^ (0.2%, 8.42, 13.5, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6158 (34, 13, 16, 28, 19) | 10/162 | 0.975 | 89.08 | 88.64 |----------------------------------------------|
0             Validation | 0.6163 (34, 13, 15, 28, 19) | 10/157 | 0.998 | 89.43 | 88.67 |##############################################| ^ (0.1%, 8.47, 13.8, 0%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6160 (34, 13, 15, 28, 19) |  9/151 | 0.980 | 89.13 | 88.66 |----------------------------------------------|
0             Validation | 0.6165 (34, 13, 15, 28, 19) |  9/148 | 1.000 | 89.48 | 88.69 |##############################################| ^ (0.2%, 8.45, 13.3, 0%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6160 (34, 13, 15, 28, 19) |  9/146 | 0.985 | 89.13 | 88.67 |----------------------------------------------|
0             Validation | 0.6165 (34, 13, 15, 28, 20) |  9/141 | 1.006 | 89.48 | 88.71 |###############################################| ^ (0.2%, 8.45, 13.4, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6158 (34, 13, 15, 28, 19) |  9/144 | 0.986 | 89.12 | 88.67 |----------------------------------------------|
0             Validation | 0.6164 (34, 13, 15, 28, 19) |  9/140 | 1.005 | 89.47 | 88.70 |###############################################| ^ (0.2%, 8.46, 13.2, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6158 (34, 13, 15, 28, 19) |  9/142 | 0.986 | 89.12 | 88.67 |----------------------------------------------|
0             Validation | 0.6164 (34, 13, 15, 28, 19) |  9/139 | 1.005 | 89.47 | 88.70 |###############################################| ^ (0.2%, 8.46, 13.2, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

0 >> DONE <<
0 Minimum Loss = 0.615811
