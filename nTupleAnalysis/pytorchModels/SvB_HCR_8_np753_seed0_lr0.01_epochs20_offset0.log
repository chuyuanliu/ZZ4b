0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6637 (40, 11, 18, 32, 18) | 0.735 | 0.879 | 87.95 | 85.31 |-------------|
0             Validation | 0.6648 (40, 11, 18, 32, 19) | 0.745 | 0.887 | 88.10 | 85.42 |##############| ^ (0.4%, 10.81, 1.3, 15%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1032 batches)
0 >>  2/20 <<   Training | 0.6501 (38, 11, 19, 32, 20) | 0.826 | 0.973 | 87.87 | 86.10 |---------------------|
0             Validation | 0.6516 (38, 11, 19, 32, 20) | 0.856 | 1.004 | 88.11 | 86.20 |#####################| ^ (0.3%, 10.83, 0.8, 64%) 
0 >>  3/20 <<   Training | 0.6516 (42, 11, 19, 29, 18) | 0.813 | 0.959 | 87.89 | 86.10 |---------------------|
0             Validation | 0.6531 (42, 11, 19, 29, 18) | 0.827 | 0.972 | 88.03 | 86.19 |#####################| ^ (0.3%, 10.84, 1.3, 13%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (516 batches)
0 >>  4/20 <<   Training | 0.6433 (39, 11, 18, 31, 21) | 0.901 | 1.062 | 88.54 | 86.34 |-----------------------|
0             Validation | 0.6446 (39, 11, 18, 31, 21) | 0.936 | 1.098 | 88.74 | 86.43 |########################| ^ (0.3%, 9.77, 1.0, 42%) 
0 >>  5/20 <<   Training | 0.6456 (38, 10, 17, 33, 21) | 0.883 | 1.037 | 88.60 | 86.45 |------------------------|
0             Validation | 0.6470 (38, 10, 17, 33, 21) | 0.924 | 1.083 | 88.72 | 86.56 |#########################| ^ (0.4%, 10.39, 0.7, 74%) 
0 >>  6/20 <<   Training | 0.6444 (37, 11, 18, 32, 22) | 0.916 | 1.076 | 88.62 | 86.51 |-------------------------|
0             Validation | 0.6454 (37, 11, 18, 32, 22) | 0.960 | 1.121 | 88.75 | 86.62 |##########################| ^ (0.3%, 10.61, 0.7, 76%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (258 batches)
0 >>  7/20 <<   Training | 0.6390 (38, 11, 18, 32, 21) | 0.913 | 1.074 | 88.56 | 86.54 |-------------------------|
0             Validation | 0.6406 (38, 11, 18, 31, 21) | 0.936 | 1.096 | 88.69 | 86.61 |##########################| ^ (0.3%, 9.82, 0.9, 58%) 
0 >>  8/20 <<   Training | 0.6399 (40, 11, 18, 30, 19) | 0.870 | 1.028 | 88.58 | 86.49 |------------------------|
0             Validation | 0.6419 (40, 11, 18, 30, 19) | 0.893 | 1.050 | 88.78 | 86.56 |#########################| ^ (0.3%, 10.01, 1.1, 30%) 
0 >>  9/20 <<   Training | 0.6390 (37, 11, 18, 32, 20) | 0.914 | 1.075 | 88.70 | 86.50 |-------------------------|
0             Validation | 0.6405 (37, 11, 18, 32, 21) | 0.946 | 1.109 | 88.88 | 86.59 |#########################| ^ (0.4%, 9.75, 0.9, 53%) 
0 >> 10/20 <<   Training | 0.6421 (37, 11, 17, 34, 21) | 0.929 | 1.091 | 88.69 | 86.56 |-------------------------|
0             Validation | 0.6442 (37, 11, 17, 33, 21) | 0.962 | 1.126 | 88.76 | 86.63 |##########################| ^ (0.3%, 10.06, 1.5, 5%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (129 batches)
0 >> 11/20 <<   Training | 0.6393 (39, 11, 17, 31, 21) | 0.927 | 1.089 | 88.95 | 86.66 |--------------------------|
0             Validation | 0.6409 (39, 11, 17, 31, 21) | 0.956 | 1.120 | 89.10 | 86.74 |###########################| ^ (0.3%, 10.11, 0.7, 79%) 
0 >> 12/20 <<   Training | 0.6394 (40, 10, 18, 32, 20) | 0.925 | 1.086 | 88.90 | 86.59 |-------------------------|
0             Validation | 0.6411 (40, 10, 18, 32, 20) | 0.957 | 1.121 | 89.05 | 86.67 |##########################| ^ (0.3%, 10.05, 1.1, 32%) 
0 >> 13/20 <<   Training | 0.6408 (38, 11, 17, 32, 22) | 0.924 | 1.087 | 88.88 | 86.65 |--------------------------|
0             Validation | 0.6423 (38, 11, 17, 32, 22) | 0.966 | 1.131 | 88.98 | 86.74 |###########################| ^ (0.4%, 10.13, 0.8, 68%) 
0 >> 14/20 <<   Training | 0.6420 (41, 10, 17, 31, 19) | 0.907 | 1.065 | 88.92 | 86.63 |--------------------------|
0             Validation | 0.6438 (41, 10, 17, 31, 19) | 0.944 | 1.106 | 89.03 | 86.69 |##########################| ^ (0.3%, 9.81, 1.8, 1%) 
0 >> 15/20 <<   Training | 0.6366 (39, 11, 18, 31, 20) | 0.905 | 1.065 | 88.83 | 86.58 |-------------------------|
0             Validation | 0.6386 (38, 11, 18, 31, 20) | 0.929 | 1.089 | 88.99 | 86.65 |##########################| ^ (0.3%, 10.13, 1.5, 6%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6356 (38, 11, 18, 32, 20) | 0.935 | 1.098 | 88.95 | 86.70 |--------------------------|
0             Validation | 0.6374 (38, 11, 18, 32, 21) | 0.963 | 1.127 | 89.08 | 86.76 |###########################| ^ (0.3%, 10.26, 1.0, 43%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6354 (38, 11, 18, 32, 20) | 0.936 | 1.099 | 88.94 | 86.71 |---------------------------|
0             Validation | 0.6372 (38, 11, 18, 32, 20) | 0.962 | 1.126 | 89.06 | 86.77 |###########################| ^ (0.3%, 10.18, 1.3, 14%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6361 (38, 11, 18, 32, 21) | 0.935 | 1.098 | 88.97 | 86.73 |---------------------------|
0             Validation | 0.6378 (38, 11, 18, 32, 21) | 0.964 | 1.129 | 89.09 | 86.79 |###########################| ^ (0.3%, 10.17, 1.2, 23%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6355 (38, 11, 18, 32, 21) | 0.937 | 1.100 | 88.96 | 86.72 |---------------------------|
0             Validation | 0.6373 (38, 11, 18, 32, 21) | 0.964 | 1.129 | 89.08 | 86.78 |###########################| ^ (0.3%, 10.18, 1.4, 10%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6356 (38, 11, 18, 32, 20) | 0.934 | 1.097 | 88.96 | 86.72 |---------------------------|
0             Validation | 0.6374 (38, 11, 18, 32, 21) | 0.964 | 1.129 | 89.08 | 86.78 |###########################| ^ (0.3%, 10.18, 1.5, 6%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7061 (28, 12, 11, 19, 10) | 0.868 | 1.023 | 87.02 | 85.63 |----------------|
0             Validation | 0.7064 (28, 12, 11, 19, 10) | 0.880 | 1.032 | 87.10 | 85.75 |#################| ^ (0.4%, 4.21, 1.6, 3%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1032 batches)
0 >>  2/20 <<   Training | 0.6909 (26, 13, 11, 19, 11) | 0.919 | 1.083 | 87.37 | 86.12 |---------------------|
0             Validation | 0.6916 (26, 13, 11, 19, 11) | 0.945 | 1.109 | 87.56 | 86.18 |#####################| ^ (0.3%, 4.33, 1.6, 4%) 
0 >>  3/20 <<   Training | 0.6919 (27, 13, 11, 18, 11) | 0.939 | 1.105 | 87.25 | 86.19 |---------------------|
0             Validation | 0.6926 (27, 13, 11, 18, 11) | 0.968 | 1.134 | 87.25 | 86.26 |######################| ^ (0.4%, 4.18, 1.3, 11%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (516 batches)
0 >>  4/20 <<   Training | 0.6823 (24, 13, 11, 19, 13) | 1.001 | 1.177 | 87.87 | 86.55 |-------------------------|
0             Validation | 0.6828 (24, 13, 11, 19, 13) | 1.036 | 1.213 | 87.97 | 86.62 |##########################| ^ (0.3%, 4.07, 3.0, 0%) 
0 >>  5/20 <<   Training | 0.6815 (23, 12, 12, 20, 13) | 1.003 | 1.178 | 87.86 | 86.51 |-------------------------|
0             Validation | 0.6821 (23, 12, 12, 20, 13) | 1.043 | 1.221 | 87.97 | 86.60 |#########################| ^ (0.4%, 4.47, 1.7, 2%) 
0 >>  6/20 <<   Training | 0.6873 (22, 13, 10, 20, 14) | 0.985 | 1.157 | 87.95 | 86.55 |-------------------------|
0             Validation | 0.6876 (22, 13, 10, 20, 14) | 1.010 | 1.182 | 88.09 | 86.63 |##########################| ^ (0.4%, 4.38, 1.7, 2%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (258 batches)
0 >>  7/20 <<   Training | 0.6812 (25, 13, 11, 19, 12) | 0.982 | 1.155 | 88.11 | 86.53 |-------------------------|
0             Validation | 0.6822 (25, 13, 11, 19, 12) | 1.010 | 1.182 | 88.28 | 86.58 |#########################| ^ (0.3%, 4.07, 2.1, 0%) 
0 >>  8/20 <<   Training | 0.6836 (27, 13, 11, 18, 11) | 0.997 | 1.172 | 88.00 | 86.61 |--------------------------|
0             Validation | 0.6847 (27, 13, 11, 18, 11) | 1.027 | 1.202 | 88.14 | 86.66 |##########################| ^ (0.3%, 4.30, 2.5, 0%) 
0 >>  9/20 <<   Training | 0.6805 (23, 13, 11, 20, 13) | 1.015 | 1.192 | 87.96 | 86.59 |-------------------------|
0             Validation | 0.6816 (23, 13, 11, 20, 13) | 1.052 | 1.232 | 88.16 | 86.66 |##########################| ^ (0.3%, 4.19, 1.7, 2%) 
0 >> 10/20 <<   Training | 0.6840 (26, 13, 10, 19, 12) | 1.023 | 1.202 | 88.18 | 86.60 |-------------------------|
0             Validation | 0.6854 (26, 13, 10, 19, 12) | 1.054 | 1.233 | 88.27 | 86.63 |##########################| ^ (0.3%, 4.32, 2.0, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (129 batches)
0 >> 11/20 <<   Training | 0.6825 (25, 13, 10, 19, 13) | 1.032 | 1.212 | 88.33 | 86.77 |---------------------------|
0             Validation | 0.6835 (25, 13, 10, 19, 13) | 1.064 | 1.245 | 88.47 | 86.82 |############################| ^ (0.3%, 4.17, 1.9, 1%) 
0 >> 12/20 <<   Training | 0.6785 (23, 12, 11, 20, 13) | 1.032 | 1.212 | 88.29 | 86.72 |---------------------------|
0             Validation | 0.6794 (23, 12, 11, 20, 13) | 1.062 | 1.243 | 88.40 | 86.78 |###########################| ^ (0.3%, 4.33, 1.4, 10%) 
0 >> 13/20 <<   Training | 0.6825 (25, 13, 10, 18, 13) | 1.029 | 1.208 | 88.28 | 86.75 |---------------------------|
0             Validation | 0.6834 (25, 13, 10, 18, 13) | 1.068 | 1.250 | 88.41 | 86.82 |############################| ^ (0.3%, 4.34, 1.7, 2%) 
0 >> 14/20 <<   Training | 0.6832 (27, 12, 10, 19, 12) | 1.026 | 1.204 | 88.31 | 86.69 |--------------------------|
0             Validation | 0.6843 (27, 12, 10, 19, 12) | 1.062 | 1.242 | 88.47 | 86.73 |###########################| ^ (0.3%, 4.20, 2.1, 0%) 
0 >> 15/20 <<   Training | 0.6793 (26, 13, 11, 19, 12) | 1.019 | 1.197 | 88.21 | 86.68 |--------------------------|
0             Validation | 0.6805 (26, 13, 11, 19, 12) | 1.045 | 1.223 | 88.39 | 86.73 |###########################| ^ (0.3%, 4.17, 1.5, 6%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6772 (24, 13, 11, 20, 12) | 1.033 | 1.212 | 88.28 | 86.73 |---------------------------|
0             Validation | 0.6785 (24, 13, 11, 20, 12) | 1.062 | 1.242 | 88.43 | 86.78 |###########################| ^ (0.3%, 4.26, 1.4, 10%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6769 (24, 13, 11, 20, 12) | 1.030 | 1.209 | 88.29 | 86.77 |---------------------------|
0             Validation | 0.6781 (24, 13, 11, 20, 12) | 1.061 | 1.241 | 88.42 | 86.81 |############################| ^ (0.3%, 4.29, 1.6, 3%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6775 (25, 13, 11, 19, 12) | 1.033 | 1.213 | 88.32 | 86.78 |---------------------------|
0             Validation | 0.6787 (25, 13, 11, 19, 12) | 1.064 | 1.244 | 88.45 | 86.82 |############################| ^ (0.3%, 4.29, 1.6, 3%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6771 (24, 13, 11, 20, 12) | 1.032 | 1.212 | 88.32 | 86.77 |---------------------------|
0             Validation | 0.6783 (24, 13, 11, 20, 12) | 1.063 | 1.243 | 88.45 | 86.82 |############################| ^ (0.3%, 4.29, 1.6, 3%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6772 (24, 13, 11, 20, 12) | 1.032 | 1.211 | 88.32 | 86.77 |---------------------------|
0             Validation | 0.6784 (24, 13, 11, 20, 12) | 1.064 | 1.245 | 88.45 | 86.82 |############################| ^ (0.3%, 4.29, 1.4, 7%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
Thu Apr 14 10:45:18 2022
Using weight: weight for classifier: SvB
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 2396428
wDB 113735.127030
wDBn -29.981275
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 128876
wT 4694.203751
wTn -16.779144
Not obvious how to handle negative ttbar weights, for now remove them
nS 645566
nB 2524908
sum_wS 201.702339
sum_wB 118506.072474
nzz =   68867, wzz =   71.9, wzz_SR =   59.2
nzh =  541264, wzh =   92.5, wzh_SR =   77.9
nhh =   35435, whh =   37.3, whh_SR =   33.4
sum_wS_SR 170.521446
sum_wB_SR 50064.653301
sum_wSp_SR 203.140887
sum_wSn_SR 32.619440
Negative weight signal events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 263.065004
sigScaleZH 390.354672
sigScaleHH 609.145944
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.192666
Setup training/validation tensors
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
Thu Apr 14 10:59:33 2022
Using weight: weight for classifier: SvB
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 2396428
wDB 113735.127030
wDBn -29.981275
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 128876
wT 4694.203751
wTn -16.779144
nS 645566
nB 2525304
sum_wS 201.702339
sum_wB 118489.293330
nzz =   68867, wzz =   71.9, wzz_SR =   59.2
nzh =  541264, wzh =   92.5, wzh_SR =   77.9
nhh =   35435, whh =   37.3, whh_SR =   33.4
sum_wS_SR 170.521446
sum_wB_SR 50056.193175
sum_wSp_SR 203.140887
sum_wSn_SR 32.619440
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 263.031478
sigScaleZH 390.285279
sigScaleHH 609.010109
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.192394
Setup training/validation tensors
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6616 (38, 10, 18, 33, 20) | 0.867 | 1.021 | 88.23 | 86.01 |--------------------|
0             Validation | 0.6615 (38, 10, 18, 33, 20) | 0.889 | 1.043 | 88.27 | 86.10 |####################| ^ (0.3%, 12.09, 0.9, 56%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1032 batches)
0 >>  2/20 <<   Training | 0.6508 (41, 10, 18, 30, 20) | 0.878 | 1.032 | 88.20 | 86.21 |----------------------|
0             Validation | 0.6514 (41, 10, 18, 30, 20) | 0.905 | 1.062 | 88.39 | 86.29 |######################| ^ (0.3%, 10.02, 1.0, 42%) 
0 >>  3/20 <<   Training | 0.6473 (39, 11, 17, 32, 21) | 0.907 | 1.064 | 88.83 | 86.51 |-------------------------|
0             Validation | 0.6483 (39, 11, 17, 31, 21) | 0.933 | 1.094 | 88.81 | 86.58 |#########################| ^ (0.3%, 11.45, 1.2, 16%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (516 batches)
0 >>  4/20 <<   Training | 0.6436 (40, 11, 18, 32, 18) | 0.826 | 0.971 | 88.79 | 86.39 |-----------------------|
0             Validation | 0.6446 (40, 11, 18, 32, 18) | 0.832 | 0.975 | 88.86 | 86.44 |########################| ^ (0.4%, 11.40, 1.1, 29%) 
0 >>  5/20 <<   Training | 0.6432 (41, 11, 17, 30, 20) | 0.914 | 1.073 | 89.05 | 86.64 |--------------------------|
0             Validation | 0.6444 (41, 11, 17, 30, 20) | 0.925 | 1.084 | 89.15 | 86.68 |##########################| ^ (0.4%, 10.88, 1.4, 8%) 
0 >>  6/20 <<   Training | 0.6468 (42, 11, 16, 29, 20) | 0.915 | 1.076 | 89.11 | 86.58 |-------------------------|
0             Validation | 0.6484 (42, 11, 16, 29, 21) | 0.932 | 1.093 | 89.19 | 86.61 |##########################| ^ (0.4%, 10.53, 1.1, 29%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (258 batches)
0 >>  7/20 <<   Training | 0.6361 (38, 11, 18, 31, 21) | 0.941 | 1.104 | 89.09 | 86.68 |--------------------------|
0             Validation | 0.6375 (38, 11, 18, 31, 21) | 0.960 | 1.123 | 89.21 | 86.71 |###########################| ^ (0.4%, 11.34, 1.3, 12%) 
Thu Apr 14 13:18:27 2022
Using weight: weight for classifier: SvB
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 2396428
wDB 113735.127030
wDBn -29.981275
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 128876
wT 4694.203751
wTn -16.779144
nS 645566
nB 2525304
sum_wS 201.702339
sum_wB 118489.293330
nzz =   68867, wzz =   71.9, wzz_SR =   59.2
nzh =  541264, wzh =   92.5, wzh_SR =   77.9
nhh =   35435, whh =   37.3, whh_SR =   33.4
sum_wS_SR 170.521446
sum_wB_SR 50056.193175
sum_wSp_SR 203.140887
sum_wSn_SR 32.619440
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 263.031478
sigScaleZH 390.285279
sigScaleHH 609.010109
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.192394
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset0.log
Setup training/validation tensors
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6616 (38, 10, 18, 33, 20) | 0.867 | 1.021 | 88.23 | 86.01 |--------------------|
0             Validation | 0.6615 (38, 10, 18, 33, 20) | 0.889 | 1.043 | 88.27 | 86.10 |####################| ^ (0.3%, 12.09, 0.9, 56%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1032 batches)
0 >>  2/20 <<   Training | 0.6508 (41, 10, 18, 30, 20) | 0.878 | 1.032 | 88.20 | 86.21 |----------------------|
0             Validation | 0.6514 (41, 10, 18, 30, 20) | 0.905 | 1.062 | 88.39 | 86.29 |######################| ^ (0.3%, 10.02, 1.0, 42%) 
0 >>  3/20 <<   Training | 0.6473 (39, 11, 17, 32, 21) | 0.907 | 1.064 | 88.83 | 86.51 |-------------------------|
0             Validation | 0.6483 (39, 11, 17, 31, 21) | 0.933 | 1.094 | 88.81 | 86.58 |#########################| ^ (0.3%, 11.45, 1.2, 16%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (516 batches)
0 >>  4/20 <<   Training | 0.6436 (40, 11, 18, 32, 18) | 0.826 | 0.971 | 88.79 | 86.39 |-----------------------|
0             Validation | 0.6446 (40, 11, 18, 32, 18) | 0.832 | 0.975 | 88.86 | 86.44 |########################| ^ (0.4%, 11.40, 1.1, 29%) 
0 >>  5/20 <<   Training | 0.6432 (41, 11, 17, 30, 20) | 0.914 | 1.073 | 89.05 | 86.64 |--------------------------|
0             Validation | 0.6444 (41, 11, 17, 30, 20) | 0.925 | 1.084 | 89.15 | 86.68 |##########################| ^ (0.4%, 10.88, 1.4, 8%) 
0 >>  6/20 <<   Training | 0.6468 (42, 11, 16, 29, 20) | 0.915 | 1.076 | 89.11 | 86.58 |-------------------------|
0             Validation | 0.6484 (42, 11, 16, 29, 21) | 0.932 | 1.093 | 89.19 | 86.61 |##########################| ^ (0.4%, 10.53, 1.1, 29%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (258 batches)
0 >>  7/20 <<   Training | 0.6361 (38, 11, 18, 31, 21) | 0.941 | 1.104 | 89.09 | 86.68 |--------------------------|
0             Validation | 0.6375 (38, 11, 18, 31, 21) | 0.960 | 1.123 | 89.21 | 86.71 |###########################| ^ (0.4%, 11.34, 1.3, 12%) 
0 >>  8/20 <<   Training | 0.6359 (37, 11, 18, 32, 21) | 0.942 | 1.105 | 89.00 | 86.70 |---------------------------|
0             Validation | 0.6374 (37, 11, 18, 32, 21) | 0.965 | 1.130 | 89.17 | 86.72 |###########################| ^ (0.4%, 11.23, 1.1, 33%) 
0 >>  9/20 <<   Training | 0.6368 (39, 11, 18, 32, 20) | 0.947 | 1.111 | 89.07 | 86.71 |---------------------------|
0             Validation | 0.6383 (39, 11, 18, 32, 20) | 0.971 | 1.136 | 89.20 | 86.73 |###########################| ^ (0.4%, 11.92, 1.7, 2%) 
0 >> 10/20 <<   Training | 0.6373 (38, 11, 18, 32, 21) | 0.942 | 1.106 | 89.11 | 86.63 |--------------------------|
0             Validation | 0.6387 (38, 10, 18, 32, 21) | 0.963 | 1.128 | 89.19 | 86.66 |##########################| ^ (0.4%, 11.14, 1.8, 1%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (129 batches)
0 >> 11/20 <<   Training | 0.6392 (41, 11, 17, 31, 20) | 0.935 | 1.098 | 89.16 | 86.74 |---------------------------|
0             Validation | 0.6406 (41, 11, 17, 31, 20) | 0.953 | 1.118 | 89.35 | 86.77 |###########################| ^ (0.3%, 10.71, 1.5, 5%) 
0 >> 12/20 <<   Training | 0.6371 (38, 11, 17, 32, 21) | 0.942 | 1.106 | 89.23 | 86.64 |--------------------------|
0             Validation | 0.6384 (38, 11, 17, 32, 21) | 0.961 | 1.126 | 89.37 | 86.67 |##########################| ^ (0.4%, 11.20, 1.5, 5%) 
0 >> 13/20 <<   Training | 0.6374 (39, 11, 17, 33, 20) | 0.935 | 1.097 | 89.20 | 86.76 |---------------------------|
0             Validation | 0.6385 (39, 11, 17, 32, 20) | 0.964 | 1.129 | 89.37 | 86.79 |###########################| ^ (0.3%, 11.55, 1.1, 30%) 
0 >> 14/20 <<   Training | 0.6355 (35, 11, 18, 34, 21) | 0.972 | 1.140 | 89.20 | 86.76 |---------------------------|
0             Validation | 0.6367 (35, 11, 18, 34, 21) | 0.978 | 1.145 | 89.37 | 86.78 |###########################| ^ (0.4%, 11.34, 1.2, 19%) 
0 >> 15/20 <<   Training | 0.6387 (40, 11, 17, 31, 21) | 0.933 | 1.095 | 89.33 | 86.70 |---------------------------|
0             Validation | 0.6397 (40, 11, 17, 31, 21) | 0.965 | 1.130 | 89.50 | 86.74 |###########################| ^ (0.4%, 10.81, 2.6, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6334 (38, 11, 18, 33, 20) | 0.948 | 1.113 | 89.22 | 86.78 |---------------------------|
0             Validation | 0.6346 (38, 11, 18, 33, 20) | 0.970 | 1.136 | 89.40 | 86.81 |############################| ^ (0.4%, 11.55, 2.2, 0%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6328 (37, 11, 18, 32, 21) | 0.959 | 1.126 | 89.27 | 86.79 |---------------------------|
0             Validation | 0.6340 (37, 11, 18, 32, 21) | 0.982 | 1.149 | 89.43 | 86.81 |############################| ^ (0.4%, 11.71, 2.4, 0%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6329 (38, 11, 18, 32, 20) | 0.956 | 1.122 | 89.26 | 86.80 |----------------------------|
0             Validation | 0.6341 (38, 11, 18, 32, 21) | 0.978 | 1.145 | 89.43 | 86.83 |############################| ^ (0.4%, 11.69, 2.2, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6332 (38, 11, 18, 32, 20) | 0.958 | 1.124 | 89.27 | 86.81 |----------------------------|
0             Validation | 0.6344 (38, 11, 18, 32, 21) | 0.980 | 1.146 | 89.44 | 86.83 |############################| ^ (0.4%, 11.68, 2.2, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6332 (38, 11, 18, 32, 20) | 0.957 | 1.123 | 89.27 | 86.81 |----------------------------|
0             Validation | 0.6344 (38, 11, 18, 32, 21) | 0.978 | 1.145 | 89.44 | 86.84 |############################| ^ (0.4%, 11.68, 2.2, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

0 >> DONE <<
Mon Apr 18 10:04:41 2022
Using weight: weight for classifier: SvB
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 2396428
wDB 114777.240743
wDBn -53.981695
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 128876
wT 4694.203751
wTn -16.779144
nS 645566
nB 2525304
sum_wS 201.702339
sum_wB 119579.407884
nzz =   68867, wzz =   71.9, wzz_SR =   59.2
nzh =  541264, wzh =   92.5, wzh_SR =   77.9
nhh =   35435, whh =   37.3, whh_SR =   33.4
sum_wS_SR 170.521446
sum_wB_SR 50599.001540
sum_wSp_SR 203.140887
sum_wSn_SR 32.619440
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 264.795507
sigScaleZH 394.679756
sigScaleHH 619.280794
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.192839
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset0.log
Setup training/validation tensors
0 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6732 (45, 10, 15, 31, 19) | 0.775 | 0.920 | 88.23 | 85.73 |-----------------|
0             Validation | 0.6745 (44, 11, 15, 31, 19) | 0.770 | 0.911 | 87.92 | 85.81 |##################| ^ (0.3%, 11.74, 9.6, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1032 batches)
0 >>  2/20 <<   Training | 0.6490 (40, 11, 17, 31, 21) | 0.883 | 1.037 | 88.59 | 86.25 |----------------------|
0             Validation | 0.6513 (39, 11, 17, 31, 21) | 0.890 | 1.043 | 88.31 | 86.31 |#######################| ^ (0.2%, 12.49, 9.2, 0%) 
0 >>  3/20 <<   Training | 0.6459 (37, 11, 18, 34, 19) | 0.833 | 0.980 | 88.64 | 86.09 |--------------------|
0             Validation | 0.6487 (37, 11, 18, 34, 19) | 0.831 | 0.976 | 88.42 | 86.09 |####################| ^ (0.2%, 12.02, 9.7, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (516 batches)
0 >>  4/20 <<   Training | 0.6391 (39, 10, 18, 32, 20) | 0.881 | 1.036 | 88.96 | 86.43 |------------------------|
0             Validation | 0.6420 (38, 11, 18, 32, 20) | 0.872 | 1.023 | 88.62 | 86.47 |########################| ^ (0.2%, 13.16, 9.0, 0%) 
0 >>  5/20 <<   Training | 0.6458 (39, 10, 16, 33, 20) | 0.929 | 1.091 | 88.94 | 86.53 |-------------------------|
0             Validation | 0.6479 (38, 11, 16, 34, 20) | 0.938 | 1.100 | 88.65 | 86.62 |##########################| ^ (0.3%, 12.25, 9.2, 0%) 
0 >>  6/20 <<   Training | 0.6400 (34, 10, 18, 34, 23) | 0.981 | 1.152 | 89.09 | 86.60 |-------------------------|
0             Validation | 0.6435 (33, 11, 18, 35, 23) | 0.978 | 1.144 | 88.78 | 86.60 |##########################| ^ (0.2%, 11.93, 9.0, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (258 batches)
0 >>  7/20 <<   Training | 0.6357 (36, 11, 18, 33, 22) | 0.941 | 1.105 | 89.21 | 86.66 |--------------------------|
0             Validation | 0.6390 (36, 11, 18, 33, 22) | 0.950 | 1.112 | 88.83 | 86.67 |##########################| ^ (0.2%, 11.78, 8.6, 0%) 
0 >>  8/20 <<   Training | 0.6349 (37, 11, 18, 32, 21) | 0.952 | 1.119 | 89.22 | 86.70 |--------------------------|
0             Validation | 0.6378 (37, 11, 18, 33, 21) | 0.962 | 1.127 | 88.84 | 86.73 |###########################| ^ (0.2%, 12.04, 8.7, 0%) 
0 >>  9/20 <<   Training | 0.6376 (41, 11, 18, 31, 20) | 0.920 | 1.079 | 89.22 | 86.66 |--------------------------|
0             Validation | 0.6400 (40, 11, 18, 31, 20) | 0.911 | 1.066 | 88.90 | 86.71 |###########################| ^ (0.2%, 11.86, 9.3, 0%) 
0 >> 10/20 <<   Training | 0.6365 (38, 11, 18, 32, 21) | 0.930 | 1.092 | 89.31 | 86.53 |-------------------------|
0             Validation | 0.6396 (37, 11, 18, 32, 21) | 0.923 | 1.079 | 88.92 | 86.56 |#########################| ^ (0.2%, 11.01, 9.2, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (129 batches)
0 >> 11/20 <<   Training | 0.6414 (39, 11, 16, 32, 21) | 0.914 | 1.075 | 89.37 | 86.62 |--------------------------|
0             Validation | 0.6440 (39, 11, 16, 33, 21) | 0.920 | 1.078 | 89.00 | 86.67 |##########################| ^ (0.2%, 11.59, 9.5, 0%) 
0 >> 12/20 <<   Training | 0.6329 (39, 11, 18, 32, 19) | 0.890 | 1.047 | 89.19 | 86.63 |--------------------------|
0             Validation | 0.6354 (39, 11, 18, 32, 19) | 0.895 | 1.047 | 88.86 | 86.68 |##########################| ^ (0.3%, 11.82, 9.2, 0%) 
0 >> 13/20 <<   Training | 0.6347 (39, 11, 17, 32, 21) | 0.934 | 1.096 | 89.41 | 86.71 |---------------------------|
0             Validation | 0.6373 (38, 11, 17, 32, 21) | 0.957 | 1.118 | 89.03 | 86.75 |###########################| ^ (0.3%, 11.12, 9.0, 0%) 
0 >> 14/20 <<   Training | 0.6340 (38, 11, 18, 32, 21) | 0.947 | 1.113 | 89.33 | 86.74 |---------------------------|
0             Validation | 0.6371 (37, 11, 18, 33, 21) | 0.951 | 1.113 | 88.96 | 86.76 |###########################| ^ (0.2%, 11.16, 8.8, 0%) 
0 >> 15/20 <<   Training | 0.6340 (40, 10, 18, 31, 20) | 0.938 | 1.100 | 89.21 | 86.73 |---------------------------|
0             Validation | 0.6367 (39, 11, 18, 31, 20) | 0.937 | 1.096 | 88.85 | 86.75 |###########################| ^ (0.2%, 10.75, 8.9, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6321 (38, 11, 18, 32, 21) | 0.947 | 1.111 | 89.26 | 86.77 |---------------------------|
0             Validation | 0.6349 (37, 11, 18, 33, 21) | 0.958 | 1.119 | 88.90 | 86.81 |############################| ^ (0.2%, 11.80, 9.5, 0%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6323 (39, 11, 18, 32, 20) | 0.944 | 1.108 | 89.41 | 86.78 |---------------------------|
0             Validation | 0.6350 (38, 11, 18, 32, 20) | 0.953 | 1.114 | 89.03 | 86.82 |############################| ^ (0.2%, 11.63, 8.9, 0%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6324 (38, 11, 18, 32, 21) | 0.947 | 1.112 | 89.42 | 86.79 |---------------------------|
0             Validation | 0.6351 (38, 11, 18, 33, 21) | 0.961 | 1.123 | 89.04 | 86.83 |############################| ^ (0.2%, 11.64, 9.1, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6321 (38, 11, 18, 32, 21) | 0.949 | 1.114 | 89.41 | 86.79 |---------------------------|
0             Validation | 0.6348 (38, 11, 18, 33, 21) | 0.960 | 1.122 | 89.03 | 86.83 |############################| ^ (0.2%, 11.67, 8.8, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6320 (38, 11, 18, 32, 21) | 0.949 | 1.115 | 89.41 | 86.79 |---------------------------|
0             Validation | 0.6348 (38, 11, 18, 33, 21) | 0.959 | 1.121 | 89.03 | 86.83 |############################| ^ (0.2%, 11.68, 8.9, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

0 >> DONE <<
0 Minimum Loss = 0.632009
