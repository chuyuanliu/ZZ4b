Mon Apr 18 14:09:50 2022
Using weight: weight for classifier: SvB
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 2396428
wDB 114777.240743
wDBn -53.981695
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 128876
wT 4694.203751
wTn -16.779144
nS 645566
nB 2525304
sum_wS 201.702339
sum_wB 119579.407884
nzz =   68867, wzz =   71.9, wzz_SR =   59.2
nzh =  541264, wzh =   92.5, wzh_SR =   77.9
nhh =   35435, whh =   37.3, whh_SR =   33.4
sum_wS_SR 170.521446
sum_wB_SR 50599.001540
sum_wSp_SR 203.140887
sum_wSn_SR 32.619440
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 264.795507
sigScaleZH 394.679756
sigScaleHH 619.280794
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.192839
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset2.log
Setup training/validation tensors
2 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >>  1/20 <<   Training | 0.6704 (37, 10, 16, 35, 21) | 0.841 | 0.993 | 87.70 | 85.82 |------------------|
2             Validation | 0.6755 (38,  9, 17, 34, 21) | 0.777 | 0.921 | 87.31 | 85.51 |###############| ^ (0.9%, 13.01, 9.3, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1031 batches)
2 >>  2/20 <<   Training | 0.6470 (36, 11, 18, 33, 21) | 0.909 | 1.068 | 88.12 | 86.30 |----------------------|
2             Validation | 0.6525 (36, 11, 19, 33, 20) | 0.837 | 0.987 | 87.56 | 86.06 |####################| ^ (0.7%, 12.31, 8.4, 0%) 
2 >>  3/20 <<   Training | 0.6524 (40, 11, 17, 31, 20) | 0.869 | 1.024 | 88.24 | 86.12 |---------------------|
2             Validation | 0.6577 (40, 11, 18, 31, 20) | 0.806 | 0.953 | 87.77 | 85.89 |##################| ^ (0.7%, 11.92, 9.8, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (515 batches)
2 >>  4/20 <<   Training | 0.6414 (37, 10, 18, 33, 21) | 0.921 | 1.080 | 88.23 | 86.54 |-------------------------|
2             Validation | 0.6475 (37, 10, 19, 32, 21) | 0.855 | 1.004 | 87.76 | 86.28 |######################| ^ (0.7%, 10.74, 8.4, 0%) 
2 >>  5/20 <<   Training | 0.6409 (35, 11, 17, 35, 21) | 0.967 | 1.136 | 88.57 | 86.64 |--------------------------|
2             Validation | 0.6464 (35, 11, 18, 34, 21) | 0.893 | 1.051 | 88.04 | 86.43 |########################| ^ (0.6%, 11.09, 8.8, 0%) 
2 >>  6/20 <<   Training | 0.6385 (38, 11, 18, 33, 19) | 0.920 | 1.081 | 88.38 | 86.61 |--------------------------|
2             Validation | 0.6443 (38, 11, 19, 33, 19) | 0.861 | 1.014 | 87.93 | 86.41 |########################| ^ (0.6%, 11.62, 8.4, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (257 batches)
2 >>  7/20 <<   Training | 0.6400 (39, 11, 16, 33, 21) | 0.950 | 1.115 | 89.05 | 86.74 |---------------------------|
2             Validation | 0.6463 (38, 10, 18, 32, 21) | 0.891 | 1.046 | 88.37 | 86.50 |#########################| ^ (0.7%, 11.80, 9.2, 0%) 
2 >>  8/20 <<   Training | 0.6400 (39, 11, 16, 32, 21) | 0.936 | 1.098 | 89.09 | 86.77 |---------------------------|
2             Validation | 0.6464 (39, 11, 17, 31, 21) | 0.875 | 1.029 | 88.40 | 86.53 |#########################| ^ (0.7%, 11.33, 8.8, 0%) 
2 >>  9/20 <<   Training | 0.6376 (39, 11, 17, 32, 20) | 0.951 | 1.117 | 88.95 | 86.74 |---------------------------|
2             Validation | 0.6439 (39, 11, 18, 32, 20) | 0.888 | 1.045 | 88.24 | 86.54 |#########################| ^ (0.6%, 10.86, 9.5, 0%) 
2 >> 10/20 <<   Training | 0.6348 (37, 10, 17, 33, 21) | 0.964 | 1.133 | 89.11 | 86.77 |---------------------------|
2             Validation | 0.6407 (37, 10, 19, 33, 21) | 0.895 | 1.053 | 88.45 | 86.58 |#########################| ^ (0.5%, 10.72, 7.8, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (128 batches)
2 >> 11/20 <<   Training | 0.6319 (37, 11, 18, 32, 21) | 0.969 | 1.137 | 89.14 | 86.80 |---------------------------|
2             Validation | 0.6389 (37, 11, 20, 31, 21) | 0.897 | 1.055 | 88.48 | 86.56 |#########################| ^ (0.6%, 10.95, 9.4, 0%) 
2 >> 12/20 <<   Training | 0.6350 (39, 11, 17, 33, 20) | 0.953 | 1.119 | 89.23 | 86.84 |----------------------------|
2             Validation | 0.6415 (38, 11, 18, 32, 20) | 0.883 | 1.038 | 88.49 | 86.62 |##########################| ^ (0.6%, 11.39, 9.8, 0%) 
2 >> 13/20 <<   Training | 0.6344 (37, 11, 18, 33, 21) | 0.969 | 1.137 | 89.15 | 86.75 |---------------------------|
2             Validation | 0.6411 (37, 10, 19, 32, 21) | 0.904 | 1.062 | 88.53 | 86.51 |#########################| ^ (0.7%, 10.98, 9.8, 0%) 
2 >> 14/20 <<   Training | 0.6334 (40, 11, 18, 32, 18) | 0.876 | 1.030 | 89.10 | 86.63 |--------------------------|
2             Validation | 0.6407 (40, 11, 19, 32, 18) | 0.807 | 0.950 | 88.53 | 86.37 |#######################| ^ (0.7%, 9.94, 9.8, 0%) 
2 >> 15/20 <<   Training | 0.6341 (41, 11, 18, 30, 20) | 0.898 | 1.054 | 89.12 | 86.71 |---------------------------|
2             Validation | 0.6403 (41, 11, 19, 29, 19) | 0.837 | 0.983 | 88.66 | 86.50 |#########################| ^ (0.6%, 10.20, 9.5, 0%) 
Decay learning rate: 0.010000 -> 0.002500
2 >> 16/20 <<   Training | 0.6300 (38, 11, 18, 33, 20) | 0.955 | 1.122 | 89.34 | 86.82 |----------------------------|
2             Validation | 0.6369 (38, 11, 19, 32, 19) | 0.888 | 1.044 | 88.64 | 86.60 |#########################| ^ (0.6%, 10.25, 9.6, 0%) 
Decay learning rate: 0.002500 -> 0.000625
2 >> 17/20 <<   Training | 0.6313 (38, 11, 17, 32, 20) | 0.968 | 1.136 | 89.36 | 86.88 |----------------------------|
2             Validation | 0.6379 (38, 11, 19, 32, 20) | 0.896 | 1.053 | 88.74 | 86.66 |##########################| ^ (0.6%, 10.33, 9.6, 0%) 
Decay learning rate: 0.000625 -> 0.000156
2 >> 18/20 <<   Training | 0.6308 (38, 11, 18, 32, 21) | 0.980 | 1.149 | 89.35 | 86.90 |-----------------------------|
2             Validation | 0.6374 (38, 11, 19, 32, 20) | 0.902 | 1.060 | 88.75 | 86.68 |##########################| ^ (0.6%, 10.39, 10.0, 0%) 
Decay learning rate: 0.000156 -> 0.000039
2 >> 19/20 <<   Training | 0.6309 (38, 11, 17, 32, 21) | 0.981 | 1.151 | 89.36 | 86.91 |-----------------------------|
2             Validation | 0.6375 (38, 11, 19, 32, 20) | 0.902 | 1.059 | 88.75 | 86.68 |##########################| ^ (0.6%, 10.40, 9.6, 0%) 
Decay learning rate: 0.000039 -> 0.000010
2 >> 20/20 <<   Training | 0.6309 (38, 11, 17, 32, 21) | 0.981 | 1.151 | 89.36 | 86.91 |-----------------------------|
2             Validation | 0.6375 (38, 11, 19, 32, 20) | 0.902 | 1.060 | 88.75 | 86.68 |##########################| ^ (0.6%, 10.40, 9.6, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset2_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

2 >> DONE <<
2 Minimum Loss = 0.630006
