Mon Apr 18 14:09:50 2022
Using weight: weight for classifier: SvB
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 2396428
wDB 114777.240743
wDBn -53.981695
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 128876
wT 4694.203751
wTn -16.779144
nS 645566
nB 2525304
sum_wS 201.702339
sum_wB 119579.407884
nzz =   68867, wzz =   71.9, wzz_SR =   59.2
nzh =  541264, wzh =   92.5, wzh_SR =   77.9
nhh =   35435, whh =   37.3, whh_SR =   33.4
sum_wS_SR 170.521446
sum_wB_SR 50599.001540
sum_wSp_SR 203.140887
sum_wSn_SR 32.619440
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 264.795507
sigScaleZH 394.679756
sigScaleHH 619.280794
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.192839
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset2.log
Setup training/validation tensors
2 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >>  1/20 <<   Training | 0.6704 (37, 10, 16, 35, 21) | 0.841 | 0.993 | 87.70 | 85.82 |------------------|
2             Validation | 0.6755 (38,  9, 17, 34, 21) | 0.777 | 0.921 | 87.31 | 85.51 |###############| ^ (0.9%, 13.01, 9.3, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1031 batches)
2 >>  2/20 <<   Training | 0.6470 (36, 11, 18, 33, 21) | 0.909 | 1.068 | 88.12 | 86.30 |----------------------|
2             Validation | 0.6525 (36, 11, 19, 33, 20) | 0.837 | 0.987 | 87.56 | 86.06 |####################| ^ (0.7%, 12.31, 8.4, 0%) 
2 >>  3/20 <<   Training | 0.6524 (40, 11, 17, 31, 20) | 0.869 | 1.024 | 88.24 | 86.12 |---------------------|
2             Validation | 0.6577 (40, 11, 18, 31, 20) | 0.806 | 0.953 | 87.77 | 85.89 |##################| ^ (0.7%, 11.92, 9.8, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (515 batches)
2 >>  4/20 <<   Training | 0.6414 (37, 10, 18, 33, 21) | 0.921 | 1.080 | 88.23 | 86.54 |-------------------------|
2             Validation | 0.6475 (37, 10, 19, 32, 21) | 0.855 | 1.004 | 87.76 | 86.28 |######################| ^ (0.7%, 10.74, 8.4, 0%) 
2 >>  5/20 <<   Training | 0.6409 (35, 11, 17, 35, 21) | 0.967 | 1.136 | 88.57 | 86.64 |--------------------------|
2             Validation | 0.6464 (35, 11, 18, 34, 21) | 0.893 | 1.051 | 88.04 | 86.43 |########################| ^ (0.6%, 11.09, 8.8, 0%) 
2 >>  6/20 <<   Training | 0.6385 (38, 11, 18, 33, 19) | 0.920 | 1.081 | 88.38 | 86.61 |--------------------------|
2             Validation | 0.6443 (38, 11, 19, 33, 19) | 0.861 | 1.014 | 87.93 | 86.41 |########################| ^ (0.6%, 11.62, 8.4, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (257 batches)
2 >>  7/20 <<   Training | 0.6400 (39, 11, 16, 33, 21) | 0.950 | 1.115 | 89.05 | 86.74 |---------------------------|
2             Validation | 0.6463 (38, 10, 18, 32, 21) | 0.891 | 1.046 | 88.37 | 86.50 |#########################| ^ (0.7%, 11.80, 9.2, 0%) 
2 >>  8/20 <<   Training | 0.6400 (39, 11, 16, 32, 21) | 0.936 | 1.098 | 89.09 | 86.77 |---------------------------|
2             Validation | 0.6464 (39, 11, 17, 31, 21) | 0.875 | 1.029 | 88.40 | 86.53 |#########################| ^ (0.7%, 11.33, 8.8, 0%) 
2 >>  9/20 <<   Training | 0.6376 (39, 11, 17, 32, 20) | 0.951 | 1.117 | 88.95 | 86.74 |---------------------------|
2             Validation | 0.6439 (39, 11, 18, 32, 20) | 0.888 | 1.045 | 88.24 | 86.54 |#########################| ^ (0.6%, 10.86, 9.5, 0%) 
2 >> 10/20 <<   Training | 0.6348 (37, 10, 17, 33, 21) | 0.964 | 1.133 | 89.11 | 86.77 |---------------------------|
2             Validation | 0.6407 (37, 10, 19, 33, 21) | 0.895 | 1.053 | 88.45 | 86.58 |#########################| ^ (0.5%, 10.72, 7.8, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (128 batches)
2 >> 11/20 <<   Training | 0.6319 (37, 11, 18, 32, 21) | 0.969 | 1.137 | 89.14 | 86.80 |---------------------------|
2             Validation | 0.6389 (37, 11, 20, 31, 21) | 0.897 | 1.055 | 88.48 | 86.56 |#########################| ^ (0.6%, 10.95, 9.4, 0%) 
2 >> 12/20 <<   Training | 0.6350 (39, 11, 17, 33, 20) | 0.953 | 1.119 | 89.23 | 86.84 |----------------------------|
2             Validation | 0.6415 (38, 11, 18, 32, 20) | 0.883 | 1.038 | 88.49 | 86.62 |##########################| ^ (0.6%, 11.39, 9.8, 0%) 
2 >> 13/20 <<   Training | 0.6344 (37, 11, 18, 33, 21) | 0.969 | 1.137 | 89.15 | 86.75 |---------------------------|
2             Validation | 0.6411 (37, 10, 19, 32, 21) | 0.904 | 1.062 | 88.53 | 86.51 |#########################| ^ (0.7%, 10.98, 9.8, 0%) 
2 >> 14/20 <<   Training | 0.6334 (40, 11, 18, 32, 18) | 0.876 | 1.030 | 89.10 | 86.63 |--------------------------|
2             Validation | 0.6407 (40, 11, 19, 32, 18) | 0.807 | 0.950 | 88.53 | 86.37 |#######################| ^ (0.7%, 9.94, 9.8, 0%) 
2 >> 15/20 <<   Training | 0.6341 (41, 11, 18, 30, 20) | 0.898 | 1.054 | 89.12 | 86.71 |---------------------------|
2             Validation | 0.6403 (41, 11, 19, 29, 19) | 0.837 | 0.983 | 88.66 | 86.50 |#########################| ^ (0.6%, 10.20, 9.5, 0%) 
Decay learning rate: 0.010000 -> 0.002500
2 >> 16/20 <<   Training | 0.6300 (38, 11, 18, 33, 20) | 0.955 | 1.122 | 89.34 | 86.82 |----------------------------|
2             Validation | 0.6369 (38, 11, 19, 32, 19) | 0.888 | 1.044 | 88.64 | 86.60 |#########################| ^ (0.6%, 10.25, 9.6, 0%) 
Decay learning rate: 0.002500 -> 0.000625
2 >> 17/20 <<   Training | 0.6313 (38, 11, 17, 32, 20) | 0.968 | 1.136 | 89.36 | 86.88 |----------------------------|
2             Validation | 0.6379 (38, 11, 19, 32, 20) | 0.896 | 1.053 | 88.74 | 86.66 |##########################| ^ (0.6%, 10.33, 9.6, 0%) 
Decay learning rate: 0.000625 -> 0.000156
2 >> 18/20 <<   Training | 0.6308 (38, 11, 18, 32, 21) | 0.980 | 1.149 | 89.35 | 86.90 |-----------------------------|
2             Validation | 0.6374 (38, 11, 19, 32, 20) | 0.902 | 1.060 | 88.75 | 86.68 |##########################| ^ (0.6%, 10.39, 10.0, 0%) 
Decay learning rate: 0.000156 -> 0.000039
2 >> 19/20 <<   Training | 0.6309 (38, 11, 17, 32, 21) | 0.981 | 1.151 | 89.36 | 86.91 |-----------------------------|
2             Validation | 0.6375 (38, 11, 19, 32, 20) | 0.902 | 1.059 | 88.75 | 86.68 |##########################| ^ (0.6%, 10.40, 9.6, 0%) 
Decay learning rate: 0.000039 -> 0.000010
2 >> 20/20 <<   Training | 0.6309 (38, 11, 17, 32, 21) | 0.981 | 1.151 | 89.36 | 86.91 |-----------------------------|
2             Validation | 0.6375 (38, 11, 19, 32, 20) | 0.902 | 1.060 | 88.75 | 86.68 |##########################| ^ (0.6%, 10.40, 9.6, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset2_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

2 >> DONE <<
2 Minimum Loss = 0.630006
Mon May  9 13:05:17 2022
Using weight: weight for classifier: SvB
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 3279028
wDB 147993.557827
wDBn -59.208815
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 179878
wT 6758.331864
wTn -23.581879
nS 940327
nB 3458906
sum_wS 167.472288
sum_wB 154870.307320
nzz =  323399, wzz =   57.4, wzz_SR =   46.2
nzh =  574169, wzh =   74.7, wzh_SR =   61.9
nhh =   42759, whh =   35.4, whh_SR =   31.2
sum_wS_SR 139.340355
sum_wB_SR 57305.889757
sum_wSp_SR 164.693187
sum_wSn_SR 25.352832
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 295.764663
sigScaleZH 457.493368
sigScaleHH 679.928873
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.099681
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset2.log
Setup training/validation tensors
2 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) |  S/B   | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >>  1/20 <<   Training | 0.6605 (40, 12, 15, 25, 18) |  9/325 | 0.705 | 88.13 | 86.96 |-----------------------------|
2             Validation | 0.6594 (40, 12, 15, 25, 18) |  9/328 | 0.699 | 87.90 | 86.91 |#############################| ^ (0.3%, 8.36, 3.7, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1431 batches)
2 >>  2/20 <<   Training | 0.6277 (33, 13, 16, 28, 20) |  6/ 84 | 0.983 | 88.74 | 88.25 |------------------------------------------|
2             Validation | 0.6271 (33, 13, 16, 28, 20) |  6/ 97 | 0.941 | 88.54 | 88.19 |#########################################| ^ (0.3%, 9.14, 1.9, 0%) 
2 >>  3/20 <<   Training | 0.6300 (37, 13, 15, 27, 18) | 10/150 | 0.968 | 88.72 | 88.38 |-------------------------------------------|
2             Validation | 0.6293 (37, 13, 15, 27, 18) |  9/175 | 0.918 | 88.54 | 88.33 |###########################################| ^ (0.2%, 9.21, 2.6, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (715 batches)
2 >>  4/20 <<   Training | 0.6245 (36, 12, 15, 27, 19) | 10/158 | 0.987 | 89.05 | 88.37 |-------------------------------------------|
2             Validation | 0.6241 (36, 12, 15, 27, 19) | 10/190 | 0.934 | 88.81 | 88.32 |###########################################| ^ (0.2%, 9.49, 2.7, 0%) 
2 >>  5/20 <<   Training | 0.6229 (33, 12, 15, 30, 20) |  8/ 98 | 1.013 | 89.00 | 88.43 |--------------------------------------------|
2             Validation | 0.6223 (33, 12, 16, 30, 20) |  8/126 | 0.954 | 88.77 | 88.40 |###########################################| ^ (0.2%, 9.74, 1.7, 2%) 
2 >>  6/20 <<   Training | 0.6228 (35, 13, 15, 28, 19) |  9/141 | 0.970 | 89.26 | 88.47 |--------------------------------------------|
2             Validation | 0.6219 (35, 13, 15, 28, 19) |  9/173 | 0.920 | 89.06 | 88.44 |############################################| ^ (0.2%, 9.62, 2.6, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (357 batches)
2 >>  7/20 <<   Training | 0.6215 (34, 13, 15, 28, 20) |  9/128 | 1.021 | 89.36 | 88.57 |---------------------------------------------|
2             Validation | 0.6209 (34, 13, 15, 28, 20) |  9/153 | 0.964 | 89.14 | 88.53 |#############################################| ^ (0.2%, 9.67, 2.0, 0%) 
2 >>  8/20 <<   Training | 0.6220 (36, 13, 15, 28, 19) |  9/125 | 1.004 | 89.33 | 88.52 |---------------------------------------------|
2             Validation | 0.6211 (36, 12, 15, 28, 19) |  9/152 | 0.946 | 89.11 | 88.48 |############################################| ^ (0.2%, 9.58, 2.5, 0%) 
2 >>  9/20 <<   Training | 0.6273 (38, 13, 14, 27, 19) | 14/300 | 0.918 | 89.42 | 88.38 |-------------------------------------------|
2             Validation | 0.6270 (38, 12, 14, 27, 19) | 14/323 | 0.892 | 89.18 | 88.33 |###########################################| ^ (0.2%, 9.72, 2.1, 0%) 
2 >> 10/20 <<   Training | 0.6187 (33, 12, 15, 29, 19) | 10/160 | 1.011 | 89.31 | 88.59 |---------------------------------------------|
2             Validation | 0.6184 (34, 12, 16, 29, 19) | 10/193 | 0.958 | 89.05 | 88.54 |#############################################| ^ (0.2%, 9.59, 2.1, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (178 batches)
2 >> 11/20 <<   Training | 0.6240 (38, 13, 14, 28, 18) | 13/235 | 0.971 | 89.33 | 88.61 |----------------------------------------------|
2             Validation | 0.6237 (38, 13, 14, 27, 18) | 13/264 | 0.928 | 89.08 | 88.56 |#############################################| ^ (0.2%, 9.66, 2.7, 0%) 
2 >> 12/20 <<   Training | 0.6165 (32, 13, 16, 29, 19) |  9/131 | 0.996 | 89.28 | 88.54 |---------------------------------------------|
2             Validation | 0.6162 (32, 13, 16, 29, 19) |  9/154 | 0.954 | 89.05 | 88.48 |############################################| ^ (0.2%, 9.44, 2.8, 0%) 
2 >> 13/20 <<   Training | 0.6198 (36, 13, 15, 27, 19) | 11/194 | 0.992 | 89.33 | 88.62 |----------------------------------------------|
2             Validation | 0.6194 (36, 13, 15, 27, 19) | 12/216 | 0.956 | 89.13 | 88.57 |#############################################| ^ (0.2%, 9.28, 2.7, 0%) 
2 >> 14/20 <<   Training | 0.6194 (31, 13, 16, 29, 21) |  7/ 78 | 1.040 | 89.31 | 88.52 |---------------------------------------------|
2             Validation | 0.6185 (31, 13, 16, 29, 21) |  7/ 97 | 0.984 | 89.07 | 88.49 |############################################| ^ (0.2%, 9.67, 2.4, 0%) 
2 >> 15/20 <<   Training | 0.6182 (31, 13, 16, 30, 20) |  7/ 78 | 1.048 | 89.37 | 88.63 |----------------------------------------------|
2             Validation | 0.6177 (31, 13, 16, 30, 20) |  7/102 | 0.981 | 89.11 | 88.59 |#############################################| ^ (0.2%, 9.43, 2.8, 0%) 
Decay learning rate: 0.010000 -> 0.002500
2 >> 16/20 <<   Training | 0.6186 (34, 13, 15, 28, 20) |  9/134 | 1.025 | 89.49 | 88.66 |----------------------------------------------|
2             Validation | 0.6183 (34, 13, 15, 28, 20) |  9/165 | 0.961 | 89.26 | 88.60 |#############################################| ^ (0.2%, 9.46, 3.7, 0%) 
Decay learning rate: 0.002500 -> 0.000625
2 >> 17/20 <<   Training | 0.6177 (34, 13, 15, 29, 20) | 10/139 | 1.025 | 89.46 | 88.65 |----------------------------------------------|
2             Validation | 0.6174 (34, 13, 15, 28, 20) | 10/166 | 0.968 | 89.22 | 88.60 |##############################################| ^ (0.2%, 9.39, 3.1, 0%) 
Decay learning rate: 0.000625 -> 0.000156
2 >> 18/20 <<   Training | 0.6173 (34, 13, 15, 28, 19) | 10/146 | 1.017 | 89.44 | 88.65 |----------------------------------------------|
2             Validation | 0.6169 (34, 13, 15, 28, 19) | 10/174 | 0.963 | 89.21 | 88.60 |#############################################| ^ (0.2%, 9.39, 2.9, 0%) 
Decay learning rate: 0.000156 -> 0.000039
2 >> 19/20 <<   Training | 0.6170 (34, 13, 15, 28, 19) | 10/142 | 1.016 | 89.43 | 88.65 |----------------------------------------------|
2             Validation | 0.6166 (34, 13, 15, 28, 19) |  9/170 | 0.961 | 89.20 | 88.60 |#############################################| ^ (0.2%, 9.38, 2.7, 0%) 
Decay learning rate: 0.000039 -> 0.000010
2 >> 20/20 <<   Training | 0.6170 (34, 13, 15, 28, 19) | 10/143 | 1.015 | 89.43 | 88.65 |----------------------------------------------|
2             Validation | 0.6166 (34, 13, 15, 28, 19) | 10/171 | 0.962 | 89.20 | 88.60 |#############################################| ^ (0.2%, 9.38, 2.7, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset2_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

2 >> DONE <<
2 Minimum Loss = 0.616499
