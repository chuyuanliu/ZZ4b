2 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >>  1/20 <<   Training | 0.8845 (27, 31,  7, 19) | 1.090 | 2.603 | 83.57 | 67.15 |---------------------|
2             Validation | 0.8845 (27, 31,  7, 18) | 1.106 | 1.490 | 83.53 | 67.31 |#######################| ^ (1.2%, 2.78, 2.5, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1337 batches)
2 >>  2/20 <<   Training | 0.8748 (25, 31,  7, 20) | 1.001 | 2.035 | 84.29 | 68.63 |------------------------------------|
2             Validation | 0.8753 (25, 31,  7, 19) | 1.016 | 1.792 | 84.23 | 68.62 |####################################| ^ (0.7%, 2.91, 3.9, 0%) 
2 >>  3/20 <<   Training | 0.8716 (26, 31,  7, 19) | 1.030 | 2.377 | 84.22 | 68.69 |------------------------------------|
2             Validation | 0.8718 (26, 31,  7, 19) | 1.046 | 1.529 | 84.18 | 68.77 |#####################################| ^ (0.8%, 3.06, 4.2, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (668 batches)
2 >>  4/20 <<   Training | 0.8680 (26, 31,  7, 19) | 1.021 | 2.235 | 84.63 | 68.68 |------------------------------------|
2             Validation | 0.8683 (26, 31,  7, 19) | 1.036 | 1.486 | 84.59 | 68.71 |#####################################| ^ (0.7%, 3.83, 2.9, 0%) 
2 >>  5/20 <<   Training | 0.8674 (24, 33,  7, 19) | 0.915 | 1.286 | 84.76 | 69.37 |-------------------------------------------|
2             Validation | 0.8678 (24, 32,  7, 19) | 0.928 | 1.219 | 84.72 | 69.29 |##########################################| ^ (0.8%, 3.57, 2.8, 0%) 
2 >>  6/20 <<   Training | 0.8667 (24, 32,  7, 19) | 0.938 | 1.629 | 84.82 | 69.20 |------------------------------------------|
2             Validation | 0.8674 (25, 32,  7, 19) | 0.952 | 1.302 | 84.76 | 69.12 |#########################################| ^ (1.0%, 3.12, 2.4, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (334 batches)
2 >>  7/20 <<   Training | 0.8658 (26, 32,  7, 18) | 1.007 | 7.111 | 84.82 | 69.12 |-----------------------------------------|
2             Validation | 0.8667 (26, 31,  7, 18) | 1.021 | 4.326 | 84.77 | 69.06 |########################################| ^ (0.8%, 3.51, 1.9, 1%) 
2 >>  8/20 <<   Training | 0.8661 (25, 31,  7, 20) | 1.006 | 2.790 | 84.98 | 69.10 |-----------------------------------------|
2             Validation | 0.8671 (25, 31,  7, 20) | 1.020 | 1.508 | 84.90 | 68.95 |#######################################| ^ (1.3%, 3.15, 2.0, 0%) 
2 >>  9/20 <<   Training | 0.8654 (25, 33,  7, 19) | 0.936 | 3.214 | 84.93 | 69.29 |------------------------------------------|
2             Validation | 0.8665 (25, 33,  7, 18) | 0.949 | 2.176 | 84.86 | 69.13 |#########################################| ^ (1.4%, 3.47, 2.0, 0%) 
2 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >>  1/20 <<   Training | 0.8845 (27, 31,  7, 19) | 1.090 | 2.603 | 83.57 | 67.15 |---------------------|
2             Validation | 0.8845 (27, 31,  7, 18) | 1.106 | 1.490 | 83.53 | 67.31 |#######################| ^ (1.2%, 2.78, 2.5, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1337 batches)
2 >>  2/20 <<   Training | 0.8748 (25, 31,  7, 20) | 1.001 | 2.035 | 84.29 | 68.63 |------------------------------------|
2             Validation | 0.8753 (25, 31,  7, 19) | 1.016 | 1.792 | 84.23 | 68.62 |####################################| ^ (0.7%, 2.91, 3.9, 0%) 
2 >>  3/20 <<   Training | 0.8716 (26, 31,  7, 19) | 1.030 | 2.377 | 84.22 | 68.69 |------------------------------------|
2             Validation | 0.8718 (26, 31,  7, 19) | 1.046 | 1.529 | 84.18 | 68.77 |#####################################| ^ (0.8%, 3.06, 4.2, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (668 batches)
2 >>  4/20 <<   Training | 0.8680 (26, 31,  7, 19) | 1.021 | 2.235 | 84.63 | 68.68 |------------------------------------|
2             Validation | 0.8683 (26, 31,  7, 19) | 1.036 | 1.486 | 84.59 | 68.71 |#####################################| ^ (0.7%, 3.83, 2.9, 0%) 
2 >>  5/20 <<   Training | 0.8674 (24, 33,  7, 19) | 0.915 | 1.286 | 84.76 | 69.37 |-------------------------------------------|
2             Validation | 0.8678 (24, 32,  7, 19) | 0.928 | 1.219 | 84.72 | 69.29 |##########################################| ^ (0.8%, 3.57, 2.8, 0%) 
2 >>  6/20 <<   Training | 0.8667 (24, 32,  7, 19) | 0.938 | 1.629 | 84.82 | 69.20 |------------------------------------------|
2             Validation | 0.8674 (25, 32,  7, 19) | 0.952 | 1.302 | 84.76 | 69.12 |#########################################| ^ (1.0%, 3.12, 2.4, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (334 batches)
2 >>  7/20 <<   Training | 0.8658 (26, 32,  7, 18) | 1.007 | 7.111 | 84.82 | 69.12 |-----------------------------------------|
2             Validation | 0.8667 (26, 31,  7, 18) | 1.021 | 4.326 | 84.77 | 69.06 |########################################| ^ (0.8%, 3.51, 1.9, 1%) 
2 >>  8/20 <<   Training | 0.8661 (25, 31,  7, 20) | 1.006 | 2.790 | 84.98 | 69.10 |-----------------------------------------|
2             Validation | 0.8671 (25, 31,  7, 20) | 1.020 | 1.508 | 84.90 | 68.95 |#######################################| ^ (1.3%, 3.15, 2.0, 0%) 
2 >>  9/20 <<   Training | 0.8654 (25, 33,  7, 19) | 0.936 | 3.214 | 84.93 | 69.29 |------------------------------------------|
2             Validation | 0.8665 (25, 33,  7, 18) | 0.949 | 2.176 | 84.86 | 69.13 |#########################################| ^ (1.4%, 3.47, 2.0, 0%) 
2 >> 10/20 <<   Training | 0.8644 (26, 31,  7, 19) | 1.035 | 1.465 | 84.93 | 69.17 |-----------------------------------------|
2             Validation | 0.8657 (26, 31,  7, 19) | 1.050 | 1.903 | 84.86 | 69.03 |########################################| ^ (1.2%, 4.20, 2.1, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (167 batches)
2 >> 11/20 <<   Training | 0.8654 (23, 32,  7, 20) | 0.898 | 1.164 | 85.01 | 69.08 |----------------------------------------|
2             Validation | 0.8663 (24, 32,  7, 20) | 0.911 | 1.045 | 84.93 | 68.94 |#######################################| ^ (1.3%, 3.61, 2.4, 0%) 
2 >> 12/20 <<   Training | 0.8639 (25, 31,  7, 20) | 0.993 | 1.556 | 85.03 | 69.04 |----------------------------------------|
2             Validation | 0.8649 (25, 31,  7, 20) | 1.007 | 1.425 | 84.95 | 68.90 |#######################################| ^ (1.2%, 3.56, 2.7, 0%) 
2 >> 13/20 <<   Training | 0.8646 (27, 29,  7, 19) | 1.146 | 4.284 | 85.04 | 69.32 |-------------------------------------------|
2             Validation | 0.8660 (28, 29,  7, 19) | 1.162 | 2.167 | 84.95 | 69.17 |#########################################| ^ (1.4%, 3.46, 2.6, 0%) 
2 >> 14/20 <<   Training | 0.8626 (25, 32,  7, 19) | 0.981 | 1.617 | 85.07 | 69.25 |------------------------------------------|
2             Validation | 0.8638 (25, 32,  7, 19) | 0.995 | 0.891 | 84.99 | 69.09 |########################################| ^ (1.4%, 3.60, 2.7, 0%) 
2 >> 15/20 <<   Training | 0.8639 (26, 31,  7, 19) | 1.033 | 6.626 | 85.04 | 69.26 |------------------------------------------|
2             Validation | 0.8656 (27, 31,  7, 19) | 1.047 | 4.411 | 84.95 | 69.08 |########################################| ^ (1.5%, 4.03, 2.3, 0%) 
Decay learning rate: 0.010000 -> 0.002500
2 >> 16/20 <<   Training | 0.8629 (26, 32,  7, 18) | 1.008 | 1.105 | 85.13 | 69.54 |---------------------------------------------|
2             Validation | 0.8644 (26, 32,  7, 18) | 1.022 | 1.430 | 85.04 | 69.35 |###########################################| ^ (1.4%, 4.07, 2.2, 0%) 
Decay learning rate: 0.002500 -> 0.000625
2 >> 17/20 <<   Training | 0.8617 (25, 32,  7, 19) | 1.002 | 1.075 | 85.14 | 69.40 |-------------------------------------------|
2             Validation | 0.8631 (26, 32,  7, 19) | 1.016 | 1.038 | 85.05 | 69.22 |##########################################| ^ (1.4%, 3.93, 2.6, 0%) 
Decay learning rate: 0.000625 -> 0.000156
2 >> 18/20 <<   Training | 0.8616 (25, 32,  7, 19) | 0.992 | 1.146 | 85.14 | 69.41 |--------------------------------------------|
2             Validation | 0.8630 (26, 32,  7, 19) | 1.006 | 0.778 | 85.06 | 69.23 |##########################################| ^ (1.4%, 3.95, 2.6, 0%) 
Decay learning rate: 0.000156 -> 0.000039
2 >> 19/20 <<   Training | 0.8616 (25, 32,  7, 19) | 0.997 | 1.177 | 85.14 | 69.42 |--------------------------------------------|
2             Validation | 0.8630 (26, 32,  7, 19) | 1.011 | 0.932 | 85.06 | 69.25 |##########################################| ^ (1.4%, 3.97, 2.8, 0%) 
Decay learning rate: 0.000039 -> 0.000010
2 >> 20/20 <<   Training | 0.8616 (25, 32,  7, 19) | 0.997 | 1.033 | 85.14 | 69.42 |--------------------------------------------|
2             Validation | 0.8630 (26, 32,  7, 19) | 1.011 | 1.044 | 85.06 | 69.24 |##########################################| ^ (1.4%, 3.96, 2.6, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset2_epoch20_before_finetuning.pkl
Run Finetuning
2 >> 20/20 <<   Training | 0.8616 (25, 32,  7, 19) | 0.997 | 1.142 | 85.14 | 69.42 |--------------------------------------------|
2             Validation | 0.8630 (26, 32,  7, 19) | 1.011 | 0.865 | 85.06 | 69.24 |##########################################| ^ (1.4%, 3.97, 2.7, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset2_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
