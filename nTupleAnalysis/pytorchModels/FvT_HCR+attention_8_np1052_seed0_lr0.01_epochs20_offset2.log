2 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >>  1/20 <<   Training | 0.8845 (27, 31,  7, 19) | 1.090 | 2.603 | 83.57 | 67.15 |---------------------|
2             Validation | 0.8845 (27, 31,  7, 18) | 1.106 | 1.490 | 83.53 | 67.31 |#######################| ^ (1.2%, 2.78, 2.5, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1337 batches)
2 >>  2/20 <<   Training | 0.8748 (25, 31,  7, 20) | 1.001 | 2.035 | 84.29 | 68.63 |------------------------------------|
2             Validation | 0.8753 (25, 31,  7, 19) | 1.016 | 1.792 | 84.23 | 68.62 |####################################| ^ (0.7%, 2.91, 3.9, 0%) 
2 >>  3/20 <<   Training | 0.8716 (26, 31,  7, 19) | 1.030 | 2.377 | 84.22 | 68.69 |------------------------------------|
2             Validation | 0.8718 (26, 31,  7, 19) | 1.046 | 1.529 | 84.18 | 68.77 |#####################################| ^ (0.8%, 3.06, 4.2, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (668 batches)
2 >>  4/20 <<   Training | 0.8680 (26, 31,  7, 19) | 1.021 | 2.235 | 84.63 | 68.68 |------------------------------------|
2             Validation | 0.8683 (26, 31,  7, 19) | 1.036 | 1.486 | 84.59 | 68.71 |#####################################| ^ (0.7%, 3.83, 2.9, 0%) 
2 >>  5/20 <<   Training | 0.8674 (24, 33,  7, 19) | 0.915 | 1.286 | 84.76 | 69.37 |-------------------------------------------|
2             Validation | 0.8678 (24, 32,  7, 19) | 0.928 | 1.219 | 84.72 | 69.29 |##########################################| ^ (0.8%, 3.57, 2.8, 0%) 
2 >>  6/20 <<   Training | 0.8667 (24, 32,  7, 19) | 0.938 | 1.629 | 84.82 | 69.20 |------------------------------------------|
2             Validation | 0.8674 (25, 32,  7, 19) | 0.952 | 1.302 | 84.76 | 69.12 |#########################################| ^ (1.0%, 3.12, 2.4, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (334 batches)
2 >>  7/20 <<   Training | 0.8658 (26, 32,  7, 18) | 1.007 | 7.111 | 84.82 | 69.12 |-----------------------------------------|
2             Validation | 0.8667 (26, 31,  7, 18) | 1.021 | 4.326 | 84.77 | 69.06 |########################################| ^ (0.8%, 3.51, 1.9, 1%) 
2 >>  8/20 <<   Training | 0.8661 (25, 31,  7, 20) | 1.006 | 2.790 | 84.98 | 69.10 |-----------------------------------------|
2             Validation | 0.8671 (25, 31,  7, 20) | 1.020 | 1.508 | 84.90 | 68.95 |#######################################| ^ (1.3%, 3.15, 2.0, 0%) 
2 >>  9/20 <<   Training | 0.8654 (25, 33,  7, 19) | 0.936 | 3.214 | 84.93 | 69.29 |------------------------------------------|
2             Validation | 0.8665 (25, 33,  7, 18) | 0.949 | 2.176 | 84.86 | 69.13 |#########################################| ^ (1.4%, 3.47, 2.0, 0%) 
2 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >>  1/20 <<   Training | 0.8845 (27, 31,  7, 19) | 1.090 | 2.603 | 83.57 | 67.15 |---------------------|
2             Validation | 0.8845 (27, 31,  7, 18) | 1.106 | 1.490 | 83.53 | 67.31 |#######################| ^ (1.2%, 2.78, 2.5, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1337 batches)
2 >>  2/20 <<   Training | 0.8748 (25, 31,  7, 20) | 1.001 | 2.035 | 84.29 | 68.63 |------------------------------------|
2             Validation | 0.8753 (25, 31,  7, 19) | 1.016 | 1.792 | 84.23 | 68.62 |####################################| ^ (0.7%, 2.91, 3.9, 0%) 
2 >>  3/20 <<   Training | 0.8716 (26, 31,  7, 19) | 1.030 | 2.377 | 84.22 | 68.69 |------------------------------------|
2             Validation | 0.8718 (26, 31,  7, 19) | 1.046 | 1.529 | 84.18 | 68.77 |#####################################| ^ (0.8%, 3.06, 4.2, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (668 batches)
2 >>  4/20 <<   Training | 0.8680 (26, 31,  7, 19) | 1.021 | 2.235 | 84.63 | 68.68 |------------------------------------|
2             Validation | 0.8683 (26, 31,  7, 19) | 1.036 | 1.486 | 84.59 | 68.71 |#####################################| ^ (0.7%, 3.83, 2.9, 0%) 
2 >>  5/20 <<   Training | 0.8674 (24, 33,  7, 19) | 0.915 | 1.286 | 84.76 | 69.37 |-------------------------------------------|
2             Validation | 0.8678 (24, 32,  7, 19) | 0.928 | 1.219 | 84.72 | 69.29 |##########################################| ^ (0.8%, 3.57, 2.8, 0%) 
2 >>  6/20 <<   Training | 0.8667 (24, 32,  7, 19) | 0.938 | 1.629 | 84.82 | 69.20 |------------------------------------------|
2             Validation | 0.8674 (25, 32,  7, 19) | 0.952 | 1.302 | 84.76 | 69.12 |#########################################| ^ (1.0%, 3.12, 2.4, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (334 batches)
2 >>  7/20 <<   Training | 0.8658 (26, 32,  7, 18) | 1.007 | 7.111 | 84.82 | 69.12 |-----------------------------------------|
2             Validation | 0.8667 (26, 31,  7, 18) | 1.021 | 4.326 | 84.77 | 69.06 |########################################| ^ (0.8%, 3.51, 1.9, 1%) 
2 >>  8/20 <<   Training | 0.8661 (25, 31,  7, 20) | 1.006 | 2.790 | 84.98 | 69.10 |-----------------------------------------|
2             Validation | 0.8671 (25, 31,  7, 20) | 1.020 | 1.508 | 84.90 | 68.95 |#######################################| ^ (1.3%, 3.15, 2.0, 0%) 
2 >>  9/20 <<   Training | 0.8654 (25, 33,  7, 19) | 0.936 | 3.214 | 84.93 | 69.29 |------------------------------------------|
2             Validation | 0.8665 (25, 33,  7, 18) | 0.949 | 2.176 | 84.86 | 69.13 |#########################################| ^ (1.4%, 3.47, 2.0, 0%) 
2 >> 10/20 <<   Training | 0.8644 (26, 31,  7, 19) | 1.035 | 1.465 | 84.93 | 69.17 |-----------------------------------------|
2             Validation | 0.8657 (26, 31,  7, 19) | 1.050 | 1.903 | 84.86 | 69.03 |########################################| ^ (1.2%, 4.20, 2.1, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (167 batches)
2 >> 11/20 <<   Training | 0.8654 (23, 32,  7, 20) | 0.898 | 1.164 | 85.01 | 69.08 |----------------------------------------|
2             Validation | 0.8663 (24, 32,  7, 20) | 0.911 | 1.045 | 84.93 | 68.94 |#######################################| ^ (1.3%, 3.61, 2.4, 0%) 
2 >> 12/20 <<   Training | 0.8639 (25, 31,  7, 20) | 0.993 | 1.556 | 85.03 | 69.04 |----------------------------------------|
2             Validation | 0.8649 (25, 31,  7, 20) | 1.007 | 1.425 | 84.95 | 68.90 |#######################################| ^ (1.2%, 3.56, 2.7, 0%) 
2 >> 13/20 <<   Training | 0.8646 (27, 29,  7, 19) | 1.146 | 4.284 | 85.04 | 69.32 |-------------------------------------------|
2             Validation | 0.8660 (28, 29,  7, 19) | 1.162 | 2.167 | 84.95 | 69.17 |#########################################| ^ (1.4%, 3.46, 2.6, 0%) 
2 >> 14/20 <<   Training | 0.8626 (25, 32,  7, 19) | 0.981 | 1.617 | 85.07 | 69.25 |------------------------------------------|
2             Validation | 0.8638 (25, 32,  7, 19) | 0.995 | 0.891 | 84.99 | 69.09 |########################################| ^ (1.4%, 3.60, 2.7, 0%) 
2 >> 15/20 <<   Training | 0.8639 (26, 31,  7, 19) | 1.033 | 6.626 | 85.04 | 69.26 |------------------------------------------|
2             Validation | 0.8656 (27, 31,  7, 19) | 1.047 | 4.411 | 84.95 | 69.08 |########################################| ^ (1.5%, 4.03, 2.3, 0%) 
Decay learning rate: 0.010000 -> 0.002500
2 >> 16/20 <<   Training | 0.8629 (26, 32,  7, 18) | 1.008 | 1.105 | 85.13 | 69.54 |---------------------------------------------|
2             Validation | 0.8644 (26, 32,  7, 18) | 1.022 | 1.430 | 85.04 | 69.35 |###########################################| ^ (1.4%, 4.07, 2.2, 0%) 
Decay learning rate: 0.002500 -> 0.000625
2 >> 17/20 <<   Training | 0.8617 (25, 32,  7, 19) | 1.002 | 1.075 | 85.14 | 69.40 |-------------------------------------------|
2             Validation | 0.8631 (26, 32,  7, 19) | 1.016 | 1.038 | 85.05 | 69.22 |##########################################| ^ (1.4%, 3.93, 2.6, 0%) 
Decay learning rate: 0.000625 -> 0.000156
2 >> 18/20 <<   Training | 0.8616 (25, 32,  7, 19) | 0.992 | 1.146 | 85.14 | 69.41 |--------------------------------------------|
2             Validation | 0.8630 (26, 32,  7, 19) | 1.006 | 0.778 | 85.06 | 69.23 |##########################################| ^ (1.4%, 3.95, 2.6, 0%) 
Decay learning rate: 0.000156 -> 0.000039
2 >> 19/20 <<   Training | 0.8616 (25, 32,  7, 19) | 0.997 | 1.177 | 85.14 | 69.42 |--------------------------------------------|
2             Validation | 0.8630 (26, 32,  7, 19) | 1.011 | 0.932 | 85.06 | 69.25 |##########################################| ^ (1.4%, 3.97, 2.8, 0%) 
Decay learning rate: 0.000039 -> 0.000010
2 >> 20/20 <<   Training | 0.8616 (25, 32,  7, 19) | 0.997 | 1.033 | 85.14 | 69.42 |--------------------------------------------|
2             Validation | 0.8630 (26, 32,  7, 19) | 1.011 | 1.044 | 85.06 | 69.24 |##########################################| ^ (1.4%, 3.96, 2.6, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset2_epoch20_before_finetuning.pkl
Run Finetuning
2 >> 20/20 <<   Training | 0.8616 (25, 32,  7, 19) | 0.997 | 1.142 | 85.14 | 69.42 |--------------------------------------------|
2             Validation | 0.8630 (26, 32,  7, 19) | 1.011 | 0.865 | 85.06 | 69.24 |##########################################| ^ (1.4%, 3.97, 2.7, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset2_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
Fri Apr 15 18:13:22 2022
Using weight: mcPseudoTagWeight for classifier: FvT
Add true class labels to data
Add true class labels to ttbar MC
concatenate data and ttbar dataframes
add encoded target: d4=0, d3=1, t4=2, t3=3
2016 ---------------------------------------------
nd4 =   22557, wd4 =  22557.0, <w> = 1.000
nd3 = 1080290, wd3 =  59314.2, <w> = 0.055
nt4 =   24964, wt4 =   1078.9, <w> = 0.043
nt3 =  365641, wt3 =   9206.6, <w> = 0.025
2017 ---------------------------------------------
nd4 =   19573, wd4 =  19573.0, <w> = 1.000
nd3 =  484439, wd3 =  29448.0, <w> = 0.061
nt4 =   47278, wt4 =   1522.8, <w> = 0.032
nt3 =  546564, wt3 =   6362.3, <w> = 0.012
2018 ---------------------------------------------
nd4 =   26575, wd4 =  26575.0, <w> = 1.000
nd3 =  831699, wd3 =  53355.4, <w> = 0.064
nt4 =   56634, wt4 =   2092.5, <w> = 0.037
nt3 =  603708, wt3 =   9210.2, <w> = 0.015
All ----------------------------------------------
nd4 =   68705, wd4 =  68705.0, <w> = 1.000
nd3 = 2396428, wd3 = 142117.6, <w> = 0.059
nt4 =  128876, wt4 =   4694.2, <w> = 0.036
nt3 = 1515913, wt3 =  24779.1, <w> = 0.016
wtn =   -142.3
Normalization = wd4_SB/(wd3_SB-wt3_SB+wt4_SB)
              = 68705/(76704-10337+2321)
              = 1.000 +/- 0.004 (0.007 validation stat uncertainty, norm should converge to about this precision)
loaded die loss inside SR: 0.589803
loaded die loss: 0.828956
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset2.log
Setup training/validation tensors
2 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >>  1/20 <<   Training | 0.8813 (25, 31,  7, 19) | 0.986 | 3.220 | 83.71 | 68.42 |----------------------------------|
2             Validation | 0.8808 (25, 31,  7, 19) | 0.978 | 2.488 | 83.68 | 68.13 |###############################| ^ (1.7%, 2.90, 1.7, 2%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1337 batches)
2 >>  2/20 <<   Training | 0.8815 (25, 29,  8, 22) | 0.985 | 13.787 | 84.11 | 68.04 |------------------------------|
2             Validation | 0.8815 (25, 29,  8, 22) | 0.977 | 8.003 | 84.05 | 67.65 |##########################| ^ (2.2%, 2.67, 1.6, 3%) 
2 >>  3/20 <<   Training | 0.8721 (26, 31,  7, 20) | 1.048 | 6.570 | 84.54 | 68.06 |------------------------------|
2             Validation | 0.8722 (26, 31,  7, 20) | 1.040 | 3.322 | 84.47 | 67.64 |##########################| ^ (2.3%, 3.79, 1.9, 1%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (668 batches)
2 >>  4/20 <<   Training | 0.8679 (24, 32,  7, 19) | 0.941 | 1.359 | 84.73 | 69.56 |---------------------------------------------|
2             Validation | 0.8682 (24, 32,  7, 19) | 0.933 | 1.163 | 84.67 | 69.13 |#########################################| ^ (2.2%, 3.55, 2.5, 0%) 
2 >>  5/20 <<   Training | 0.8734 (25, 32,  7, 19) | 0.983 | 2.447 | 83.95 | 68.77 |-------------------------------------|
2             Validation | 0.8738 (25, 32,  7, 19) | 0.975 | 1.464 | 83.88 | 68.30 |################################| ^ (2.5%, 4.41, 1.1, 32%) 
2 >>  6/20 <<   Training | 0.8662 (25, 32,  7, 19) | 1.003 | 4.169 | 84.87 | 69.78 |-----------------------------------------------|
2             Validation | 0.8665 (25, 32,  7, 19) | 0.995 | 1.819 | 84.79 | 69.34 |###########################################| ^ (2.3%, 5.84, 1.3, 11%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (334 batches)
2 >>  7/20 <<   Training | 0.8673 (26, 32,  7, 17) | 1.072 | 5.135 | 85.01 | 69.54 |---------------------------------------------|
2             Validation | 0.8679 (26, 33,  7, 17) | 1.064 | 2.291 | 84.92 | 69.06 |########################################| ^ (2.6%, 5.35, 1.4, 10%) 
2 >>  8/20 <<   Training | 0.8652 (26, 31,  7, 19) | 1.042 | 1.981 | 84.89 | 69.28 |------------------------------------------|
2             Validation | 0.8657 (26, 32,  7, 19) | 1.034 | 1.253 | 84.81 | 68.84 |######################################| ^ (2.3%, 5.64, 1.2, 21%) 
2 >>  9/20 <<   Training | 0.8643 (26, 31,  7, 19) | 1.038 | 1.883 | 84.96 | 69.36 |-------------------------------------------|
2             Validation | 0.8651 (26, 31,  7, 19) | 1.029 | 1.811 | 84.88 | 68.89 |######################################| ^ (2.4%, 3.56, 0.9, 48%) 
2 >> 10/20 <<   Training | 0.8652 (27, 31,  7, 18) | 1.065 | 1.216 | 84.98 | 69.21 |------------------------------------------|
2             Validation | 0.8659 (27, 31,  7, 18) | 1.057 | 1.174 | 84.89 | 68.70 |#####################################| ^ (2.7%, 6.05, 1.7, 2%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (167 batches)
2 >> 11/20 <<   Training | 0.8635 (24, 32,  7, 19) | 0.937 | 1.317 | 85.03 | 69.64 |----------------------------------------------|
2             Validation | 0.8644 (24, 32,  7, 19) | 0.930 | 1.185 | 84.94 | 69.12 |#########################################| ^ (2.6%, 3.72, 1.9, 0%) 
2 >> 12/20 <<   Training | 0.8646 (23, 33,  7, 20) | 0.885 | 0.653 | 85.07 | 69.65 |----------------------------------------------|
2             Validation | 0.8657 (23, 33,  7, 20) | 0.878 | 0.836 | 84.98 | 69.10 |########################################| ^ (2.9%, 4.51, 1.1, 28%) 
2 >> 13/20 <<   Training | 0.8630 (25, 32,  7, 19) | 0.964 | 2.590 | 85.01 | 69.61 |----------------------------------------------|
2             Validation | 0.8642 (25, 32,  7, 19) | 0.956 | 2.206 | 84.92 | 69.04 |########################################| ^ (3.0%, 4.88, 1.4, 9%) 
2 >> 14/20 <<   Training | 0.8632 (26, 31,  7, 19) | 1.045 | 1.274 | 85.08 | 69.75 |-----------------------------------------------|
2             Validation | 0.8643 (26, 31,  7, 19) | 1.036 | 1.103 | 84.99 | 69.16 |#########################################| ^ (3.1%, 5.57, 1.0, 37%) 
2 >> 15/20 <<   Training | 0.8628 (25, 32,  7, 19) | 0.979 | 1.590 | 85.03 | 69.96 |-------------------------------------------------|
2             Validation | 0.8640 (25, 32,  7, 19) | 0.971 | 1.689 | 84.95 | 69.41 |############################################| ^ (2.8%, 4.96, 0.8, 59%) 
Decay learning rate: 0.010000 -> 0.002500
2 >> 16/20 <<   Training | 0.8615 (25, 32,  7, 19) | 0.993 | 1.061 | 85.18 | 69.80 |------------------------------------------------|
2             Validation | 0.8627 (25, 32,  7, 19) | 0.985 | 1.234 | 85.08 | 69.21 |##########################################| ^ (3.0%, 4.97, 1.5, 4%) 
Decay learning rate: 0.002500 -> 0.000625
2 >> 17/20 <<   Training | 0.8613 (25, 32,  7, 19) | 0.988 | 1.497 | 85.18 | 69.79 |-----------------------------------------------|
2             Validation | 0.8626 (25, 32,  7, 19) | 0.980 | 1.038 | 85.09 | 69.21 |##########################################| ^ (3.0%, 5.08, 1.0, 41%) 
Decay learning rate: 0.000625 -> 0.000156
2 >> 18/20 <<   Training | 0.8613 (25, 32,  7, 19) | 0.998 | 1.307 | 85.19 | 69.78 |-----------------------------------------------|
2             Validation | 0.8625 (25, 32,  7, 19) | 0.990 | 1.332 | 85.09 | 69.20 |#########################################| ^ (3.0%, 5.04, 1.0, 43%) 
Decay learning rate: 0.000156 -> 0.000039
2 >> 19/20 <<   Training | 0.8613 (25, 32,  7, 19) | 0.997 | 1.266 | 85.18 | 69.77 |-----------------------------------------------|
2             Validation | 0.8625 (25, 32,  7, 19) | 0.989 | 1.356 | 85.09 | 69.19 |#########################################| ^ (3.0%, 5.03, 0.9, 56%) 
Decay learning rate: 0.000039 -> 0.000010
2 >> 20/20 <<   Training | 0.8613 (25, 32,  7, 19) | 0.998 | 1.221 | 85.18 | 69.77 |-----------------------------------------------|
2             Validation | 0.8625 (25, 32,  7, 19) | 0.990 | 1.283 | 85.09 | 69.19 |#########################################| ^ (3.0%, 5.03, 0.9, 50%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset2_epoch20_before_finetuning.pkl
Run Finetuning
2 >> 20/20 <<   Training | 0.8612 (25, 32,  7, 19) | 0.997 | 1.250 | 85.18 | 69.77 |-----------------------------------------------|
2             Validation | 0.8625 (25, 32,  7, 19) | 0.989 | 1.385 | 85.09 | 69.19 |#########################################| ^ (3.0%, 5.03, 0.9, 57%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset2_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

2 >> DONE <<
2 Minimum Loss = 0.861250
Tue Apr 26 04:57:58 2022
Using weight: mcPseudoTagWeight for classifier: FvT
Add true class labels to data
Add true class labels to ttbar MC
concatenate data and ttbar dataframes
add encoded target: d4=0, d3=1, t4=2, t3=3
2016 ---------------------------------------------
nd4 =   33303, wd4 =  33303.0, <w> = 1.000
nd3 = 1545810, wd3 =  75998.2, <w> = 0.049
nt4 =   34931, wt4 =   1488.9, <w> = 0.043
nt3 =  464056, wt3 =  10340.1, <w> = 0.022
2017 ---------------------------------------------
nd4 =   27207, wd4 =  27207.0, <w> = 1.000
nd3 =  642531, wd3 =  35456.4, <w> = 0.055
nt4 =   65736, wt4 =   2075.0, <w> = 0.032
nt3 =  693073, wt3 =   7105.7, <w> = 0.010
2018 ---------------------------------------------
nd4 =   36667, wd4 =  36667.0, <w> = 1.000
nd3 = 1090687, wd3 =  63725.1, <w> = 0.058
nt4 =   79211, wt4 =   2860.7, <w> = 0.036
nt3 =  760219, wt3 =  10223.0, <w> = 0.013
All ----------------------------------------------
nd4 =   97177, wd4 =  97177.0, <w> = 1.000
nd3 = 3279028, wd3 = 175179.7, <w> = 0.053
nt4 =  179878, wt4 =   6424.6, <w> = 0.036
nt3 = 1917348, wt3 =  27668.8, <w> = 0.014
wtn =   -155.6
Normalization = wd4_SB/(wd3_SB-wt3_SB+wt4_SB)
              = 97177/(106760-13238+3646)
              = 1.000 +/- 0.003 (0.006 validation stat uncertainty, norm should converge to about this precision)
loaded die loss inside SR: 0.590595
loaded die loss: 0.848897
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset2.log
Setup training/validation tensors
2 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >>  1/20 <<   Training | 0.8837 (27, 33,  7, 18) | 0.958 | 4.140 | 83.59 | 68.06 |------------------------------|
2             Validation | 0.8826 (27, 33,  7, 18) | 0.951 | 2.162 | 83.63 | 67.75 |###########################| ^ (1.8%, 2.58, 1.7, 2%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1781 batches)
2 >>  2/20 <<   Training | 0.8797 (28, 30,  8, 20) | 1.033 | 1.704 | 84.67 | 67.44 |------------------------|
2             Validation | 0.8785 (28, 30,  8, 20) | 1.025 | 1.388 | 84.66 | 67.18 |#####################| ^ (1.6%, 2.90, 1.7, 2%) 
2 >>  3/20 <<   Training | 0.8741 (28, 33,  7, 17) | 1.031 | 6.422 | 84.74 | 68.31 |---------------------------------|
2             Validation | 0.8730 (28, 33,  7, 17) | 1.023 | 3.321 | 84.75 | 68.09 |##############################| ^ (1.5%, 3.45, 2.5, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (890 batches)
2 >>  4/20 <<   Training | 0.8734 (29, 33,  7, 16) | 1.063 | 3.434 | 84.91 | 67.98 |-----------------------------|
2             Validation | 0.8730 (29, 33,  7, 16) | 1.054 | 1.854 | 84.92 | 67.64 |##########################| ^ (1.9%, 3.69, 2.3, 0%) 
2 >>  5/20 <<   Training | 0.8699 (26, 33,  7, 18) | 0.934 | 2.084 | 85.24 | 68.57 |-----------------------------------|
2             Validation | 0.8693 (26, 33,  7, 18) | 0.927 | 1.206 | 85.23 | 68.29 |################################| ^ (1.6%, 2.93, 1.8, 1%) 
2 >>  6/20 <<   Training | 0.8707 (27, 34,  7, 16) | 0.953 | 5.289 | 85.31 | 68.60 |------------------------------------|
2             Validation | 0.8706 (27, 35,  7, 16) | 0.946 | 3.191 | 85.30 | 68.24 |################################| ^ (2.0%, 4.75, 2.6, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (445 batches)
2 >>  7/20 <<   Training | 0.8732 (29, 34,  7, 15) | 1.067 | 2.122 | 85.24 | 68.64 |------------------------------------|
2             Validation | 0.8730 (29, 34,  7, 15) | 1.059 | 1.506 | 85.22 | 68.32 |#################################| ^ (1.8%, 3.88, 1.8, 1%) 
2 >>  8/20 <<   Training | 0.8708 (27, 35,  7, 16) | 0.982 | 2.869 | 85.24 | 68.70 |-------------------------------------|
2             Validation | 0.8701 (27, 35,  7, 16) | 0.975 | 1.340 | 85.23 | 68.45 |##################################| ^ (1.4%, 4.65, 1.6, 3%) 
2 >>  9/20 <<   Training | 0.8671 (28, 32,  7, 18) | 1.038 | 1.361 | 85.35 | 68.45 |----------------------------------|
2             Validation | 0.8665 (28, 32,  7, 18) | 1.030 | 1.090 | 85.33 | 68.14 |###############################| ^ (1.7%, 4.96, 1.5, 6%) 
2 >> 10/20 <<   Training | 0.8678 (27, 32,  7, 18) | 0.976 | 2.111 | 85.43 | 68.74 |-------------------------------------|
2             Validation | 0.8671 (27, 32,  7, 18) | 0.968 | 1.084 | 85.41 | 68.48 |##################################| ^ (1.5%, 3.29, 1.5, 4%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (222 batches)
2 >> 11/20 <<   Training | 0.8662 (28, 33,  7, 17) | 1.026 | 2.209 | 85.51 | 68.88 |--------------------------------------|
2             Validation | 0.8658 (28, 33,  7, 17) | 1.018 | 1.336 | 85.49 | 68.56 |###################################| ^ (1.8%, 4.53, 2.5, 0%) 
2 >> 12/20 <<   Training | 0.8667 (29, 32,  7, 17) | 1.043 | 2.315 | 85.47 | 68.56 |-----------------------------------|
2             Validation | 0.8663 (29, 32,  7, 17) | 1.035 | 1.397 | 85.45 | 68.23 |################################| ^ (1.9%, 5.04, 2.4, 0%) 
2 >> 13/20 <<   Training | 0.8656 (28, 32,  7, 18) | 1.015 | 0.797 | 85.52 | 68.96 |---------------------------------------|
2             Validation | 0.8651 (28, 32,  7, 18) | 1.007 | 0.575 | 85.51 | 68.70 |####################################| ^ (1.4%, 3.58, 1.8, 1%) 
2 >> 14/20 <<   Training | 0.8658 (26, 33,  7, 18) | 0.932 | 1.112 | 85.50 | 68.67 |------------------------------------|
2             Validation | 0.8655 (26, 33,  7, 18) | 0.924 | 1.171 | 85.49 | 68.38 |#################################| ^ (1.6%, 4.43, 2.1, 0%) 
2 >> 15/20 <<   Training | 0.8673 (26, 34,  7, 19) | 0.881 | 5.580 | 85.45 | 68.60 |------------------------------------|
2             Validation | 0.8670 (25, 34,  7, 19) | 0.874 | 3.667 | 85.43 | 68.33 |#################################| ^ (1.5%, 4.84, 2.7, 0%) 
Decay learning rate: 0.010000 -> 0.002500
2 >> 16/20 <<   Training | 0.8644 (28, 32,  7, 18) | 1.025 | 1.059 | 85.54 | 68.77 |-------------------------------------|
2             Validation | 0.8641 (28, 32,  7, 18) | 1.017 | 1.081 | 85.53 | 68.46 |##################################| ^ (1.7%, 4.49, 2.1, 0%) 
Decay learning rate: 0.002500 -> 0.000625
2 >> 17/20 <<   Training | 0.8641 (28, 33,  7, 17) | 0.998 | 0.765 | 85.58 | 68.89 |--------------------------------------|
2             Validation | 0.8638 (28, 33,  7, 17) | 0.990 | 1.098 | 85.56 | 68.59 |###################################| ^ (1.6%, 4.57, 2.0, 0%) 
Decay learning rate: 0.000625 -> 0.000156
2 >> 18/20 <<   Training | 0.8641 (28, 33,  7, 17) | 1.008 | 0.871 | 85.58 | 68.89 |--------------------------------------|
2             Validation | 0.8638 (28, 33,  7, 17) | 1.000 | 1.166 | 85.56 | 68.59 |###################################| ^ (1.7%, 4.51, 2.1, 0%) 
Decay learning rate: 0.000156 -> 0.000039
2 >> 19/20 <<   Training | 0.8640 (28, 33,  7, 17) | 1.001 | 0.733 | 85.58 | 68.88 |--------------------------------------|
2             Validation | 0.8637 (28, 33,  7, 17) | 0.993 | 0.953 | 85.56 | 68.58 |###################################| ^ (1.6%, 4.50, 2.0, 0%) 
Decay learning rate: 0.000039 -> 0.000010
2 >> 20/20 <<   Training | 0.8640 (28, 33,  7, 17) | 1.000 | 0.764 | 85.58 | 68.88 |--------------------------------------|
2             Validation | 0.8637 (28, 33,  7, 17) | 0.992 | 0.904 | 85.56 | 68.58 |###################################| ^ (1.6%, 4.50, 2.1, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset2_epoch20_before_finetuning.pkl
Run Finetuning
2 >> 20/20 <<   Training | 0.8640 (28, 33,  7, 17) | 0.998 | 0.753 | 85.58 | 68.89 |--------------------------------------|
2             Validation | 0.8637 (28, 33,  7, 17) | 0.990 | 0.933 | 85.56 | 68.58 |###################################| ^ (1.6%, 4.51, 2.1, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_seed0_lr0.01_epochs20_offset2_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

2 >> DONE <<
2 Minimum Loss = 0.864032
