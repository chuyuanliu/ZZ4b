Mon Apr 18 12:28:22 2022
Using weight: weight for classifier: SvB
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 2396428
wDB 114777.240743
wDBn -53.981695
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 128876
wT 4694.203751
wTn -16.779144
nS 645566
nB 2525304
sum_wS 201.702339
sum_wB 119579.407884
nzz =   68867, wzz =   71.9, wzz_SR =   59.2
nzh =  541264, wzh =   92.5, wzh_SR =   77.9
nhh =   35435, whh =   37.3, whh_SR =   33.4
sum_wS_SR 170.521446
sum_wB_SR 50599.001540
sum_wSp_SR 203.140887
sum_wSn_SR 32.619440
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 264.795507
sigScaleZH 394.679756
sigScaleHH 619.280794
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.192839
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset1.log
Setup training/validation tensors
1 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >>  1/20 <<   Training | 0.6663 (42, 10, 18, 31, 18) | 0.759 | 0.893 | 87.63 | 85.63 |----------------|
1             Validation | 0.6609 (42, 10, 17, 31, 19) | 0.814 | 0.957 | 88.44 | 85.76 |#################| ^ (0.5%, 11.13, 5.3, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1033 batches)
1 >>  2/20 <<   Training | 0.6507 (38, 11, 18, 33, 19) | 0.820 | 0.963 | 88.35 | 86.21 |----------------------|
1             Validation | 0.6469 (39, 11, 17, 33, 20) | 0.889 | 1.043 | 88.99 | 86.26 |######################| ^ (0.4%, 10.32, 4.0, 0%) 
1 >>  3/20 <<   Training | 0.6525 (40, 11, 19, 31, 19) | 0.759 | 0.896 | 88.35 | 85.82 |------------------|
1             Validation | 0.6475 (41, 11, 18, 31, 19) | 0.812 | 0.957 | 88.96 | 85.94 |###################| ^ (0.4%, 9.57, 4.5, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (516 batches)
1 >>  4/20 <<   Training | 0.6457 (38, 11, 17, 32, 21) | 0.891 | 1.048 | 88.69 | 86.53 |-------------------------|
1             Validation | 0.6429 (39, 10, 17, 32, 21) | 0.958 | 1.124 | 89.25 | 86.53 |#########################| ^ (0.2%, 11.51, 3.7, 0%) 
1 >>  5/20 <<   Training | 0.6444 (38, 11, 18, 32, 20) | 0.881 | 1.037 | 88.67 | 86.50 |-------------------------|
1             Validation | 0.6417 (39, 11, 17, 32, 21) | 0.941 | 1.106 | 89.30 | 86.48 |########################| ^ (0.2%, 11.17, 3.7, 0%) 
1 >>  6/20 <<   Training | 0.6443 (40, 11, 17, 31, 20) | 0.868 | 1.018 | 88.92 | 86.55 |-------------------------|
1             Validation | 0.6416 (41, 11, 16, 31, 20) | 0.933 | 1.094 | 89.39 | 86.56 |#########################| ^ (0.3%, 11.03, 4.6, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (258 batches)
1 >>  7/20 <<   Training | 0.6425 (39, 11, 17, 30, 21) | 0.906 | 1.066 | 89.00 | 86.66 |--------------------------|
1             Validation | 0.6402 (40, 11, 17, 30, 22) | 0.966 | 1.134 | 89.45 | 86.66 |##########################| ^ (0.3%, 11.96, 4.1, 0%) 
1 >>  8/20 <<   Training | 0.6438 (42, 11, 17, 30, 19) | 0.883 | 1.037 | 88.92 | 86.65 |--------------------------|
1             Validation | 0.6413 (42, 11, 16, 30, 20) | 0.950 | 1.114 | 89.40 | 86.65 |##########################| ^ (0.3%, 11.48, 4.2, 0%) 
1 >>  9/20 <<   Training | 0.6380 (39, 11, 18, 31, 20) | 0.905 | 1.064 | 88.92 | 86.68 |--------------------------|
1             Validation | 0.6356 (39, 11, 18, 31, 20) | 0.987 | 1.159 | 89.37 | 86.63 |##########################| ^ (0.3%, 11.78, 4.6, 0%) 
1 >> 10/20 <<   Training | 0.6436 (41, 11, 17, 32, 19) | 0.882 | 1.035 | 88.98 | 86.67 |--------------------------|
1             Validation | 0.6408 (41, 11, 16, 32, 19) | 0.931 | 1.092 | 89.68 | 86.67 |##########################| ^ (0.3%, 11.23, 4.4, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (129 batches)
1 >> 11/20 <<   Training | 0.6387 (41, 11, 18, 30, 19) | 0.862 | 1.013 | 89.06 | 86.57 |-------------------------|
1             Validation | 0.6359 (42, 11, 18, 30, 20) | 0.943 | 1.109 | 89.58 | 86.54 |#########################| ^ (0.4%, 10.92, 4.1, 0%) 
1 >> 12/20 <<   Training | 0.6419 (41, 11, 17, 30, 21) | 0.901 | 1.059 | 88.98 | 86.76 |---------------------------|
1             Validation | 0.6399 (41, 11, 16, 30, 21) | 0.974 | 1.142 | 89.48 | 86.73 |###########################| ^ (0.3%, 10.84, 4.1, 0%) 
1 >> 13/20 <<   Training | 0.6347 (36, 11, 19, 33, 20) | 0.891 | 1.046 | 88.92 | 86.63 |--------------------------|
1             Validation | 0.6316 (36, 11, 19, 33, 20) | 0.969 | 1.136 | 89.38 | 86.64 |##########################| ^ (0.4%, 11.60, 4.0, 0%) 
1 >> 14/20 <<   Training | 0.6381 (39, 11, 17, 32, 20) | 0.910 | 1.068 | 89.14 | 86.74 |---------------------------|
1             Validation | 0.6354 (39, 11, 17, 32, 21) | 0.985 | 1.155 | 89.72 | 86.72 |###########################| ^ (0.3%, 10.98, 4.2, 0%) 
1 >> 15/20 <<   Training | 0.6388 (36, 11, 17, 33, 22) | 0.934 | 1.096 | 89.11 | 86.80 |----------------------------|
1             Validation | 0.6364 (37, 11, 17, 34, 22) | 1.008 | 1.182 | 89.64 | 86.76 |###########################| ^ (0.3%, 11.12, 4.0, 0%) 
Decay learning rate: 0.010000 -> 0.002500
1 >> 16/20 <<   Training | 0.6370 (38, 11, 17, 32, 21) | 0.925 | 1.086 | 89.18 | 86.80 |----------------------------|
1             Validation | 0.6345 (39, 11, 17, 32, 21) | 0.996 | 1.168 | 89.76 | 86.78 |###########################| ^ (0.3%, 11.08, 4.3, 0%) 
Decay learning rate: 0.002500 -> 0.000625
1 >> 17/20 <<   Training | 0.6359 (38, 11, 18, 32, 20) | 0.922 | 1.082 | 89.16 | 86.80 |---------------------------|
1             Validation | 0.6333 (39, 11, 17, 32, 21) | 1.002 | 1.175 | 89.73 | 86.77 |###########################| ^ (0.3%, 11.07, 4.3, 0%) 
Decay learning rate: 0.000625 -> 0.000156
1 >> 18/20 <<   Training | 0.6357 (38, 11, 18, 32, 21) | 0.924 | 1.084 | 89.16 | 86.80 |----------------------------|
1             Validation | 0.6331 (38, 11, 17, 32, 21) | 1.007 | 1.180 | 89.71 | 86.77 |###########################| ^ (0.3%, 11.10, 4.1, 0%) 
Decay learning rate: 0.000156 -> 0.000039
1 >> 19/20 <<   Training | 0.6357 (38, 11, 18, 32, 21) | 0.925 | 1.086 | 89.16 | 86.80 |----------------------------|
1             Validation | 0.6332 (38, 11, 17, 32, 21) | 1.007 | 1.180 | 89.71 | 86.77 |###########################| ^ (0.3%, 11.12, 4.1, 0%) 
Decay learning rate: 0.000039 -> 0.000010
1 >> 20/20 <<   Training | 0.6358 (38, 11, 18, 32, 21) | 0.925 | 1.086 | 89.16 | 86.80 |----------------------------|
1             Validation | 0.6332 (38, 11, 17, 32, 21) | 1.006 | 1.179 | 89.71 | 86.78 |###########################| ^ (0.3%, 11.12, 4.2, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset1_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

1 >> DONE <<
1 Minimum Loss = 0.634740
