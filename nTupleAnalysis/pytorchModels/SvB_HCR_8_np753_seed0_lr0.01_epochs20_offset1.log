Mon Apr 18 12:28:22 2022
Using weight: weight for classifier: SvB
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 2396428
wDB 114777.240743
wDBn -53.981695
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 128876
wT 4694.203751
wTn -16.779144
nS 645566
nB 2525304
sum_wS 201.702339
sum_wB 119579.407884
nzz =   68867, wzz =   71.9, wzz_SR =   59.2
nzh =  541264, wzh =   92.5, wzh_SR =   77.9
nhh =   35435, whh =   37.3, whh_SR =   33.4
sum_wS_SR 170.521446
sum_wB_SR 50599.001540
sum_wSp_SR 203.140887
sum_wSn_SR 32.619440
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 264.795507
sigScaleZH 394.679756
sigScaleHH 619.280794
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.192839
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset1.log
Setup training/validation tensors
1 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >>  1/20 <<   Training | 0.6663 (42, 10, 18, 31, 18) | 0.759 | 0.893 | 87.63 | 85.63 |----------------|
1             Validation | 0.6609 (42, 10, 17, 31, 19) | 0.814 | 0.957 | 88.44 | 85.76 |#################| ^ (0.5%, 11.13, 5.3, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1033 batches)
1 >>  2/20 <<   Training | 0.6507 (38, 11, 18, 33, 19) | 0.820 | 0.963 | 88.35 | 86.21 |----------------------|
1             Validation | 0.6469 (39, 11, 17, 33, 20) | 0.889 | 1.043 | 88.99 | 86.26 |######################| ^ (0.4%, 10.32, 4.0, 0%) 
1 >>  3/20 <<   Training | 0.6525 (40, 11, 19, 31, 19) | 0.759 | 0.896 | 88.35 | 85.82 |------------------|
1             Validation | 0.6475 (41, 11, 18, 31, 19) | 0.812 | 0.957 | 88.96 | 85.94 |###################| ^ (0.4%, 9.57, 4.5, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (516 batches)
1 >>  4/20 <<   Training | 0.6457 (38, 11, 17, 32, 21) | 0.891 | 1.048 | 88.69 | 86.53 |-------------------------|
1             Validation | 0.6429 (39, 10, 17, 32, 21) | 0.958 | 1.124 | 89.25 | 86.53 |#########################| ^ (0.2%, 11.51, 3.7, 0%) 
1 >>  5/20 <<   Training | 0.6444 (38, 11, 18, 32, 20) | 0.881 | 1.037 | 88.67 | 86.50 |-------------------------|
1             Validation | 0.6417 (39, 11, 17, 32, 21) | 0.941 | 1.106 | 89.30 | 86.48 |########################| ^ (0.2%, 11.17, 3.7, 0%) 
1 >>  6/20 <<   Training | 0.6443 (40, 11, 17, 31, 20) | 0.868 | 1.018 | 88.92 | 86.55 |-------------------------|
1             Validation | 0.6416 (41, 11, 16, 31, 20) | 0.933 | 1.094 | 89.39 | 86.56 |#########################| ^ (0.3%, 11.03, 4.6, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (258 batches)
1 >>  7/20 <<   Training | 0.6425 (39, 11, 17, 30, 21) | 0.906 | 1.066 | 89.00 | 86.66 |--------------------------|
1             Validation | 0.6402 (40, 11, 17, 30, 22) | 0.966 | 1.134 | 89.45 | 86.66 |##########################| ^ (0.3%, 11.96, 4.1, 0%) 
1 >>  8/20 <<   Training | 0.6438 (42, 11, 17, 30, 19) | 0.883 | 1.037 | 88.92 | 86.65 |--------------------------|
1             Validation | 0.6413 (42, 11, 16, 30, 20) | 0.950 | 1.114 | 89.40 | 86.65 |##########################| ^ (0.3%, 11.48, 4.2, 0%) 
1 >>  9/20 <<   Training | 0.6380 (39, 11, 18, 31, 20) | 0.905 | 1.064 | 88.92 | 86.68 |--------------------------|
1             Validation | 0.6356 (39, 11, 18, 31, 20) | 0.987 | 1.159 | 89.37 | 86.63 |##########################| ^ (0.3%, 11.78, 4.6, 0%) 
1 >> 10/20 <<   Training | 0.6436 (41, 11, 17, 32, 19) | 0.882 | 1.035 | 88.98 | 86.67 |--------------------------|
1             Validation | 0.6408 (41, 11, 16, 32, 19) | 0.931 | 1.092 | 89.68 | 86.67 |##########################| ^ (0.3%, 11.23, 4.4, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (129 batches)
1 >> 11/20 <<   Training | 0.6387 (41, 11, 18, 30, 19) | 0.862 | 1.013 | 89.06 | 86.57 |-------------------------|
1             Validation | 0.6359 (42, 11, 18, 30, 20) | 0.943 | 1.109 | 89.58 | 86.54 |#########################| ^ (0.4%, 10.92, 4.1, 0%) 
1 >> 12/20 <<   Training | 0.6419 (41, 11, 17, 30, 21) | 0.901 | 1.059 | 88.98 | 86.76 |---------------------------|
1             Validation | 0.6399 (41, 11, 16, 30, 21) | 0.974 | 1.142 | 89.48 | 86.73 |###########################| ^ (0.3%, 10.84, 4.1, 0%) 
1 >> 13/20 <<   Training | 0.6347 (36, 11, 19, 33, 20) | 0.891 | 1.046 | 88.92 | 86.63 |--------------------------|
1             Validation | 0.6316 (36, 11, 19, 33, 20) | 0.969 | 1.136 | 89.38 | 86.64 |##########################| ^ (0.4%, 11.60, 4.0, 0%) 
1 >> 14/20 <<   Training | 0.6381 (39, 11, 17, 32, 20) | 0.910 | 1.068 | 89.14 | 86.74 |---------------------------|
1             Validation | 0.6354 (39, 11, 17, 32, 21) | 0.985 | 1.155 | 89.72 | 86.72 |###########################| ^ (0.3%, 10.98, 4.2, 0%) 
1 >> 15/20 <<   Training | 0.6388 (36, 11, 17, 33, 22) | 0.934 | 1.096 | 89.11 | 86.80 |----------------------------|
1             Validation | 0.6364 (37, 11, 17, 34, 22) | 1.008 | 1.182 | 89.64 | 86.76 |###########################| ^ (0.3%, 11.12, 4.0, 0%) 
Decay learning rate: 0.010000 -> 0.002500
1 >> 16/20 <<   Training | 0.6370 (38, 11, 17, 32, 21) | 0.925 | 1.086 | 89.18 | 86.80 |----------------------------|
1             Validation | 0.6345 (39, 11, 17, 32, 21) | 0.996 | 1.168 | 89.76 | 86.78 |###########################| ^ (0.3%, 11.08, 4.3, 0%) 
Decay learning rate: 0.002500 -> 0.000625
1 >> 17/20 <<   Training | 0.6359 (38, 11, 18, 32, 20) | 0.922 | 1.082 | 89.16 | 86.80 |---------------------------|
1             Validation | 0.6333 (39, 11, 17, 32, 21) | 1.002 | 1.175 | 89.73 | 86.77 |###########################| ^ (0.3%, 11.07, 4.3, 0%) 
Decay learning rate: 0.000625 -> 0.000156
1 >> 18/20 <<   Training | 0.6357 (38, 11, 18, 32, 21) | 0.924 | 1.084 | 89.16 | 86.80 |----------------------------|
1             Validation | 0.6331 (38, 11, 17, 32, 21) | 1.007 | 1.180 | 89.71 | 86.77 |###########################| ^ (0.3%, 11.10, 4.1, 0%) 
Decay learning rate: 0.000156 -> 0.000039
1 >> 19/20 <<   Training | 0.6357 (38, 11, 18, 32, 21) | 0.925 | 1.086 | 89.16 | 86.80 |----------------------------|
1             Validation | 0.6332 (38, 11, 17, 32, 21) | 1.007 | 1.180 | 89.71 | 86.77 |###########################| ^ (0.3%, 11.12, 4.1, 0%) 
Decay learning rate: 0.000039 -> 0.000010
1 >> 20/20 <<   Training | 0.6358 (38, 11, 18, 32, 21) | 0.925 | 1.086 | 89.16 | 86.80 |----------------------------|
1             Validation | 0.6332 (38, 11, 17, 32, 21) | 1.006 | 1.179 | 89.71 | 86.78 |###########################| ^ (0.3%, 11.12, 4.2, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset1_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

1 >> DONE <<
1 Minimum Loss = 0.634740
Mon May  9 12:02:42 2022
Using weight: weight for classifier: SvB
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 3279028
wDB 147993.557827
wDBn -59.208815
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 179878
wT 6758.331864
wTn -23.581879
nS 940327
nB 3458906
sum_wS 167.472288
sum_wB 154870.307320
nzz =  323399, wzz =   57.4, wzz_SR =   46.2
nzh =  574169, wzh =   74.7, wzh_SR =   61.9
nhh =   42759, whh =   35.4, whh_SR =   31.2
sum_wS_SR 139.340355
sum_wB_SR 57305.889757
sum_wSp_SR 164.693187
sum_wSn_SR 25.352832
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 295.764663
sigScaleZH 457.493368
sigScaleHH 679.928873
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.099681
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset1.log
Setup training/validation tensors
1 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) |  S/B   | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >>  1/20 <<   Training | 0.6480 (35, 12, 15, 29, 19) |  2/ 16 | 0.881 | 87.75 | 87.42 |----------------------------------|
1             Validation | 0.6500 (35, 12, 15, 29, 19) |  2/ 15 | 0.892 | 87.62 | 87.41 |##################################| ^ (0.2%, 8.73, 9.2, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1432 batches)
1 >>  2/20 <<   Training | 0.6307 (34, 13, 16, 27, 20) |  5/ 66 | 0.922 | 88.43 | 88.15 |-----------------------------------------|
1             Validation | 0.6338 (34, 13, 16, 27, 20) |  4/ 53 | 0.933 | 88.23 | 88.07 |########################################| ^ (0.2%, 8.94, 7.4, 0%) 
1 >>  3/20 <<   Training | 0.6325 (37, 13, 15, 28, 17) |  9/198 | 0.855 | 88.73 | 88.14 |-----------------------------------------|
1             Validation | 0.6353 (37, 13, 15, 28, 17) |  9/190 | 0.862 | 88.57 | 88.07 |########################################| ^ (0.2%, 8.91, 10.6, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (716 batches)
1 >>  4/20 <<   Training | 0.6224 (33, 13, 16, 28, 20) |  9/148 | 0.958 | 88.56 | 88.40 |-------------------------------------------|
1             Validation | 0.6255 (33, 12, 17, 28, 20) |  9/135 | 0.974 | 88.33 | 88.33 |###########################################| ^ (0.2%, 9.30, 11.0, 0%) 
1 >>  5/20 <<   Training | 0.6255 (34, 13, 15, 27, 21) | 10/154 | 0.985 | 88.92 | 88.46 |--------------------------------------------|
1             Validation | 0.6289 (34, 13, 15, 27, 21) |  9/138 | 0.997 | 88.72 | 88.38 |###########################################| ^ (0.2%, 9.56, 9.3, 0%) 
1 >>  6/20 <<   Training | 0.6248 (33, 13, 16, 28, 20) |  8/115 | 0.973 | 88.98 | 88.32 |-------------------------------------------|
1             Validation | 0.6272 (33, 13, 16, 28, 21) |  8/101 | 0.989 | 88.84 | 88.28 |##########################################| ^ (0.2%, 9.59, 11.2, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (358 batches)
1 >>  7/20 <<   Training | 0.6221 (33, 13, 16, 28, 20) |  7/113 | 0.972 | 88.52 | 88.48 |--------------------------------------------|
1             Validation | 0.6252 (33, 13, 16, 28, 20) |  7/ 99 | 0.985 | 88.34 | 88.39 |###########################################| ^ (0.2%, 9.77, 10.5, 0%) 
1 >>  8/20 <<   Training | 0.6200 (34, 13, 16, 28, 20) |  9/149 | 0.990 | 89.04 | 88.54 |---------------------------------------------|
1             Validation | 0.6231 (34, 13, 16, 28, 20) |  9/134 | 1.000 | 88.82 | 88.48 |############################################| ^ (0.2%, 9.92, 10.9, 0%) 
1 >>  9/20 <<   Training | 0.6220 (35, 13, 15, 28, 19) | 11/207 | 0.936 | 89.05 | 88.49 |--------------------------------------------|
1             Validation | 0.6250 (35, 13, 15, 28, 19) | 11/188 | 0.952 | 88.84 | 88.42 |############################################| ^ (0.2%, 9.74, 10.4, 0%) 
1 >> 10/20 <<   Training | 0.6244 (35, 13, 15, 27, 20) | 10/175 | 0.964 | 89.09 | 88.54 |---------------------------------------------|
1             Validation | 0.6278 (35, 13, 15, 27, 20) | 10/163 | 0.970 | 88.92 | 88.45 |############################################| ^ (0.3%, 9.27, 10.5, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (179 batches)
1 >> 11/20 <<   Training | 0.6217 (35, 13, 15, 28, 19) |  9/161 | 0.968 | 89.12 | 88.56 |---------------------------------------------|
1             Validation | 0.6249 (35, 13, 15, 28, 19) |  9/143 | 0.978 | 88.91 | 88.49 |############################################| ^ (0.2%, 9.81, 12.5, 0%) 
1 >> 12/20 <<   Training | 0.6191 (33, 13, 16, 29, 20) |  9/132 | 1.008 | 89.10 | 88.57 |---------------------------------------------|
1             Validation | 0.6222 (33, 12, 16, 29, 20) |  9/125 | 1.012 | 88.87 | 88.50 |#############################################| ^ (0.2%, 10.08, 9.9, 0%) 
1 >> 13/20 <<   Training | 0.6198 (35, 13, 16, 28, 19) | 11/203 | 0.961 | 88.93 | 88.55 |---------------------------------------------|
1             Validation | 0.6228 (35, 13, 16, 28, 19) | 11/185 | 0.971 | 88.72 | 88.50 |############################################| ^ (0.2%, 10.42, 9.9, 0%) 
1 >> 14/20 <<   Training | 0.6213 (35, 13, 15, 28, 19) | 10/181 | 0.965 | 89.19 | 88.57 |---------------------------------------------|
1             Validation | 0.6246 (35, 13, 15, 28, 19) | 10/166 | 0.971 | 88.98 | 88.50 |############################################| ^ (0.2%, 10.09, 10.0, 0%) 
1 >> 15/20 <<   Training | 0.6198 (34, 13, 16, 28, 19) | 10/183 | 0.969 | 89.20 | 88.47 |--------------------------------------------|
1             Validation | 0.6226 (34, 13, 16, 28, 19) | 10/174 | 0.977 | 89.00 | 88.43 |############################################| ^ (0.2%, 10.43, 10.4, 0%) 
Decay learning rate: 0.010000 -> 0.002500
1 >> 16/20 <<   Training | 0.6182 (34, 13, 16, 29, 19) |  9/137 | 0.988 | 89.18 | 88.58 |---------------------------------------------|
1             Validation | 0.6214 (34, 13, 16, 29, 19) |  9/124 | 1.001 | 88.98 | 88.51 |#############################################| ^ (0.2%, 10.36, 10.7, 0%) 
Decay learning rate: 0.002500 -> 0.000625
1 >> 17/20 <<   Training | 0.6185 (34, 13, 15, 28, 19) | 10/173 | 0.978 | 89.18 | 88.58 |---------------------------------------------|
1             Validation | 0.6217 (34, 13, 16, 28, 19) | 10/158 | 0.991 | 88.97 | 88.51 |#############################################| ^ (0.2%, 10.44, 10.5, 0%) 
Decay learning rate: 0.000625 -> 0.000156
1 >> 18/20 <<   Training | 0.6185 (34, 13, 15, 28, 19) | 10/166 | 0.981 | 89.19 | 88.59 |---------------------------------------------|
1             Validation | 0.6217 (34, 13, 16, 28, 19) | 10/153 | 0.993 | 88.99 | 88.52 |#############################################| ^ (0.2%, 10.43, 10.1, 0%) 
Decay learning rate: 0.000156 -> 0.000039
1 >> 19/20 <<   Training | 0.6184 (34, 13, 15, 28, 19) |  9/154 | 0.986 | 89.20 | 88.60 |---------------------------------------------|
1             Validation | 0.6216 (34, 13, 16, 28, 19) |  9/140 | 1.000 | 88.99 | 88.53 |#############################################| ^ (0.2%, 10.41, 10.1, 0%) 
Decay learning rate: 0.000039 -> 0.000010
1 >> 20/20 <<   Training | 0.6185 (34, 13, 15, 28, 19) |  9/156 | 0.985 | 89.20 | 88.60 |----------------------------------------------|
1             Validation | 0.6217 (34, 13, 16, 28, 19) |  9/142 | 1.000 | 88.99 | 88.53 |#############################################| ^ (0.2%, 10.41, 10.2, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_8_np753_seed0_lr0.01_epochs20_offset1_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

1 >> DONE <<
1 Minimum Loss = 0.618247
