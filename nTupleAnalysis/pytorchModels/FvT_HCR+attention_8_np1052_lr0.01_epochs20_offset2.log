2 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >>  1/20 <<   Training | 0.9522 (22, 28,  7, 19) | 0.972 | 8.933 | 82.81 | 67.05 |--------------------|
2             Validation | 0.9500 (21, 28,  7, 19) | 0.971 | 4.030 | 82.88 | 67.34 |#######################| ^ (1.7%, 2.88, 0.9, 57%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1269 batches)
2 >>  2/20 <<   Training | 0.9439 (21, 30,  7, 18) | 0.965 | 3.266 | 83.67 | 68.44 |----------------------------------|
2             Validation | 0.9424 (21, 30,  7, 18) | 0.964 | 2.258 | 83.71 | 68.68 |####################################| ^ (1.3%, 2.75, 1.6, 3%) 
2 >>  3/20 <<   Training | 0.9401 (22, 28,  6, 18) | 1.007 | 10.429 | 83.89 | 69.27 |------------------------------------------|
2             Validation | 0.9388 (22, 28,  6, 18) | 1.007 | 6.143 | 83.92 | 69.46 |############################################| ^ (1.2%, 3.78, 1.5, 4%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (634 batches)
2 >>  4/20 <<   Training | 0.9376 (21, 28,  7, 19) | 0.989 | 1.304 | 83.90 | 69.14 |-----------------------------------------|
2             Validation | 0.9363 (21, 28,  7, 19) | 0.989 | 1.451 | 83.92 | 69.37 |###########################################| ^ (1.5%, 3.31, 1.5, 5%) 
2 >>  5/20 <<   Training | 0.9398 (22, 26,  7, 20) | 1.019 | 2.944 | 84.03 | 68.96 |---------------------------------------|
2             Validation | 0.9385 (22, 26,  7, 20) | 1.019 | 2.277 | 84.05 | 69.14 |#########################################| ^ (1.1%, 2.99, 1.5, 5%) 
2 >>  6/20 <<   Training | 0.9383 (20, 29,  7, 19) | 0.862 | 21.526 | 84.19 | 69.50 |--------------------------------------------|
2             Validation | 0.9374 (20, 29,  7, 19) | 0.861 | 11.399 | 84.20 | 69.59 |#############################################| ^ (0.9%, 4.17, 1.7, 1%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (317 batches)
2 >>  7/20 <<   Training | 0.9347 (22, 29,  7, 18) | 0.999 | 1.319 | 84.29 | 69.56 |---------------------------------------------|
2             Validation | 0.9338 (22, 29,  7, 18) | 0.998 | 1.488 | 84.31 | 69.73 |###############################################| ^ (1.3%, 4.02, 1.2, 19%) 
2 >>  8/20 <<   Training | 0.9359 (22, 29,  7, 18) | 1.032 | 4.615 | 84.20 | 68.75 |-------------------------------------|
2             Validation | 0.9351 (22, 29,  7, 18) | 1.032 | 2.777 | 84.22 | 68.87 |######################################| ^ (1.0%, 4.04, 1.9, 1%) 
2 >>  9/20 <<   Training | 0.9340 (21, 28,  7, 19) | 0.998 | 1.119 | 84.22 | 68.54 |-----------------------------------|
2             Validation | 0.9328 (21, 28,  7, 19) | 0.998 | 1.385 | 84.25 | 68.70 |####################################| ^ (1.3%, 3.36, 2.0, 0%) 
2 >> 10/20 <<   Training | 0.9319 (21, 29,  7, 19) | 0.972 | 1.753 | 84.34 | 68.92 |---------------------------------------|
2             Validation | 0.9313 (21, 29,  7, 19) | 0.971 | 1.791 | 84.35 | 68.99 |#######################################| ^ (1.0%, 3.54, 1.5, 6%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (158 batches)
2 >> 11/20 <<   Training | 0.9334 (23, 28,  6, 18) | 1.113 | 10.978 | 84.43 | 68.91 |---------------------------------------|
2             Validation | 0.9329 (23, 28,  6, 18) | 1.112 | 5.990 | 84.44 | 68.98 |#######################################| ^ (0.8%, 3.31, 1.6, 3%) 
2 >> 12/20 <<   Training | 0.9360 (21, 31,  6, 17) | 0.949 | 3.443 | 84.42 | 69.13 |-----------------------------------------|
2             Validation | 0.9356 (21, 31,  6, 17) | 0.948 | 2.181 | 84.43 | 69.22 |##########################################| ^ (0.8%, 3.64, 0.8, 60%) 
2 >> 13/20 <<   Training | 0.9323 (21, 30,  7, 18) | 0.943 | 4.064 | 84.44 | 69.53 |---------------------------------------------|
2             Validation | 0.9318 (21, 30,  7, 18) | 0.942 | 2.474 | 84.45 | 69.65 |##############################################| ^ (1.0%, 3.19, 1.2, 21%) 
2 >> 14/20 <<   Training | 0.9321 (20, 30,  7, 18) | 0.912 | 8.300 | 84.45 | 69.35 |-------------------------------------------|
2             Validation | 0.9319 (20, 30,  7, 18) | 0.911 | 4.719 | 84.44 | 69.39 |###########################################| ^ (0.8%, 3.23, 1.5, 5%) 
2 >> 15/20 <<   Training | 0.9324 (21, 28,  7, 20) | 0.965 | 3.118 | 84.44 | 69.09 |----------------------------------------|
2             Validation | 0.9320 (21, 28,  7, 20) | 0.964 | 2.528 | 84.45 | 69.11 |#########################################| ^ (0.8%, 3.20, 1.1, 30%) 
Decay learning rate: 0.010000 -> 0.002500
2 >> 16/20 <<   Training | 0.9302 (22, 28,  7, 19) | 1.018 | 1.260 | 84.52 | 69.34 |-------------------------------------------|
2             Validation | 0.9299 (22, 28,  7, 19) | 1.017 | 0.998 | 84.53 | 69.36 |###########################################| ^ (0.9%, 3.41, 1.0, 37%) 
Decay learning rate: 0.002500 -> 0.000625
2 >> 17/20 <<   Training | 0.9299 (22, 28,  7, 19) | 0.998 | 0.945 | 84.51 | 69.42 |--------------------------------------------|
2             Validation | 0.9296 (22, 28,  7, 19) | 0.998 | 1.280 | 84.51 | 69.45 |############################################| ^ (1.0%, 3.66, 1.4, 10%) 
Decay learning rate: 0.000625 -> 0.000156
2 >> 18/20 <<   Training | 0.9298 (22, 28,  7, 19) | 0.999 | 0.929 | 84.51 | 69.42 |--------------------------------------------|
2             Validation | 0.9295 (22, 28,  7, 19) | 0.998 | 1.162 | 84.51 | 69.45 |############################################| ^ (0.9%, 3.62, 1.2, 22%) 
Decay learning rate: 0.000156 -> 0.000039
2 >> 19/20 <<   Training | 0.9298 (22, 28,  7, 19) | 0.997 | 0.908 | 84.51 | 69.43 |--------------------------------------------|
2             Validation | 0.9295 (22, 28,  7, 19) | 0.996 | 1.142 | 84.51 | 69.46 |############################################| ^ (0.9%, 3.62, 1.2, 17%) 
Decay learning rate: 0.000039 -> 0.000010
2 >> 20/20 <<   Training | 0.9298 (22, 28,  7, 19) | 0.997 | 0.971 | 84.51 | 69.43 |--------------------------------------------|
2             Validation | 0.9295 (22, 28,  7, 19) | 0.997 | 1.129 | 84.51 | 69.46 |############################################| ^ (0.9%, 3.63, 1.2, 19%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_lr0.01_epochs20_offset2_epoch20_before_finetuning.pkl
Run Finetuning
2 >> 20/20 <<   Training | 0.9298 (22, 28,  7, 19) | 0.996 | 1.061 | 84.51 | 69.43 |--------------------------------------------|
2             Validation | 0.9295 (22, 28,  7, 19) | 0.996 | 1.058 | 84.51 | 69.46 |############################################| ^ (0.9%, 3.63, 1.3, 14%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_lr0.01_epochs20_offset2_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
2 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >>  1/20 <<   Training | 0.8859 (27, 28,  7, 21) | 1.216 | 28.921 | 82.75 | 66.57 |---------------|
2             Validation | 0.8857 (27, 28,  7, 20) | 1.230 | 17.874 | 82.79 | 66.72 |#################| ^ (1.3%, 4.72, 1.6, 3%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1216 batches)
2 >>  2/20 <<   Training | 0.8783 (23, 30,  8, 22) | 0.907 | 9.529 | 83.52 | 67.68 |--------------------------|
2             Validation | 0.8777 (23, 30,  8, 22) | 0.918 | 4.060 | 83.55 | 67.79 |###########################| ^ (0.8%, 3.31, 1.6, 3%) 
2 >>  3/20 <<   Training | 0.8706 (25, 31,  7, 20) | 1.014 | 5.294 | 83.87 | 68.12 |-------------------------------|
2             Validation | 0.8705 (25, 31,  7, 20) | 1.026 | 3.309 | 83.89 | 68.21 |################################| ^ (0.9%, 3.33, 2.0, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (608 batches)
2 >>  4/20 <<   Training | 0.8677 (24, 31,  7, 20) | 0.984 | 2.358 | 84.15 | 68.61 |------------------------------------|
2             Validation | 0.8672 (24, 31,  7, 20) | 0.996 | 1.637 | 84.17 | 68.78 |#####################################| ^ (1.0%, 3.29, 1.4, 9%) 
2 >>  5/20 <<   Training | 0.8662 (25, 31,  7, 20) | 1.036 | 3.398 | 84.10 | 68.89 |--------------------------------------|
2             Validation | 0.8661 (25, 31,  7, 20) | 1.048 | 2.544 | 84.12 | 68.97 |#######################################| ^ (0.6%, 3.36, 2.0, 0%) 
2 >>  6/20 <<   Training | 0.8674 (22, 32,  8, 20) | 0.857 | 23.338 | 84.28 | 68.96 |---------------------------------------|
2             Validation | 0.8670 (22, 32,  8, 20) | 0.868 | 10.471 | 84.29 | 69.01 |########################################| ^ (0.5%, 4.12, 1.4, 9%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (304 batches)
2 >>  7/20 <<   Training | 0.8647 (25, 32,  7, 18) | 1.026 | 1.470 | 84.45 | 68.84 |--------------------------------------|
2             Validation | 0.8649 (25, 32,  7, 18) | 1.038 | 1.376 | 84.47 | 68.86 |######################################| ^ (0.8%, 3.77, 1.9, 1%) 
2 >>  8/20 <<   Training | 0.8633 (23, 32,  7, 20) | 0.940 | 4.101 | 84.47 | 69.05 |----------------------------------------|
2             Validation | 0.8636 (24, 32,  7, 19) | 0.951 | 2.384 | 84.47 | 69.02 |########################################| ^ (0.6%, 4.01, 1.5, 5%) 
2 >>  9/20 <<   Training | 0.8697 (24, 28,  8, 23) | 1.043 | 2.411 | 84.57 | 68.01 |------------------------------|
2             Validation | 0.8704 (25, 28,  8, 22) | 1.055 | 2.173 | 84.57 | 67.90 |#############################| ^ (0.9%, 3.45, 1.4, 8%) 
2 >> 10/20 <<   Training | 0.8662 (24, 34,  7, 18) | 0.941 | 8.577 | 84.40 | 68.91 |---------------------------------------|
2             Validation | 0.8667 (24, 34,  7, 18) | 0.953 | 4.269 | 84.40 | 68.86 |######################################| ^ (0.8%, 4.04, 1.6, 3%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (152 batches)
2 >> 11/20 <<   Training | 0.8612 (24, 31,  7, 21) | 0.975 | 1.717 | 84.66 | 68.85 |--------------------------------------|
2             Validation | 0.8618 (24, 31,  7, 20) | 0.987 | 1.187 | 84.66 | 68.76 |#####################################| ^ (0.8%, 4.03, 1.4, 9%) 
2 >> 12/20 <<   Training | 0.8636 (26, 28,  7, 20) | 1.122 | 10.256 | 84.63 | 68.76 |-------------------------------------|
2             Validation | 0.8647 (26, 28,  7, 20) | 1.135 | 6.317 | 84.62 | 68.63 |####################################| ^ (1.0%, 4.36, 2.2, 0%) 
2 >> 13/20 <<   Training | 0.8653 (22, 35,  7, 19) | 0.812 | 41.596 | 84.60 | 69.15 |-----------------------------------------|
2             Validation | 0.8650 (22, 34,  7, 19) | 0.821 | 19.804 | 84.60 | 69.22 |##########################################| ^ (0.7%, 4.21, 1.7, 2%) 
2 >> 14/20 <<   Training | 0.8618 (26, 31,  7, 19) | 1.083 | 5.206 | 84.67 | 68.91 |---------------------------------------|
2             Validation | 0.8629 (26, 31,  7, 19) | 1.096 | 3.574 | 84.67 | 68.79 |#####################################| ^ (1.0%, 4.17, 1.4, 9%) 
2 >> 15/20 <<   Training | 0.8631 (25, 32,  7, 18) | 1.020 | 1.454 | 84.71 | 69.31 |-------------------------------------------|
2             Validation | 0.8639 (25, 32,  7, 18) | 1.033 | 1.325 | 84.70 | 69.21 |##########################################| ^ (1.0%, 3.81, 1.8, 1%) 
Decay learning rate: 0.010000 -> 0.002500
2 >> 16/20 <<   Training | 0.8598 (24, 31,  7, 20) | 1.012 | 1.279 | 84.77 | 69.15 |-----------------------------------------|
2             Validation | 0.8608 (25, 31,  7, 20) | 1.024 | 1.369 | 84.75 | 69.01 |########################################| ^ (1.1%, 3.90, 1.3, 16%) 
Decay learning rate: 0.002500 -> 0.000625
2 >> 17/20 <<   Training | 0.8597 (24, 32,  7, 19) | 1.002 | 0.881 | 84.76 | 69.25 |------------------------------------------|
2             Validation | 0.8606 (25, 31,  7, 19) | 1.013 | 0.983 | 84.75 | 69.12 |#########################################| ^ (1.0%, 4.08, 1.5, 4%) 
Decay learning rate: 0.000625 -> 0.000156
2 >> 18/20 <<   Training | 0.8595 (24, 31,  7, 20) | 1.007 | 0.780 | 84.76 | 69.23 |------------------------------------------|
2             Validation | 0.8605 (25, 31,  7, 20) | 1.019 | 1.124 | 84.75 | 69.10 |#########################################| ^ (1.0%, 4.06, 1.3, 13%) 
Decay learning rate: 0.000156 -> 0.000039
2 >> 19/20 <<   Training | 0.8595 (24, 31,  7, 20) | 1.003 | 0.826 | 84.76 | 69.23 |------------------------------------------|
2             Validation | 0.8604 (25, 31,  7, 20) | 1.015 | 1.146 | 84.74 | 69.10 |########################################| ^ (1.0%, 4.06, 1.3, 14%) 
Decay learning rate: 0.000039 -> 0.000010
2 >> 20/20 <<   Training | 0.8595 (24, 31,  7, 20) | 1.002 | 0.860 | 84.76 | 69.23 |------------------------------------------|
2             Validation | 0.8604 (25, 31,  7, 20) | 1.014 | 1.052 | 84.74 | 69.10 |########################################| ^ (1.0%, 4.06, 1.3, 13%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_lr0.01_epochs20_offset2_epoch20_before_finetuning.pkl
Run Finetuning
2 >> 20/20 <<   Training | 0.8595 (24, 31,  7, 20) | 1.004 | 0.987 | 84.76 | 69.23 |------------------------------------------|
2             Validation | 0.8604 (25, 31,  7, 20) | 1.015 | 1.102 | 84.74 | 69.10 |########################################| ^ (1.0%, 4.06, 1.3, 14%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_lr0.01_epochs20_offset2_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
