0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.9540 (23, 25,  7, 21) | 1.124 | 20.475 | 83.40 | 67.52 |-------------------------|
0             Validation | 0.9545 (22, 25,  7, 21) | 1.114 | 9.803 | 83.35 | 67.17 |#####################| ^ (2.0%, 3.13, 1.8, 1%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1269 batches)
0 >>  2/20 <<   Training | 0.9400 (22, 28,  7, 18) | 1.029 | 2.140 | 84.06 | 68.84 |--------------------------------------|
0             Validation | 0.9414 (22, 28,  7, 18) | 1.020 | 2.041 | 84.01 | 68.34 |#################################| ^ (2.7%, 3.81, 2.6, 0%) 
0 >>  3/20 <<   Training | 0.9376 (22, 28,  7, 19) | 1.014 | 2.926 | 84.17 | 68.19 |-------------------------------|
0             Validation | 0.9390 (22, 28,  7, 19) | 1.005 | 2.886 | 84.12 | 67.74 |###########################| ^ (2.5%, 4.29, 1.3, 14%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (634 batches)
0 >>  4/20 <<   Training | 0.9366 (23, 26,  7, 20) | 1.151 | 18.770 | 84.34 | 68.24 |--------------------------------|
0             Validation | 0.9380 (23, 26,  7, 20) | 1.141 | 8.475 | 84.28 | 67.65 |##########################| ^ (3.2%, 4.37, 3.1, 0%) 
0 >>  5/20 <<   Training | 0.9366 (21, 28,  7, 20) | 0.962 | 2.264 | 84.01 | 68.61 |------------------------------------|
0             Validation | 0.9378 (21, 28,  7, 20) | 0.954 | 2.247 | 83.95 | 68.18 |###############################| ^ (2.3%, 4.78, 1.4, 9%) 
0 >>  6/20 <<   Training | 0.9399 (22, 31,  6, 16) | 1.041 | 3.078 | 84.39 | 69.39 |-------------------------------------------|
0             Validation | 0.9418 (22, 31,  6, 16) | 1.031 | 2.723 | 84.33 | 68.83 |######################################| ^ (2.9%, 5.02, 1.8, 1%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (317 batches)
0 >>  7/20 <<   Training | 0.9304 (21, 29,  6, 19) | 0.976 | 1.868 | 84.51 | 69.73 |-----------------------------------------------|
0             Validation | 0.9320 (21, 29,  6, 19) | 0.967 | 1.327 | 84.45 | 69.21 |##########################################| ^ (2.7%, 4.93, 1.6, 3%) 
0 >>  8/20 <<   Training | 0.9318 (22, 29,  6, 18) | 1.042 | 2.312 | 84.59 | 70.25 |----------------------------------------------------|
0             Validation | 0.9331 (22, 29,  6, 18) | 1.033 | 1.524 | 84.51 | 69.78 |###############################################| ^ (2.3%, 4.88, 1.5, 6%) 
0 >>  9/20 <<   Training | 0.9368 (19, 28,  7, 21) | 0.883 | 13.019 | 84.55 | 69.65 |----------------------------------------------|
0             Validation | 0.9386 (19, 28,  7, 21) | 0.876 | 8.288 | 84.48 | 69.15 |#########################################| ^ (2.6%, 5.07, 1.6, 4%) 
0 >> 10/20 <<   Training | 0.9303 (22, 27,  7, 19) | 1.012 | 2.386 | 84.64 | 69.81 |------------------------------------------------|
0             Validation | 0.9322 (22, 27,  7, 19) | 1.003 | 2.179 | 84.57 | 69.28 |##########################################| ^ (2.7%, 5.40, 1.9, 1%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (158 batches)
0 >> 11/20 <<   Training | 0.9305 (22, 27,  7, 20) | 1.056 | 8.921 | 84.70 | 69.84 |------------------------------------------------|
0             Validation | 0.9318 (22, 27,  7, 20) | 1.046 | 3.210 | 84.62 | 69.32 |###########################################| ^ (2.6%, 4.09, 1.7, 2%) 
0 >> 12/20 <<   Training | 0.9286 (21, 29,  7, 19) | 0.928 | 5.611 | 84.69 | 70.15 |---------------------------------------------------|
0             Validation | 0.9307 (21, 29,  7, 19) | 0.920 | 4.405 | 84.61 | 69.60 |##############################################| ^ (2.7%, 5.54, 1.5, 6%) 
0 >> 13/20 <<   Training | 0.9284 (21, 29,  7, 18) | 0.991 | 1.217 | 84.71 | 69.90 |-------------------------------------------------|
0             Validation | 0.9302 (21, 29,  7, 18) | 0.982 | 1.088 | 84.63 | 69.34 |###########################################| ^ (2.9%, 4.97, 2.0, 0%) 
0 >> 14/20 <<   Training | 0.9283 (22, 28,  7, 19) | 0.996 | 1.314 | 84.73 | 69.80 |------------------------------------------------|
0             Validation | 0.9303 (22, 28,  7, 19) | 0.987 | 1.176 | 84.66 | 69.25 |##########################################| ^ (2.8%, 4.74, 2.4, 0%) 
0 >> 15/20 <<   Training | 0.9292 (22, 28,  7, 18) | 1.076 | 7.588 | 84.75 | 70.02 |--------------------------------------------------|
0             Validation | 0.9309 (22, 28,  7, 18) | 1.066 | 3.051 | 84.67 | 69.48 |############################################| ^ (2.7%, 5.25, 2.2, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.9272 (21, 29,  7, 18) | 0.978 | 1.291 | 84.73 | 69.99 |-------------------------------------------------|
0             Validation | 0.9293 (21, 29,  7, 18) | 0.969 | 1.676 | 84.65 | 69.41 |############################################| ^ (2.9%, 5.19, 2.5, 0%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.9269 (22, 28,  7, 18) | 1.027 | 1.278 | 84.74 | 69.89 |------------------------------------------------|
0             Validation | 0.9291 (22, 28,  7, 18) | 1.017 | 1.063 | 84.66 | 69.28 |##########################################| ^ (3.1%, 5.22, 1.9, 0%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.9268 (22, 28,  7, 19) | 1.000 | 0.762 | 84.75 | 69.91 |-------------------------------------------------|
0             Validation | 0.9290 (22, 28,  7, 19) | 0.990 | 1.232 | 84.67 | 69.32 |###########################################| ^ (3.0%, 5.26, 2.3, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.9268 (22, 28,  7, 19) | 1.001 | 0.864 | 84.75 | 69.92 |-------------------------------------------------|
0             Validation | 0.9290 (22, 28,  7, 19) | 0.992 | 1.215 | 84.67 | 69.32 |###########################################| ^ (3.0%, 5.26, 2.3, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.9268 (22, 28,  7, 19) | 1.001 | 0.832 | 84.75 | 69.92 |-------------------------------------------------|
0             Validation | 0.9290 (22, 28,  7, 19) | 0.991 | 1.173 | 84.67 | 69.32 |###########################################| ^ (3.0%, 5.26, 2.2, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_lr0.01_epochs20_offset0_epoch20_before_finetuning.pkl
Run Finetuning
0 >> 20/20 <<   Training | 0.9268 (22, 29,  7, 19) | 0.998 | 0.770 | 84.75 | 69.92 |-------------------------------------------------|
0             Validation | 0.9290 (22, 28,  7, 19) | 0.989 | 1.215 | 84.67 | 69.32 |###########################################| ^ (3.0%, 5.26, 2.2, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.8817 (26, 28,  7, 21) | 1.127 | 9.889 | 83.16 | 67.71 |---------------------------|
0             Validation | 0.8819 (26, 28,  7, 21) | 1.120 | 5.362 | 83.11 | 67.66 |##########################| ^ (0.7%, 3.03, 1.8, 1%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1216 batches)
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.8817 (26, 28,  7, 21) | 1.127 | 9.889 | 83.16 | 67.71 |---------------------------|
0             Validation | 0.8819 (26, 28,  7, 21) | 1.120 | 5.362 | 83.11 | 67.66 |##########################| ^ (0.7%, 3.03, 1.8, 1%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1216 batches)
0 >>  2/20 <<   Training | 0.8739 (25, 32,  7, 18) | 1.026 | 1.313 | 83.58 | 67.98 |-----------------------------|
0             Validation | 0.8743 (25, 32,  7, 18) | 1.020 | 1.042 | 83.51 | 67.87 |############################| ^ (0.8%, 3.83, 1.8, 1%) 
0 >>  3/20 <<   Training | 0.8731 (25, 33,  7, 18) | 1.012 | 1.146 | 83.79 | 68.39 |---------------------------------|
0             Validation | 0.8739 (25, 33,  7, 18) | 1.007 | 1.062 | 83.70 | 68.21 |################################| ^ (1.1%, 2.80, 1.6, 3%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (608 batches)
0 >>  4/20 <<   Training | 0.8684 (23, 33,  7, 20) | 0.871 | 19.839 | 83.96 | 68.43 |----------------------------------|
0             Validation | 0.8694 (23, 33,  7, 20) | 0.867 | 11.347 | 83.88 | 68.30 |#################################| ^ (0.9%, 3.09, 1.0, 41%) 
0 >>  5/20 <<   Training | 0.8674 (24, 31,  7, 20) | 1.010 | 3.561 | 84.03 | 68.74 |-------------------------------------|
0             Validation | 0.8688 (24, 31,  7, 20) | 1.005 | 2.187 | 83.92 | 68.46 |##################################| ^ (1.5%, 2.80, 1.3, 14%) 
0 >>  6/20 <<   Training | 0.8670 (24, 32,  7, 20) | 0.945 | 4.305 | 84.02 | 68.58 |-----------------------------------|
0             Validation | 0.8681 (23, 32,  7, 20) | 0.940 | 2.993 | 83.94 | 68.38 |#################################| ^ (1.2%, 3.19, 1.2, 18%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (304 batches)
0 >>  7/20 <<   Training | 0.8657 (26, 30,  7, 19) | 1.100 | 9.550 | 84.26 | 68.87 |--------------------------------------|
0             Validation | 0.8669 (26, 30,  7, 19) | 1.094 | 4.696 | 84.15 | 68.65 |####################################| ^ (1.2%, 2.82, 1.0, 40%) 
0 >>  8/20 <<   Training | 0.8655 (25, 30,  7, 20) | 1.051 | 3.238 | 84.24 | 68.27 |--------------------------------|
0             Validation | 0.8669 (25, 30,  7, 20) | 1.046 | 2.196 | 84.12 | 68.02 |##############################| ^ (1.6%, 2.99, 0.9, 57%) 
0 >>  9/20 <<   Training | 0.8643 (24, 31,  7, 20) | 0.997 | 1.082 | 84.26 | 69.14 |-----------------------------------------|
0             Validation | 0.8658 (24, 31,  7, 20) | 0.992 | 1.084 | 84.15 | 68.83 |######################################| ^ (1.6%, 2.88, 1.1, 27%) 
0 >> 10/20 <<   Training | 0.8653 (23, 32,  7, 20) | 0.959 | 4.062 | 84.21 | 68.97 |---------------------------------------|
0             Validation | 0.8669 (23, 32,  7, 20) | 0.954 | 2.257 | 84.12 | 68.64 |####################################| ^ (1.8%, 2.99, 1.2, 17%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (152 batches)
0 >> 11/20 <<   Training | 0.8661 (22, 32,  8, 21) | 0.881 | 14.854 | 84.34 | 69.00 |---------------------------------------|
0             Validation | 0.8677 (22, 32,  7, 21) | 0.877 | 8.304 | 84.22 | 68.66 |####################################| ^ (1.8%, 2.80, 1.0, 36%) 
0 >> 12/20 <<   Training | 0.8655 (25, 32,  7, 18) | 1.046 | 5.625 | 84.38 | 68.95 |---------------------------------------|
0             Validation | 0.8669 (25, 32,  7, 18) | 1.041 | 2.252 | 84.28 | 68.65 |####################################| ^ (1.7%, 2.84, 1.1, 32%) 
0 >> 13/20 <<   Training | 0.8630 (24, 31,  7, 20) | 1.009 | 1.579 | 84.40 | 68.85 |--------------------------------------|
0             Validation | 0.8646 (24, 31,  7, 20) | 1.004 | 0.746 | 84.29 | 68.54 |###################################| ^ (1.7%, 2.74, 1.3, 15%) 
0 >> 14/20 <<   Training | 0.8632 (23, 32,  7, 20) | 0.914 | 9.407 | 84.36 | 68.88 |--------------------------------------|
0             Validation | 0.8650 (23, 32,  7, 20) | 0.909 | 5.906 | 84.24 | 68.58 |###################################| ^ (1.6%, 3.15, 0.8, 72%) 
0 >> 15/20 <<   Training | 0.8632 (24, 32,  7, 19) | 0.993 | 0.909 | 84.32 | 68.89 |--------------------------------------|
0             Validation | 0.8649 (24, 32,  7, 19) | 0.988 | 1.281 | 84.21 | 68.55 |###################################| ^ (1.8%, 2.91, 1.8, 1%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.8623 (23, 32,  7, 20) | 0.929 | 5.366 | 84.43 | 69.06 |----------------------------------------|
0             Validation | 0.8642 (23, 32,  7, 20) | 0.924 | 3.486 | 84.32 | 68.72 |#####################################| ^ (1.8%, 3.04, 1.8, 1%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.8617 (24, 31,  7, 20) | 0.989 | 1.621 | 84.43 | 69.04 |----------------------------------------|
0             Validation | 0.8636 (24, 31,  7, 20) | 0.984 | 1.093 | 84.31 | 68.69 |####################################| ^ (1.8%, 3.05, 1.0, 42%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.8616 (24, 31,  7, 20) | 0.996 | 1.455 | 84.46 | 69.08 |----------------------------------------|
0             Validation | 0.8635 (24, 31,  7, 20) | 0.991 | 0.935 | 84.33 | 68.73 |#####################################| ^ (1.8%, 3.08, 0.9, 49%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.8616 (24, 31,  7, 20) | 0.996 | 1.466 | 84.45 | 69.08 |----------------------------------------|
0             Validation | 0.8635 (24, 31,  7, 20) | 0.991 | 0.994 | 84.33 | 68.73 |#####################################| ^ (1.8%, 3.08, 0.9, 48%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.8616 (24, 31,  7, 20) | 0.996 | 1.343 | 84.45 | 69.08 |----------------------------------------|
0             Validation | 0.8635 (24, 31,  7, 20) | 0.990 | 0.967 | 84.33 | 68.73 |#####################################| ^ (1.8%, 3.08, 1.0, 45%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_lr0.01_epochs20_offset0_epoch20_before_finetuning.pkl
Run Finetuning
0 >> 20/20 <<   Training | 0.8616 (24, 31,  7, 20) | 0.995 | 1.370 | 84.45 | 69.08 |----------------------------------------|
0             Validation | 0.8635 (24, 31,  7, 20) | 0.990 | 1.009 | 84.33 | 68.73 |#####################################| ^ (1.8%, 3.08, 1.0, 41%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_8_np1052_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
