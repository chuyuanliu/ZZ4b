Tue Apr 19 19:29:52 2022
Using weight: weight for classifier: SvB_MA
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 2396428
wDB 114777.240743
wDBn -53.981695
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 128876
wT 4694.203751
wTn -16.779144
nS 645566
nB 2525304
sum_wS 201.702339
sum_wB 119579.407884
nzz =   68867, wzz =   71.9, wzz_SR =   59.2
nzh =  541264, wzh =   92.5, wzh_SR =   77.9
nhh =   35435, whh =   37.3, whh_SR =   33.4
sum_wS_SR 170.521446
sum_wB_SR 50599.001540
sum_wSp_SR 203.140887
sum_wSn_SR 32.619440
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 264.795507
sigScaleZH 394.679756
sigScaleHH 619.280794
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.192839
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_8_np1061_seed0_lr0.01_epochs20_offset2.log
Setup training/validation tensors
2 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >>  1/20 <<   Training | 0.6398 (47, 11, 17, 27, 17) | 0.852 | 1.004 | 90.29 | 86.77 |---------------------------|
2             Validation | 0.6468 (47, 11, 18, 27, 16) | 0.804 | 0.952 | 89.85 | 86.38 |#######################| ^ (1.1%, 10.49, 11.8, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1031 batches)
2 >>  2/20 <<   Training | 0.6165 (41, 12, 15, 31, 20) | 0.962 | 1.126 | 90.90 | 87.76 |-------------------------------------|
2             Validation | 0.6222 (41, 12, 17, 30, 20) | 0.914 | 1.073 | 90.43 | 87.53 |###################################| ^ (0.6%, 10.57, 9.0, 0%) 
2 >>  3/20 <<   Training | 0.6042 (41, 12, 18, 30, 18) | 1.027 | 1.204 | 90.74 | 87.92 |---------------------------------------|
2             Validation | 0.6115 (41, 11, 19, 30, 18) | 0.965 | 1.135 | 90.28 | 87.63 |####################################| ^ (0.8%, 10.63, 10.3, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (515 batches)
2 >>  4/20 <<   Training | 0.5997 (41, 11, 18, 31, 18) | 1.016 | 1.189 | 90.93 | 87.99 |---------------------------------------|
2             Validation | 0.6063 (41, 11, 19, 31, 18) | 0.946 | 1.110 | 90.46 | 87.74 |#####################################| ^ (0.7%, 10.69, 10.1, 0%) 
2 >>  5/20 <<   Training | 0.5953 (38, 11, 18, 33, 20) | 1.112 | 1.303 | 91.16 | 88.09 |----------------------------------------|
2             Validation | 0.6009 (38, 11, 19, 32, 19) | 1.019 | 1.195 | 90.68 | 87.92 |#######################################| ^ (0.5%, 9.66, 9.6, 0%) 
2 >>  6/20 <<   Training | 0.6036 (38, 11, 17, 35, 19) | 1.010 | 1.184 | 90.64 | 87.99 |---------------------------------------|
2             Validation | 0.6092 (38, 10, 19, 34, 19) | 0.954 | 1.123 | 89.97 | 87.80 |#####################################| ^ (0.5%, 9.65, 8.5, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (257 batches)
2 >>  7/20 <<   Training | 0.5975 (38, 11, 18, 33, 20) | 1.014 | 1.189 | 91.10 | 87.83 |--------------------------------------|
2             Validation | 0.6034 (38, 11, 19, 32, 20) | 0.939 | 1.105 | 90.63 | 87.61 |####################################| ^ (0.6%, 9.39, 8.4, 0%) 
2 >>  8/20 <<   Training | 0.5938 (36, 11, 17, 33, 22) | 1.100 | 1.286 | 91.26 | 88.22 |------------------------------------------|
2             Validation | 0.5989 (36, 11, 18, 33, 21) | 1.013 | 1.188 | 90.79 | 88.04 |########################################| ^ (0.5%, 9.84, 7.6, 0%) 
2 >>  9/20 <<   Training | 0.5926 (39,  9, 18, 33, 19) | 1.051 | 1.232 | 90.80 | 88.16 |-----------------------------------------|
2             Validation | 0.5985 (39,  9, 20, 33, 19) | 0.986 | 1.157 | 90.33 | 87.92 |#######################################| ^ (0.7%, 10.20, 8.8, 0%) 
2 >> 10/20 <<   Training | 0.5874 (37,  9, 19, 34, 20) | 1.103 | 1.291 | 90.92 | 88.24 |------------------------------------------|
2             Validation | 0.5929 (37,  9, 20, 33, 20) | 1.020 | 1.197 | 90.52 | 88.04 |########################################| ^ (0.6%, 9.33, 8.8, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (128 batches)
2 >> 11/20 <<   Training | 0.5922 (36, 10, 18, 36, 20) | 1.096 | 1.283 | 91.02 | 88.24 |------------------------------------------|
2             Validation | 0.5980 (35, 10, 19, 35, 20) | 1.014 | 1.189 | 90.50 | 88.02 |########################################| ^ (0.6%, 10.57, 8.2, 0%) 
2 >> 12/20 <<   Training | 0.5867 (40,  9, 17, 31, 21) | 1.104 | 1.295 | 91.19 | 88.31 |-------------------------------------------|
2             Validation | 0.5926 (40,  9, 19, 31, 20) | 1.035 | 1.215 | 90.75 | 88.08 |########################################| ^ (0.6%, 10.21, 10.1, 0%) 
2 >> 13/20 <<   Training | 0.5912 (37, 11, 17, 34, 21) | 1.096 | 1.283 | 91.19 | 88.25 |------------------------------------------|
2             Validation | 0.5963 (37, 11, 18, 33, 21) | 1.026 | 1.204 | 90.72 | 88.02 |########################################| ^ (0.6%, 9.56, 9.0, 0%) 
2 >> 14/20 <<   Training | 0.5851 (37, 11, 18, 32, 21) | 1.104 | 1.291 | 91.03 | 88.25 |------------------------------------------|
2             Validation | 0.5909 (37, 11, 20, 32, 20) | 1.020 | 1.195 | 90.63 | 88.02 |########################################| ^ (0.6%, 10.22, 8.2, 0%) 
2 >> 15/20 <<   Training | 0.5903 (41, 10, 16, 31, 21) | 1.083 | 1.268 | 91.28 | 88.33 |-------------------------------------------|
2             Validation | 0.5956 (41, 10, 17, 30, 20) | 1.003 | 1.179 | 90.75 | 88.12 |#########################################| ^ (0.6%, 9.80, 10.0, 0%) 
Decay learning rate: 0.010000 -> 0.002500
2 >> 16/20 <<   Training | 0.5842 (39, 10, 17, 33, 20) | 1.107 | 1.295 | 91.26 | 88.40 |-------------------------------------------|
2             Validation | 0.5900 (39, 10, 18, 32, 20) | 1.032 | 1.210 | 90.81 | 88.16 |#########################################| ^ (0.6%, 10.33, 9.5, 0%) 
Decay learning rate: 0.002500 -> 0.000625
2 >> 17/20 <<   Training | 0.5829 (39, 10, 17, 32, 20) | 1.103 | 1.290 | 91.23 | 88.39 |-------------------------------------------|
2             Validation | 0.5889 (39, 10, 19, 32, 20) | 1.027 | 1.205 | 90.79 | 88.15 |#########################################| ^ (0.7%, 10.35, 9.2, 0%) 
Decay learning rate: 0.000625 -> 0.000156
2 >> 18/20 <<   Training | 0.5828 (39, 10, 17, 33, 20) | 1.102 | 1.289 | 91.23 | 88.40 |-------------------------------------------|
2             Validation | 0.5887 (39, 10, 19, 32, 20) | 1.028 | 1.204 | 90.78 | 88.16 |#########################################| ^ (0.6%, 10.35, 9.3, 0%) 
Decay learning rate: 0.000156 -> 0.000039
2 >> 19/20 <<   Training | 0.5825 (39, 10, 18, 33, 20) | 1.102 | 1.289 | 91.22 | 88.39 |-------------------------------------------|
2             Validation | 0.5884 (39, 10, 19, 32, 20) | 1.026 | 1.202 | 90.78 | 88.16 |#########################################| ^ (0.6%, 10.35, 9.4, 0%) 
Decay learning rate: 0.000039 -> 0.000010
2 >> 20/20 <<   Training | 0.5827 (39, 10, 18, 32, 20) | 1.099 | 1.285 | 91.23 | 88.39 |-------------------------------------------|
2             Validation | 0.5886 (39, 10, 19, 32, 20) | 1.027 | 1.203 | 90.78 | 88.16 |#########################################| ^ (0.6%, 10.34, 9.2, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_8_np1061_seed0_lr0.01_epochs20_offset2_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

2 >> DONE <<
2 Minimum Loss = 0.582546
