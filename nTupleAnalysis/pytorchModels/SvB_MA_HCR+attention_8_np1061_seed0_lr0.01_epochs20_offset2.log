Tue Apr 19 19:29:52 2022
Using weight: weight for classifier: SvB_MA
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 2396428
wDB 114777.240743
wDBn -53.981695
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 128876
wT 4694.203751
wTn -16.779144
nS 645566
nB 2525304
sum_wS 201.702339
sum_wB 119579.407884
nzz =   68867, wzz =   71.9, wzz_SR =   59.2
nzh =  541264, wzh =   92.5, wzh_SR =   77.9
nhh =   35435, whh =   37.3, whh_SR =   33.4
sum_wS_SR 170.521446
sum_wB_SR 50599.001540
sum_wSp_SR 203.140887
sum_wSn_SR 32.619440
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 264.795507
sigScaleZH 394.679756
sigScaleHH 619.280794
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.192839
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_8_np1061_seed0_lr0.01_epochs20_offset2.log
Setup training/validation tensors
2 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >>  1/20 <<   Training | 0.6398 (47, 11, 17, 27, 17) | 0.852 | 1.004 | 90.29 | 86.77 |---------------------------|
2             Validation | 0.6468 (47, 11, 18, 27, 16) | 0.804 | 0.952 | 89.85 | 86.38 |#######################| ^ (1.1%, 10.49, 11.8, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1031 batches)
2 >>  2/20 <<   Training | 0.6165 (41, 12, 15, 31, 20) | 0.962 | 1.126 | 90.90 | 87.76 |-------------------------------------|
2             Validation | 0.6222 (41, 12, 17, 30, 20) | 0.914 | 1.073 | 90.43 | 87.53 |###################################| ^ (0.6%, 10.57, 9.0, 0%) 
2 >>  3/20 <<   Training | 0.6042 (41, 12, 18, 30, 18) | 1.027 | 1.204 | 90.74 | 87.92 |---------------------------------------|
2             Validation | 0.6115 (41, 11, 19, 30, 18) | 0.965 | 1.135 | 90.28 | 87.63 |####################################| ^ (0.8%, 10.63, 10.3, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (515 batches)
2 >>  4/20 <<   Training | 0.5997 (41, 11, 18, 31, 18) | 1.016 | 1.189 | 90.93 | 87.99 |---------------------------------------|
2             Validation | 0.6063 (41, 11, 19, 31, 18) | 0.946 | 1.110 | 90.46 | 87.74 |#####################################| ^ (0.7%, 10.69, 10.1, 0%) 
2 >>  5/20 <<   Training | 0.5953 (38, 11, 18, 33, 20) | 1.112 | 1.303 | 91.16 | 88.09 |----------------------------------------|
2             Validation | 0.6009 (38, 11, 19, 32, 19) | 1.019 | 1.195 | 90.68 | 87.92 |#######################################| ^ (0.5%, 9.66, 9.6, 0%) 
2 >>  6/20 <<   Training | 0.6036 (38, 11, 17, 35, 19) | 1.010 | 1.184 | 90.64 | 87.99 |---------------------------------------|
2             Validation | 0.6092 (38, 10, 19, 34, 19) | 0.954 | 1.123 | 89.97 | 87.80 |#####################################| ^ (0.5%, 9.65, 8.5, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (257 batches)
2 >>  7/20 <<   Training | 0.5975 (38, 11, 18, 33, 20) | 1.014 | 1.189 | 91.10 | 87.83 |--------------------------------------|
2             Validation | 0.6034 (38, 11, 19, 32, 20) | 0.939 | 1.105 | 90.63 | 87.61 |####################################| ^ (0.6%, 9.39, 8.4, 0%) 
2 >>  8/20 <<   Training | 0.5938 (36, 11, 17, 33, 22) | 1.100 | 1.286 | 91.26 | 88.22 |------------------------------------------|
2             Validation | 0.5989 (36, 11, 18, 33, 21) | 1.013 | 1.188 | 90.79 | 88.04 |########################################| ^ (0.5%, 9.84, 7.6, 0%) 
2 >>  9/20 <<   Training | 0.5926 (39,  9, 18, 33, 19) | 1.051 | 1.232 | 90.80 | 88.16 |-----------------------------------------|
2             Validation | 0.5985 (39,  9, 20, 33, 19) | 0.986 | 1.157 | 90.33 | 87.92 |#######################################| ^ (0.7%, 10.20, 8.8, 0%) 
2 >> 10/20 <<   Training | 0.5874 (37,  9, 19, 34, 20) | 1.103 | 1.291 | 90.92 | 88.24 |------------------------------------------|
2             Validation | 0.5929 (37,  9, 20, 33, 20) | 1.020 | 1.197 | 90.52 | 88.04 |########################################| ^ (0.6%, 9.33, 8.8, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (128 batches)
2 >> 11/20 <<   Training | 0.5922 (36, 10, 18, 36, 20) | 1.096 | 1.283 | 91.02 | 88.24 |------------------------------------------|
2             Validation | 0.5980 (35, 10, 19, 35, 20) | 1.014 | 1.189 | 90.50 | 88.02 |########################################| ^ (0.6%, 10.57, 8.2, 0%) 
2 >> 12/20 <<   Training | 0.5867 (40,  9, 17, 31, 21) | 1.104 | 1.295 | 91.19 | 88.31 |-------------------------------------------|
2             Validation | 0.5926 (40,  9, 19, 31, 20) | 1.035 | 1.215 | 90.75 | 88.08 |########################################| ^ (0.6%, 10.21, 10.1, 0%) 
2 >> 13/20 <<   Training | 0.5912 (37, 11, 17, 34, 21) | 1.096 | 1.283 | 91.19 | 88.25 |------------------------------------------|
2             Validation | 0.5963 (37, 11, 18, 33, 21) | 1.026 | 1.204 | 90.72 | 88.02 |########################################| ^ (0.6%, 9.56, 9.0, 0%) 
2 >> 14/20 <<   Training | 0.5851 (37, 11, 18, 32, 21) | 1.104 | 1.291 | 91.03 | 88.25 |------------------------------------------|
2             Validation | 0.5909 (37, 11, 20, 32, 20) | 1.020 | 1.195 | 90.63 | 88.02 |########################################| ^ (0.6%, 10.22, 8.2, 0%) 
2 >> 15/20 <<   Training | 0.5903 (41, 10, 16, 31, 21) | 1.083 | 1.268 | 91.28 | 88.33 |-------------------------------------------|
2             Validation | 0.5956 (41, 10, 17, 30, 20) | 1.003 | 1.179 | 90.75 | 88.12 |#########################################| ^ (0.6%, 9.80, 10.0, 0%) 
Decay learning rate: 0.010000 -> 0.002500
2 >> 16/20 <<   Training | 0.5842 (39, 10, 17, 33, 20) | 1.107 | 1.295 | 91.26 | 88.40 |-------------------------------------------|
2             Validation | 0.5900 (39, 10, 18, 32, 20) | 1.032 | 1.210 | 90.81 | 88.16 |#########################################| ^ (0.6%, 10.33, 9.5, 0%) 
Decay learning rate: 0.002500 -> 0.000625
2 >> 17/20 <<   Training | 0.5829 (39, 10, 17, 32, 20) | 1.103 | 1.290 | 91.23 | 88.39 |-------------------------------------------|
2             Validation | 0.5889 (39, 10, 19, 32, 20) | 1.027 | 1.205 | 90.79 | 88.15 |#########################################| ^ (0.7%, 10.35, 9.2, 0%) 
Decay learning rate: 0.000625 -> 0.000156
2 >> 18/20 <<   Training | 0.5828 (39, 10, 17, 33, 20) | 1.102 | 1.289 | 91.23 | 88.40 |-------------------------------------------|
2             Validation | 0.5887 (39, 10, 19, 32, 20) | 1.028 | 1.204 | 90.78 | 88.16 |#########################################| ^ (0.6%, 10.35, 9.3, 0%) 
Decay learning rate: 0.000156 -> 0.000039
2 >> 19/20 <<   Training | 0.5825 (39, 10, 18, 33, 20) | 1.102 | 1.289 | 91.22 | 88.39 |-------------------------------------------|
2             Validation | 0.5884 (39, 10, 19, 32, 20) | 1.026 | 1.202 | 90.78 | 88.16 |#########################################| ^ (0.6%, 10.35, 9.4, 0%) 
Decay learning rate: 0.000039 -> 0.000010
2 >> 20/20 <<   Training | 0.5827 (39, 10, 18, 32, 20) | 1.099 | 1.285 | 91.23 | 88.39 |-------------------------------------------|
2             Validation | 0.5886 (39, 10, 19, 32, 20) | 1.027 | 1.203 | 90.78 | 88.16 |#########################################| ^ (0.6%, 10.34, 9.2, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_8_np1061_seed0_lr0.01_epochs20_offset2_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

2 >> DONE <<
2 Minimum Loss = 0.582546
Mon May  9 17:07:41 2022
Using weight: weight for classifier: SvB_MA
Setting dfDB weight: weight to: mcPseudoTagWeight * FvT
nDB 3279028
wDB 147993.557827
wDBn -59.208815
Negative weight means P(4b ttbar)>P(4b data) so we should switch these to ttbar events and flip their weights postive
dfDB['mj']=~weight_negative
dfDB['tt']= weight_negative
dfDB[weight] = dfDB[weight].abs()
nT 179878
wT 6758.331864
wTn -23.581879
nS 940327
nB 3458906
sum_wS 167.472288
sum_wB 154870.307320
nzz =  323399, wzz =   57.4, wzz_SR =   46.2
nzh =  574169, wzh =   74.7, wzh_SR =   61.9
nhh =   42759, whh =   35.4, whh_SR =   31.2
sum_wS_SR 139.340355
sum_wB_SR 57305.889757
sum_wSp_SR 164.693187
sum_wSn_SR 25.352832
Negative weight events will be zeroed out in the loss calculation used during training but nowhere else
sigScaleZZ 295.764663
sigScaleZH 457.493368
sigScaleHH 679.928873
add encoded target: mj=0, tt=1, zz=2, zh=3, hh=4
loaded die loss: 1.099681
Set log file: ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_8_np1061_seed0_lr0.01_epochs20_offset2.log
Setup training/validation tensors
2 >> Epoch <<   Data Set |  Loss %(mj, tt, zz, zh, hh) |  S/B   | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >>  1/20 <<   Training | 0.5987 (32, 12, 16, 34, 17) |  8/177 | 0.918 | 90.37 | 88.99 |-------------------------------------------------|
2             Validation | 0.6009 (32, 12, 16, 34, 17) |  8/195 | 0.889 | 90.08 | 88.85 |################################################| ^ (0.4%, 9.02, 6.4, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1431 batches)
2 >>  2/20 <<   Training | 0.5793 (36, 10, 16, 30, 19) |  7/ 68 | 1.113 | 90.76 | 89.51 |-------------------------------------------------------|
2             Validation | 0.5805 (36, 10, 15, 29, 19) |  7/ 89 | 1.054 | 90.51 | 89.38 |#####################################################| ^ (0.4%, 9.22, 6.9, 0%) 
2 >>  3/20 <<   Training | 0.5797 (37, 10, 15, 30, 18) | 17/315 | 1.060 | 90.72 | 89.68 |--------------------------------------------------------|
2             Validation | 0.5817 (37, 10, 15, 29, 18) | 17/342 | 1.023 | 90.48 | 89.55 |#######################################################| ^ (0.4%, 8.49, 6.9, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (715 batches)
2 >>  4/20 <<   Training | 0.5759 (37, 11, 15, 29, 18) | 15/253 | 1.082 | 90.89 | 89.72 |---------------------------------------------------------|
2             Validation | 0.5782 (37, 11, 15, 29, 18) | 16/287 | 1.036 | 90.60 | 89.58 |#######################################################| ^ (0.4%, 9.22, 6.8, 0%) 
2 >>  5/20 <<   Training | 0.5722 (36, 12, 16, 26, 20) | 15/216 | 1.139 | 90.96 | 89.78 |---------------------------------------------------------|
2             Validation | 0.5737 (36, 12, 16, 26, 20) | 15/260 | 1.068 | 90.71 | 89.64 |########################################################| ^ (0.4%, 8.19, 4.9, 0%) 
2 >>  6/20 <<   Training | 0.5742 (37, 11, 14, 29, 19) | 16/259 | 1.113 | 90.97 | 89.85 |----------------------------------------------------------|
2             Validation | 0.5763 (37, 10, 14, 29, 19) | 16/291 | 1.065 | 90.74 | 89.69 |########################################################| ^ (0.5%, 8.83, 6.3, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (357 batches)
2 >>  7/20 <<   Training | 0.5732 (35, 11, 15, 29, 20) | 11/120 | 1.175 | 90.77 | 89.85 |----------------------------------------------------------|
2             Validation | 0.5746 (35, 11, 15, 29, 20) | 11/145 | 1.111 | 90.57 | 89.71 |#########################################################| ^ (0.4%, 8.52, 5.2, 0%) 
2 >>  8/20 <<   Training | 0.5678 (36, 11, 16, 28, 19) | 12/164 | 1.152 | 90.99 | 89.92 |-----------------------------------------------------------|
2             Validation | 0.5695 (36, 11, 16, 28, 19) | 13/188 | 1.102 | 90.73 | 89.78 |#########################################################| ^ (0.4%, 8.30, 5.4, 0%) 
2 >>  9/20 <<   Training | 0.5667 (35, 11, 16, 29, 19) | 16/235 | 1.142 | 91.01 | 89.89 |----------------------------------------------------------|
2             Validation | 0.5685 (36, 11, 16, 29, 19) | 16/269 | 1.081 | 90.81 | 89.74 |#########################################################| ^ (0.4%, 9.01, 6.3, 0%) 
2 >> 10/20 <<   Training | 0.5753 (36, 12, 14, 29, 20) | 15/235 | 1.121 | 91.10 | 89.83 |----------------------------------------------------------|
2             Validation | 0.5768 (36, 12, 14, 28, 20) | 16/267 | 1.072 | 90.82 | 89.72 |#########################################################| ^ (0.3%, 8.63, 4.5, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (178 batches)
2 >> 11/20 <<   Training | 0.5679 (34, 13, 16, 29, 18) | 16/252 | 1.104 | 91.10 | 89.79 |---------------------------------------------------------|
2             Validation | 0.5696 (35, 12, 16, 29, 18) | 16/288 | 1.054 | 90.83 | 89.65 |########################################################| ^ (0.4%, 8.73, 4.7, 0%) 
2 >> 12/20 <<   Training | 0.5672 (36, 10, 15, 30, 19) | 13/163 | 1.188 | 91.00 | 89.98 |-----------------------------------------------------------|
2             Validation | 0.5691 (36, 10, 15, 30, 19) | 13/204 | 1.108 | 90.79 | 89.82 |##########################################################| ^ (0.4%, 8.82, 6.4, 0%) 
2 >> 13/20 <<   Training | 0.5679 (37, 10, 15, 29, 19) | 15/209 | 1.155 | 91.08 | 89.97 |-----------------------------------------------------------|
2             Validation | 0.5699 (37, 10, 15, 29, 19) | 15/243 | 1.096 | 90.86 | 89.82 |##########################################################| ^ (0.4%, 8.84, 6.5, 0%) 
2 >> 14/20 <<   Training | 0.5687 (33, 12, 15, 30, 20) | 13/172 | 1.172 | 91.06 | 89.89 |----------------------------------------------------------|
2             Validation | 0.5703 (33, 12, 15, 30, 20) | 13/211 | 1.103 | 90.80 | 89.76 |#########################################################| ^ (0.4%, 9.26, 5.4, 0%) 
2 >> 15/20 <<   Training | 0.5687 (38, 11, 15, 28, 19) | 17/266 | 1.121 | 91.19 | 89.94 |-----------------------------------------------------------|
2             Validation | 0.5706 (38, 11, 15, 28, 19) | 17/308 | 1.064 | 90.92 | 89.80 |#########################################################| ^ (0.4%, 8.18, 5.4, 0%) 
Decay learning rate: 0.010000 -> 0.002500
2 >> 16/20 <<   Training | 0.5647 (36, 11, 15, 29, 19) | 16/228 | 1.154 | 91.05 | 90.02 |------------------------------------------------------------|
2             Validation | 0.5667 (36, 11, 15, 29, 19) | 16/270 | 1.089 | 90.82 | 89.88 |##########################################################| ^ (0.4%, 8.82, 6.0, 0%) 
Decay learning rate: 0.002500 -> 0.000625
2 >> 17/20 <<   Training | 0.5640 (35, 11, 16, 29, 19) | 14/193 | 1.169 | 91.16 | 90.01 |------------------------------------------------------------|
2             Validation | 0.5658 (35, 11, 16, 29, 19) | 14/235 | 1.096 | 90.92 | 89.87 |##########################################################| ^ (0.4%, 8.73, 5.5, 0%) 
Decay learning rate: 0.000625 -> 0.000156
2 >> 18/20 <<   Training | 0.5643 (35, 11, 15, 29, 19) | 15/203 | 1.168 | 91.16 | 90.02 |------------------------------------------------------------|
2             Validation | 0.5661 (36, 11, 15, 29, 19) | 15/247 | 1.095 | 90.92 | 89.88 |##########################################################| ^ (0.4%, 8.74, 5.9, 0%) 
Decay learning rate: 0.000156 -> 0.000039
2 >> 19/20 <<   Training | 0.5644 (35, 11, 15, 29, 19) | 15/200 | 1.170 | 91.16 | 90.02 |------------------------------------------------------------|
2             Validation | 0.5663 (36, 11, 15, 29, 19) | 15/243 | 1.098 | 90.93 | 89.89 |##########################################################| ^ (0.4%, 8.73, 5.8, 0%) 
Decay learning rate: 0.000039 -> 0.000010
2 >> 20/20 <<   Training | 0.5643 (35, 11, 15, 29, 19) | 15/197 | 1.171 | 91.16 | 90.02 |------------------------------------------------------------|
2             Validation | 0.5661 (36, 11, 15, 29, 19) | 15/242 | 1.098 | 90.92 | 89.88 |##########################################################| ^ (0.4%, 8.73, 5.9, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_8_np1061_seed0_lr0.01_epochs20_offset2_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002

2 >> DONE <<
2 Minimum Loss = 0.563971
